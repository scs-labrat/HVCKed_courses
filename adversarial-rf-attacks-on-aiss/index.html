<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Adversarial-RF-Attacks-On-AISS</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="container">
        
        <p><a href="../index.html">← Back to Course Catalog</a></p>

        <!-- Header Area -->
        <div class="course-header">
             <span class="category-tag">Category Placeholder</span> <!-- Add category data if available -->
            <h1>Adversarial-RF-Attacks-On-AISS</h1>
            <p class="course-description">Description placeholder based on folder name</p> <!-- Add description data if available -->
            <div class="course-stats">
                <span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-clock h-5 w-5 mr-2 text-primary"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg> Duration Placeholder</span> <!-- Add duration data if available -->
                <span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-layers h-5 w-5 mr-2 text-primary"><path d="m12 18-6-6-4 4 10 10 10-10-4-4-6 6"/><path d="m12 18v4"/><path d="m2 12 10 10"/><path d="M12 18 22 8"/><path d="M6 6 10 2l10 10"/></svg> 8 Modules</span>
                <span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-zap h-5 w-5 mr-2 text-primary"><path d="M13 2v10h6l-7 10v-10H5z"/></svg> Difficulty Placeholder</span> <!-- Add difficulty data if available -->
            </div>
            <button>Start Learning</button>
        </div>

        <!-- Course Body: Tabs Navigation -->
        <!-- Added relative positioning to tabs-nav for potential dropdown positioning -->
        <div class="course-tabs-nav" style="position: relative;">
             <!-- Links use data attributes for JS handling and #hashes for history -->
             <a href="#overview" class="tab-link active" data-view="overview">Overview</a>
             <!-- Course Content tab now acts as a dropdown toggle -->
             <a href="#course-content" class="tab-link" data-view="course-content-toggle">Course Content</a>
             <a href="#discussion" class="tab-link disabled" data-view="discussion">Discussion (Static)</a>
        </div>
        <!-- The dropdown menu will be dynamically created and appended near the tabs nav -->


        <!-- Course Body: Content Area (Two-Column Layout) -->
        <!-- This grid structure is always present on course pages -->
        <div class="course-body-grid">
            <div class="main-content-column">
                 <!-- Content will be loaded here by JS -->
                 <!-- Initial content is Overview (handled by JS on load) -->
                 <!-- The 'card main-content-card' is now part of the fragment HTML itself -->
            </div>
            <div class="sidebar-column">
                 <!-- Sidebar content (only for overview) will be loaded here by JS -->
            </div>
        </div>

         <!-- Hidden container for content fragments and data -->
         <!-- Store fragments and raw data as JSON string for easier parsing in JS -->
        <script id="course-fragments" type="application/json">
        {
  "overview": "\n        <div class=\"card main-content-card\"> <!-- Added main-content-card class -->\n            <h2>About This Course</h2>\n            <div class=\"markdown-content\">\n                <p>Okay, let&#39;s craft a comprehensive 8-module course outline on building Adversarial RF Attacks on AI Spectrum Systems using SDRs. This journey will take learners from foundational concepts to executing sophisticated over-the-air attacks, all while building the components for their capstone project.</p>\n<p>Here is the outline, designed with a focus on practical application and deep understanding:</p>\n<hr>\n<p><strong>Course Title:</strong> Adversarial RF Attacks on AI Spectrum Systems using SDRs</p>\n<p><strong>Overall Course Objective:</strong> By the end of this course, learners will be able to design, implement, and execute adversarial RF attacks against AI-driven signal classification systems using Software-Defined Radios, effectively creating a functional clone of the concepts explored.</p>\n<p><strong>Target Audience:</strong> Learners with basic proficiency in Python programming, a foundational understanding of RF principles (frequency, modulation), and a basic grasp of Machine Learning concepts (training, classification).</p>\n<hr>\n<p><strong>Module 1: Foundations: RF, SDRs, and the AI Spectrum Landscape</strong></p>\n<ul>\n<li><strong>Module Objective:</strong> Understand the fundamental concepts of Radio Frequency communications, the capabilities of Software-Defined Radios, and the growing role of Artificial Intelligence in spectrum analysis, identifying the critical intersection where vulnerabilities arise.</li>\n<li><strong>Essential Subtopics:</strong><ul>\n<li>Introduction to the Electromagnetic Spectrum and RF Signals.</li>\n<li>Basic Signal Properties: Frequency, Amplitude, Phase, Modulation types (AM, FM, FSK, PSK).</li>\n<li>What is a Software-Defined Radio (SDR)? Architecture and capabilities (RX, TX, bandwidth, sample rate).</li>\n<li>Overview of AI applications in RF: Spectrum Sensing, Signal Classification (Modulation Recognition), Anomaly Detection, Jamming Detection.</li>\n<li>Why AI in RF is a target: Reliance on data patterns, potential for deception, impact of failure in critical applications.</li>\n<li>Introduction to Adversarial Machine Learning (AML) concept: The idea of crafted inputs to fool AI.</li>\n<li>Ethical and Legal Considerations: Responsible disclosure, legal boundaries of RF transmission.</li>\n<li>Setting up the Development Environment: SDR drivers, Python, essential libraries (NumPy, SciPy, Matplotlib).</li>\n</ul>\n</li>\n<li><strong>Suggested Resources/Prerequisites:</strong><ul>\n<li>Basic Python programming skills.</li>\n<li>Familiarity with command line interfaces.</li>\n<li>Access to an SDR (e.g., RTL-SDR for RX, HackRF One, LimeSDR, ADALM-PLUTO for RX/TX).</li>\n<li>Recommended Reading: &quot;RF &amp; Wireless Technologies&quot; basics, introductory SDR guides.</li>\n</ul>\n</li>\n<li><strong>Module Project/Exercise:</strong><ul>\n<li>Set up the SDR development environment.</li>\n<li>Use the SDR to capture a segment of the local RF spectrum (e.g., FM broadcast band, ISM band noise).</li>\n<li>Visualize the captured spectrum data (e.g., using <code>matplotlib</code> or <code>inspectrum</code>) to identify basic signal types or noise floor.</li>\n<li><em>Contribution to Capstone:</em> Confirms SDR setup and basic data capture capability.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Module 2: SDR Mastery: Capture, Analysis, and Generation</strong></p>\n<ul>\n<li><strong>Module Objective:</strong> Become proficient in using SDRs to capture, analyze, and generate arbitrary RF signals, laying the practical groundwork for handling RF data and transmitting crafted signals.</li>\n<li><strong>Essential Subtopics:</strong><ul>\n<li>SDR Software Ecosystem: Gnu Radio, <code>pyrtlsdr</code>, <code>pyhackrf</code>, <code>pysdr</code>.</li>\n<li>Working with IQ Data: Understanding In-phase and Quadrature components.</li>\n<li>Capturing RF Signals: Techniques for recording raw IQ data from the air.</li>\n<li>Signal Analysis with SDRs: Spectrograms, waterfall plots, frequency analysis.</li>\n<li>Basic Signal Generation: Creating simple waveforms (sine waves, pulses) and transmitting them (requires TX-capable SDR).</li>\n<li>Modulation/Demodulation Basics in Software.</li>\n<li>Storing and Managing RF Datasets (IQ files).</li>\n</ul>\n</li>\n<li><strong>Suggested Resources/Prerequisites:</strong><ul>\n<li>Completed Module 1.</li>\n<li>Access to an SDR (TX capability needed for generation parts).</li>\n<li>Introduction to Digital Signal Processing (DSP) concepts (optional but helpful).</li>\n</ul>\n</li>\n<li><strong>Module Project/Exercise:</strong><ul>\n<li>Capture a known signal source (e.g., a local pager, a simple FSK device, or generate a tone with one SDR and capture with another).</li>\n<li>Analyze the captured signal&#39;s properties (frequency, bandwidth, modulation type if simple).</li>\n<li>Generate and transmit a simple, low-power test tone or modulated signal using the SDR (ensure legal limits and test environment).</li>\n<li><em>Contribution to Capstone:</em> Develops the core SDR data handling and transmission skills required for the attack system.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Module 3: AI/ML for Spectrum: Classification &amp; Feature Engineering</strong></p>\n<ul>\n<li><strong>Module Objective:</strong> Understand how RF signals are processed and represented for Artificial Intelligence models, and learn the fundamentals of training a basic AI model for signal classification.</li>\n<li><strong>Essential Subtopics:</strong><ul>\n<li>Review of relevant ML concepts: Supervised Learning, Classification, Training Data, Labels, Features, Models.</li>\n<li>Representing RF Data for ML:<ul>\n<li>Raw IQ data.</li>\n<li>Time-series features (statistical properties).</li>\n<li>Frequency-domain features (FFT-based).</li>\n<li>Time-Frequency representations (Spectrograms) - often used with CNNs.</li>\n</ul>\n</li>\n<li>Selecting and Engineering Features for RF Signals.</li>\n<li>Introduction to Neural Networks suitable for RF (e.g., CNNs for Spectrograms, LSTMs for IQ data).</li>\n<li>Using ML Frameworks: TensorFlow or PyTorch basics for data loading, model definition, training loop.</li>\n<li>Evaluating Classifier Performance: Accuracy, Confusion Matrix.</li>\n<li><strong>Case Study:</strong> How AI is used in military/intelligence signal identification (SIGINT).</li>\n</ul>\n</li>\n<li><strong>Suggested Resources/Prerequisites:</strong><ul>\n<li>Completed Module 2.</li>\n<li>Basic understanding of Machine Learning concepts.</li>\n<li>Familiarity with NumPy and Matplotlib.</li>\n<li>Introduction to TensorFlow or PyTorch (tutorials).</li>\n</ul>\n</li>\n<li><strong>Module Project/Exercise:</strong><ul>\n<li>Using pre-recorded RF signal data (or data captured in M2), process it into a suitable format for ML (e.g., generate spectrogram images).</li>\n<li>Create a small labeled dataset for 2-3 distinct signal types.</li>\n<li>Train a simple neural network (e.g., a small CNN) to classify these signal types.</li>\n<li>Evaluate the trained model&#39;s accuracy on a test set.</li>\n<li><em>Contribution to Capstone:</em> Creates the <em>target</em> AI system that will be attacked in later modules.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Module 4: Building the Target: An AI-Driven RF Classifier System</strong></p>\n<ul>\n<li><strong>Module Objective:</strong> Integrate SDR data capture with the trained AI model to build a functional, albeit basic, real-time or near-real-time RF signal classification system. This system will serve as the primary target for adversarial attacks.</li>\n<li><strong>Essential Subtopics:</strong><ul>\n<li>Connecting SDR capture pipeline to ML inference pipeline.</li>\n<li>Data Buffering and Processing for continuous classification.</li>\n<li>Handling real-world RF data variability.</li>\n<li>Loading pre-trained models for inference.</li>\n<li>Implementing the classification logic based on SDR input.</li>\n<li>Visualization of classification results overlaid on spectrum data.</li>\n<li>Setting up a controlled test environment (e.g., using attenuators, fixed antennas) for repeatable experiments.</li>\n</ul>\n</li>\n<li><strong>Suggested Resources/Prerequisites:</strong><ul>\n<li>Completed Module 3 (trained model).</li>\n<li>Proficiency in using SDR libraries (M2).</li>\n<li>Basic understanding of system integration.</li>\n</ul>\n</li>\n<li><strong>Module Project/Exercise:</strong><ul>\n<li>Write a script that uses the SDR to capture RF data chunks.</li>\n<li>Process each chunk (e.g., generate a spectrogram) and feed it to the AI model trained in M3.</li>\n<li>Output or display the model&#39;s classification result for each chunk.</li>\n<li>Test the system with known clean signals to verify it classifies them correctly.</li>\n<li><em>Contribution to Capstone:</em> Completes the <em>target</em> system component of the functional clone.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Module 5: The Adversarial Mindset: Introduction to AI Attacks</strong></p>\n<ul>\n<li><strong>Module Objective:</strong> Dive into the theory and common techniques of Adversarial Machine Learning, understanding <em>why</em> AI models are vulnerable and the different goals of an attacker.</li>\n<li><strong>Essential Subtopics:</strong><ul>\n<li>What are Adversarial Examples? Small perturbations with large impacts.</li>\n<li>The Geometry of Adversarial Attacks: Decision boundaries and model linearity.</li>\n<li>Attack Goals: Targeted Misclassification (forcing a specific wrong class), Untargeted Misclassification (any wrong class), Evasion (appearing as noise or unknown).</li>\n<li>Threat Models: White-box (attacker knows model details), Black-box (attacker only interacts with model input/output).</li>\n<li>Common White-box Attack Techniques:<ul>\n<li>Fast Gradient Sign Method (FGSM).</li>\n<li>Projected Gradient Descent (PGD).</li>\n<li>DeepFool.</li>\n</ul>\n</li>\n<li>Common Black-box Attack Techniques: Transferability of adversarial examples, Query-based attacks.</li>\n<li>Metrics for Adversarial Attacks: Perturbation size (Lp norms), Success Rate.</li>\n<li><strong>Case Study:</strong> Adversarial attacks in the image domain (e.g., stopping signs mistaken for speed limits) and their implications.</li>\n</ul>\n</li>\n<li><strong>Suggested Resources/Prerequisites:</strong><ul>\n<li>Completed Module 4.</li>\n<li>Solid understanding of basic ML concepts and model training (M3).</li>\n<li>Familiarity with gradient-based optimization (basic calculus helpful but not strictly required).</li>\n</ul>\n</li>\n<li><strong>Module Project/Exercise:</strong><ul>\n<li>Implement the FGSM attack algorithm using a chosen ML framework (TensorFlow/PyTorch).</li>\n<li>Apply the FGSM attack to a <em>pre-trained image classifier</em> (e.g., on MNIST or CIFAR-10) to generate adversarial images.</li>\n<li>Verify that the generated adversarial images successfully fool the image classifier.</li>\n<li><em>Contribution to Capstone:</em> Builds the foundational code and understanding for generating adversarial perturbations.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Module 6: Crafting RF Adversarial Examples (Digital)</strong></p>\n<ul>\n<li><strong>Module Objective:</strong> Adapt general adversarial machine learning techniques to the RF domain, focusing on generating adversarial <em>data</em> (IQ or spectrograms) that can deceive the target RF classifier <em>before</em> over-the-air transmission.</li>\n<li><strong>Essential Subtopics:</strong><ul>\n<li>Applying FGSM, PGD, etc., to RF data representations (IQ, Spectrograms).</li>\n<li>Challenges of RF Perturbations: Physical constraints (power limits, bandwidth limits), maintaining signal structure.</li>\n<li>Generating Adversarial Perturbations in IQ domain vs. Spectrogram domain.</li>\n<li>Constraining the Adversarial Perturbation: L-infinity, L-2 norms translated to power/amplitude constraints.</li>\n<li>Targeted vs. Untargeted Attacks on RF Classifiers (e.g., make WiFi look like LTE, or just make WiFi look like anything but WiFi).</li>\n<li>Implementing Adversarial Generation Code using the target RF classifier model (from M4).</li>\n<li>Evaluating Digital Adversarial Examples: Feeding the generated data directly into the loaded model and checking the output.</li>\n</ul>\n</li>\n<li><strong>Suggested Resources/Prerequisites:</strong><ul>\n<li>Completed Module 5 (AML theory and FGSM implementation).</li>\n<li>Completed Module 4 (the target RF classifier model).</li>\n<li>Strong Python and ML framework skills.</li>\n</ul>\n</li>\n<li><strong>Module Project/Exercise:</strong><ul>\n<li>Take the RF classifier model trained in M4.</li>\n<li>Implement an adversarial attack (e.g., FGSM or a simple variant adapted for RF data).</li>\n<li>Generate adversarial versions of clean RF signal data points (e.g., take a clean WiFi sample, generate an adversarial version).</li>\n<li>Digitally feed the generated adversarial data into the <em>trained</em> model and verify that it is misclassified or evaded.</li>\n<li><em>Contribution to Capstone:</em> Creates the core adversarial signal <em>generation</em> component (digital stage).</li>\n</ul>\n</li>\n</ul>\n<p><strong>Module 7: Over-the-Air Attacks: Transmitting Adversarial RF</strong></p>\n<ul>\n<li><strong>Module Objective:</strong> Bridge the gap between digital adversarial examples and physical RF transmission, addressing the challenges of over-the-air effects and executing adversarial attacks using SDRs in a controlled environment.</li>\n<li><strong>Essential Subtopics:</strong><ul>\n<li>Challenges of Over-the-Air AML: Channel effects (fading, multipath), Noise, Interference, Non-linearities in TX/RX hardware, Synchronization.</li>\n<li>Translating Digital Adversarial Examples to Analog Waveforms for Transmission.</li>\n<li>Using SDRs for Adversarial Transmission: Loading generated IQ data into the SDR TX buffer.</li>\n<li>Power Control and Attenuation: Ensuring the adversarial signal is received at the target system with the intended perturbation level relative to the clean signal.</li>\n<li>Setting up a Controlled Over-the-Air Testbed (e.g., using cables, attenuators, or closely spaced antennas in a shielded environment if possible).</li>\n<li>Executing the Attack: Transmitting the adversarial signal and simultaneously receiving it (or having a separate receiver) to feed into the target AI system (from M4).</li>\n<li>Evaluating Over-the-Air Attack Success: Observing the target system&#39;s misclassification rate.</li>\n<li>Real-world Considerations and Tuning: Adjusting power, timing, and perturbation based on observed effects.</li>\n<li><strong>Case Study:</strong> Potential attacks on AI-driven cognitive radio systems or drone communication links.</li>\n</ul>\n</li>\n<li><strong>Suggested Resources/Prerequisites:</strong><ul>\n<li>Completed Module 6 (digital adversarial examples).</li>\n<li>Completed Module 4 (target RF classifier system).</li>\n<li>TX-capable SDR.</li>\n<li>Basic understanding of RF propagation (optional).</li>\n</ul>\n</li>\n<li><strong>Module Project/Exercise:</strong><ul>\n<li>Set up the over-the-air testbed with a TX SDR and an RX SDR (connected to the M4 classifier).</li>\n<li>Generate an adversarial signal for a clean signal using the method from M6.</li>\n<li>Transmit the <em>clean</em> signal and verify the target classifier correctly identifies it.</li>\n<li>Transmit the <em>adversarial</em> signal (potentially simultaneously with the clean signal, or as a modified version of the clean signal if attacking a live stream concept).</li>\n<li>Observe and record the target classifier&#39;s output when receiving the adversarial signal.</li>\n<li>Demonstrate successful misclassification or evasion over the air.</li>\n<li><em>Contribution to Capstone:</em> Implements the critical over-the-air execution phase of the functional clone.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Module 8: Capstone Project: Building the Adversarial RF Lab &amp; Future Directions</strong></p>\n<ul>\n<li><strong>Module Objective:</strong> Integrate all previously built components into a cohesive system that demonstrates a functional adversarial RF attack against an AI classifier, and explore potential defenses and future research directions.</li>\n<li><strong>Essential Subtopics:</strong><ul>\n<li>Integrating the SDR capture (M2/M4), AI classification (M4), Adversarial Generation (M6), and SDR Transmission (M7) components into a single workflow or script.</li>\n<li>Refining the Attack: Optimizing perturbation parameters, exploring different attack types (if time permits), improving synchronization.</li>\n<li>Building the &quot;Functional Clone&quot;: Setting up the end-to-end system to demonstrate the attack.</li>\n<li>Evaluating the &quot;Clone&quot;: Documenting the attack success rate under different conditions.</li>\n<li>Potential Defense Strategies: Adversarial Training, Robust Architectures, Detection of Adversarial Examples, RF Fingerprinting.</li>\n<li>Limitations and Future Research: Black-box RF attacks, hardware-level vulnerabilities, attacks on other AI-driven RF tasks (regression, anomaly detection).</li>\n<li>Ethical Implications Revisited: Responsible research and disclosure.</li>\n<li>Presenting Findings and Demonstrating the Attack.</li>\n</ul>\n</li>\n<li><strong>Suggested Resources/Prerequisites:</strong><ul>\n<li>Completed all previous modules.</li>\n<li>All code components developed throughout the course.</li>\n<li>Access to SDRs for the final demonstration.</li>\n</ul>\n</li>\n<li><strong>Module Project/Exercise:</strong><ul>\n<li>Assemble the code from M4, M6, and M7 into a single executable system (e.g., a Python script).</li>\n<li>Set up the physical testbed.</li>\n<li>Execute the end-to-end adversarial RF attack, demonstrating how a legitimate signal can be misclassified or an illegitimate signal can evade detection by the AI system using SDR-generated adversarial perturbations.</li>\n<li>Document the system architecture, the attack methodology, the experimental setup, and the results.</li>\n<li>(Optional) Implement a simple defense mechanism and evaluate its effectiveness.</li>\n<li>(Optional) Explore variations or extensions (e.g., attacking a different signal type, trying a black-box approach if possible).</li>\n<li><em>Contribution to Capstone:</em> This <em>is</em> the capstone project, integrating all parts to achieve the overall course objective.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<p>This outline provides a structured path for learners to acquire both the theoretical knowledge and the practical skills needed to understand and execute adversarial RF attacks on AI systems using SDRs. The progression ensures that learners build a solid foundation before tackling the more complex adversarial concepts and real-world implementation challenges. The module projects are specifically designed as building blocks, culminating in the functional clone required by the capstone. Remember to emphasize hands-on work and experimentation throughout the course!</p>\n\n            </div>\n            <h2 class=\"module-list-heading\">Course Content</h2> <!-- Add heading for module list -->\n            <ul class=\"module-list\">\n        <li class=\"module-item\">\n            <!-- Link uses data attributes for JS handling -->\n            <a href=\"#module-1\" data-view=\"module-1\" data-module-order=\"1\">\n                <div class=\"card module-card\">\n                    <div class=\"module-card-content\">\n                        <div class=\"module-title-area\">\n                           <h3>Module 1: module_1</h3>\n                           <!-- Add description if available -->\n                           <!-- <p class=\"module-description\">module_1 Overview</p> -->\n                        </div>\n                        <div class=\"module-meta\">\n                            <span class=\"module-duration\">30min</span>\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-lock h-5 w-5 text-gray-500\"><rect width=\"18\" height=\"11\" x=\"3\" y=\"11\" rx=\"2\" ry=\"2\"/><path d=\"M7 11V7a5 5 0 0 1 10 0v4\"/></svg> <!-- Lock Icon -->\n                        </div>\n                    </div>\n                </div>\n            </a>\n        </li>\n        \n        <li class=\"module-item\">\n            <!-- Link uses data attributes for JS handling -->\n            <a href=\"#module-2\" data-view=\"module-2\" data-module-order=\"2\">\n                <div class=\"card module-card\">\n                    <div class=\"module-card-content\">\n                        <div class=\"module-title-area\">\n                           <h3>Module 2: module_2</h3>\n                           <!-- Add description if available -->\n                           <!-- <p class=\"module-description\">module_2 Overview</p> -->\n                        </div>\n                        <div class=\"module-meta\">\n                            <span class=\"module-duration\">30min</span>\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-lock h-5 w-5 text-gray-500\"><rect width=\"18\" height=\"11\" x=\"3\" y=\"11\" rx=\"2\" ry=\"2\"/><path d=\"M7 11V7a5 5 0 0 1 10 0v4\"/></svg> <!-- Lock Icon -->\n                        </div>\n                    </div>\n                </div>\n            </a>\n        </li>\n        \n        <li class=\"module-item\">\n            <!-- Link uses data attributes for JS handling -->\n            <a href=\"#module-3\" data-view=\"module-3\" data-module-order=\"3\">\n                <div class=\"card module-card\">\n                    <div class=\"module-card-content\">\n                        <div class=\"module-title-area\">\n                           <h3>Module 3: module_3</h3>\n                           <!-- Add description if available -->\n                           <!-- <p class=\"module-description\">module_3 Overview</p> -->\n                        </div>\n                        <div class=\"module-meta\">\n                            <span class=\"module-duration\">30min</span>\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-lock h-5 w-5 text-gray-500\"><rect width=\"18\" height=\"11\" x=\"3\" y=\"11\" rx=\"2\" ry=\"2\"/><path d=\"M7 11V7a5 5 0 0 1 10 0v4\"/></svg> <!-- Lock Icon -->\n                        </div>\n                    </div>\n                </div>\n            </a>\n        </li>\n        \n        <li class=\"module-item\">\n            <!-- Link uses data attributes for JS handling -->\n            <a href=\"#module-4\" data-view=\"module-4\" data-module-order=\"4\">\n                <div class=\"card module-card\">\n                    <div class=\"module-card-content\">\n                        <div class=\"module-title-area\">\n                           <h3>Module 4: module_4</h3>\n                           <!-- Add description if available -->\n                           <!-- <p class=\"module-description\">module_4 Overview</p> -->\n                        </div>\n                        <div class=\"module-meta\">\n                            <span class=\"module-duration\">30min</span>\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-lock h-5 w-5 text-gray-500\"><rect width=\"18\" height=\"11\" x=\"3\" y=\"11\" rx=\"2\" ry=\"2\"/><path d=\"M7 11V7a5 5 0 0 1 10 0v4\"/></svg> <!-- Lock Icon -->\n                        </div>\n                    </div>\n                </div>\n            </a>\n        </li>\n        \n        <li class=\"module-item\">\n            <!-- Link uses data attributes for JS handling -->\n            <a href=\"#module-5\" data-view=\"module-5\" data-module-order=\"5\">\n                <div class=\"card module-card\">\n                    <div class=\"module-card-content\">\n                        <div class=\"module-title-area\">\n                           <h3>Module 5: module_5</h3>\n                           <!-- Add description if available -->\n                           <!-- <p class=\"module-description\">module_5 Overview</p> -->\n                        </div>\n                        <div class=\"module-meta\">\n                            <span class=\"module-duration\">30min</span>\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-lock h-5 w-5 text-gray-500\"><rect width=\"18\" height=\"11\" x=\"3\" y=\"11\" rx=\"2\" ry=\"2\"/><path d=\"M7 11V7a5 5 0 0 1 10 0v4\"/></svg> <!-- Lock Icon -->\n                        </div>\n                    </div>\n                </div>\n            </a>\n        </li>\n        \n        <li class=\"module-item\">\n            <!-- Link uses data attributes for JS handling -->\n            <a href=\"#module-6\" data-view=\"module-6\" data-module-order=\"6\">\n                <div class=\"card module-card\">\n                    <div class=\"module-card-content\">\n                        <div class=\"module-title-area\">\n                           <h3>Module 6: module_6</h3>\n                           <!-- Add description if available -->\n                           <!-- <p class=\"module-description\">module_6 Overview</p> -->\n                        </div>\n                        <div class=\"module-meta\">\n                            <span class=\"module-duration\">30min</span>\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-lock h-5 w-5 text-gray-500\"><rect width=\"18\" height=\"11\" x=\"3\" y=\"11\" rx=\"2\" ry=\"2\"/><path d=\"M7 11V7a5 5 0 0 1 10 0v4\"/></svg> <!-- Lock Icon -->\n                        </div>\n                    </div>\n                </div>\n            </a>\n        </li>\n        \n        <li class=\"module-item\">\n            <!-- Link uses data attributes for JS handling -->\n            <a href=\"#module-7\" data-view=\"module-7\" data-module-order=\"7\">\n                <div class=\"card module-card\">\n                    <div class=\"module-card-content\">\n                        <div class=\"module-title-area\">\n                           <h3>Module 7: module_7</h3>\n                           <!-- Add description if available -->\n                           <!-- <p class=\"module-description\">module_7 Overview</p> -->\n                        </div>\n                        <div class=\"module-meta\">\n                            <span class=\"module-duration\">30min</span>\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-lock h-5 w-5 text-gray-500\"><rect width=\"18\" height=\"11\" x=\"3\" y=\"11\" rx=\"2\" ry=\"2\"/><path d=\"M7 11V7a5 5 0 0 1 10 0v4\"/></svg> <!-- Lock Icon -->\n                        </div>\n                    </div>\n                </div>\n            </a>\n        </li>\n        \n        <li class=\"module-item\">\n            <!-- Link uses data attributes for JS handling -->\n            <a href=\"#module-8\" data-view=\"module-8\" data-module-order=\"8\">\n                <div class=\"card module-card\">\n                    <div class=\"module-card-content\">\n                        <div class=\"module-title-area\">\n                           <h3>Module 8: module_8</h3>\n                           <!-- Add description if available -->\n                           <!-- <p class=\"module-description\">module_8 Overview</p> -->\n                        </div>\n                        <div class=\"module-meta\">\n                            <span class=\"module-duration\">30min</span>\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-lock h-5 w-5 text-gray-500\"><rect width=\"18\" height=\"11\" x=\"3\" y=\"11\" rx=\"2\" ry=\"2\"/><path d=\"M7 11V7a5 5 0 0 1 10 0v4\"/></svg> <!-- Lock Icon -->\n                        </div>\n                    </div>\n                </div>\n            </a>\n        </li>\n        </ul> <!-- Include the module list for Overview -->\n        </div>\n    ",
  "modules": {
    "module-1": "\n             <div class=\"card main-content-card\"> <!-- Added main-content-card class -->\n                <h1>Module 1: module_1</h1> <!-- Use module title here -->\n                <div class=\"markdown-content\">\n                    <p>Alright team, strap in! We&#39;re about to embark on a seriously cool journey into the heart of the RF spectrum, armed with software and a healthy dose of curiosity. This first module is all about building that rock-solid foundation. Think of it as learning the alphabet before you write a novel. We&#39;ll cover the basics of what makes radio signals tick, how our awesome Software-Defined Radios (SDRs) let us play with them, and why putting AI in charge of analyzing signals opens up fascinating new attack surfaces.</p>\n<p>Let&#39;s dive into Module 1!</p>\n<hr>\n<h2><strong>Module 1: Foundations: RF, SDRs, and the AI Spectrum Landscape</strong></h2>\n<p><strong>Welcome!</strong></p>\n<p>Hey everyone, and welcome to &quot;Adversarial RF Attacks on AI Spectrum Systems using SDRs&quot;! I&#39;m genuinely excited to guide you through this cutting-edge topic. We&#39;re going to bridge the worlds of radio frequencies, hacking, and artificial intelligence, and you&#39;ll gain the skills to understand and even replicate the core concepts behind adversarial attacks in the physical RF domain.</p>\n<p>This isn&#39;t just theoretical; we&#39;re building towards a functional &quot;clone&quot; of an adversarial system by the end of the course. And it all starts right here, with the fundamentals.</p>\n<p><strong>Module Objective:</strong></p>\n<p>By the end of this module, you will:</p>\n<ul>\n<li>Understand the fundamental concepts of Radio Frequency communications and the Electromagnetic Spectrum.</li>\n<li>Grasp the core capabilities and architecture of Software-Defined Radios (SDRs).</li>\n<li>Recognize the increasing role of Artificial Intelligence in analyzing the RF spectrum.</li>\n<li>Identify the critical intersection where AI&#39;s reliance on data meets the physical reality of RF, creating potential vulnerabilities.</li>\n<li>Be introduced to the concept of Adversarial Machine Learning (AML) in the RF context.</li>\n<li>Understand the crucial ethical and legal considerations when working with RF transmission.</li>\n<li>Have your development environment set up and ready to capture and visualize real-world RF data using an SDR.</li>\n</ul>\n<p><strong>Why This Module Matters:</strong></p>\n<p>You can&#39;t attack what you don&#39;t understand. This module provides the essential vocabulary and foundational knowledge in RF and SDRs. It also introduces <em>why</em> AI in this domain is interesting from an offensive security perspective, setting the stage for everything that follows. Think of it as learning the landscape and identifying the target building before planning your approach.</p>\n<hr>\n<h3><strong>Essential Subtopics Deep Dive:</strong></h3>\n<h4><strong>1. Introduction to the Electromagnetic Spectrum and RF Signals</strong></h4>\n<ul>\n<li><strong>What is RF?</strong> Radio Frequency is a part of the electromagnetic (EM) spectrum. Just like visible light, X-rays, or microwaves, RF energy travels in waves. The key difference is their frequency (how many wave cycles pass a point per second) and wavelength (the physical distance of one wave cycle).</li>\n<li><strong>The EM Spectrum:</strong> Imagine a giant rainbow of energy. At one end, you have very low frequencies (like power lines). As you move up, you get AM radio, FM radio, TV broadcasts, Wi-Fi, Bluetooth, cellular signals, satellite communications, radar, microwaves (for heating food!), infrared, visible light, ultraviolet, X-rays, and finally, gamma rays. RF generally sits below the infrared part of the spectrum.<ul>\n<li><strong>Key Relationship:</strong> The speed of light (<code>c</code>, approximately 300,000,000 meters/second) is constant for all EM waves in a vacuum. The relationship is <code>c = frequency (f) * wavelength (λ)</code>. This means higher frequencies have shorter wavelengths, and lower frequencies have longer wavelengths. This is important for antenna design and how signals propagate.</li>\n</ul>\n</li>\n<li><strong>RF Signals:</strong> These are time-varying electromagnetic waves used to carry information wirelessly. They start as electrical signals, are converted to EM waves by a transmitting antenna, travel through the air, and are converted back to electrical signals by a receiving antenna.</li>\n</ul>\n<h4><strong>2. Basic Signal Properties: Frequency, Amplitude, Phase, Modulation Types</strong></h4>\n<p>Think of a pure, simple radio wave like a single musical note.</p>\n<ul>\n<li><strong>Frequency (Hz):</strong> This is the <em>pitch</em> of our note. It&#39;s the number of cycles the wave completes per second. Measured in Hertz (Hz), Kilohertz (kHz), Megahertz (MHz), Gigahertz (GHz). A signal centered at 99.5 MHz is an FM radio station. A signal at 2.4 GHz is likely Wi-Fi or Bluetooth.</li>\n<li><strong>Amplitude (V or Power):</strong> This is the <em>loudness</em> of our note. It&#39;s the height or intensity of the wave. Higher amplitude usually means a stronger signal, easier to receive. Measured in Volts (V) or often discussed in terms of Power (Watts, milliwatts, dBm).</li>\n<li><strong>Phase (degrees or radians):</strong> This describes the position of the wave within a single cycle at a specific point in time relative to a reference. Think of it as the <em>timing</em> or <em>starting point</em> of the wave&#39;s oscillation. If you have two waves of the same frequency, their phase difference tells you how &quot;offset&quot; they are from each other.</li>\n<li><strong>Modulation:</strong> This is how we take raw information (like voice, music, or digital data) and <em>encode</em> it onto our carrier wave (the basic frequency). We change one or more of the carrier wave&#39;s properties (Amplitude, Frequency, or Phase) in a controlled way according to the information we want to send.<ul>\n<li><strong>Amplitude Modulation (AM):</strong> The amplitude of the carrier wave is varied according to the information signal. Simpler to implement, but more susceptible to noise. (Think classic AM radio).</li>\n<li><strong>Frequency Modulation (FM):</strong> The frequency of the carrier wave is varied according to the information signal. More complex, but more resistant to noise. (Think FM radio).</li>\n<li><strong>Phase Modulation (PM):</strong> The phase of the carrier wave is varied according to the information signal. Often used in digital schemes.</li>\n<li><strong>Digital Modulation:</strong> Combinations and variations of AM, FM, and PM are used to encode digital bits (0s and 1s). Examples include Frequency Shift Keying (FSK), Phase Shift Keying (PSK), Quadrature Amplitude Modulation (QAM - combines AM and PSK), Orthogonal Frequency-Division Multiplexing (OFDM - used in Wi-Fi, LTE, 5G). We&#39;ll encounter these as the signals our AI tries to classify.</li>\n</ul>\n</li>\n</ul>\n<h4><strong>3. What is a Software-Defined Radio (SDR)? Architecture and Capabilities</strong></h4>\n<p>This is our primary tool!</p>\n<ul>\n<li><strong>Traditional Radio vs. SDR:</strong> A traditional radio (like the one in your car) uses dedicated, specialized electronic circuits (hardware) for each step: tuning to a frequency, filtering out others, demodulating the signal. To change what kind of signal it can receive (e.g., from AM to FM), you need different hardware circuits.</li>\n<li><strong>SDR Philosophy:</strong> An SDR moves as much of the signal processing as possible from dedicated hardware into <em>software</em> running on a general-purpose processor (like your computer&#39;s CPU).</li>\n<li><strong>Basic Architecture:</strong><ol>\n<li><strong>Antenna:</strong> Captures RF energy from the air.</li>\n<li><strong>RF Front End:</strong> Basic amplification, filtering, and frequency shifting (downconversion for RX, upconversion for TX) to bring the signal into a range that can be digitized.</li>\n<li><strong>Analog-to-Digital Converter (ADC) / Digital-to-Analog Converter (DAC):</strong> This is the crucial step. The ADC converts the analog RF signal (or a downconverted version) into a stream of digital numbers (for RX). The DAC converts digital numbers back into an analog signal (for TX).</li>\n<li><strong>Digital Processing:</strong> This is where the &quot;software-defined&quot; magic happens. A powerful chip (often an FPGA or a dedicated DSP) or your computer&#39;s CPU processes these digital samples. This is where tuning, filtering, demodulation, modulation, etc., are performed <em>in software</em>.</li>\n</ol>\n</li>\n<li><strong>Key Capabilities:</strong><ul>\n<li><strong>Receive (RX):</strong> Capture a specific range of frequencies.</li>\n<li><strong>Transmit (TX):</strong> Generate and broadcast signals (requires a TX-capable SDR).</li>\n<li><strong>Bandwidth:</strong> The <em>range</em> of frequencies the SDR can capture or transmit <em>simultaneously</em>. A larger bandwidth means you can see or interact with more of the spectrum at once. Limited by the ADC/DAC speed.</li>\n<li><strong>Sample Rate:</strong> The number of digital samples taken per second from the analog signal. According to the Nyquist theorem, the sample rate must be at least twice the bandwidth you want to capture. Often, sample rate and bandwidth are closely related or used interchangeably in SDR specs, but technically Bandwidth is the usable frequency range, Sample Rate is how fast you&#39;re digitizing.</li>\n<li><strong>Frequency Range:</strong> The range of frequencies the hardware&#39;s front end can tune to.</li>\n</ul>\n</li>\n<li><strong>Why SDRs are Powerful for this Course:</strong> They give us the flexibility to programmatically capture <em>any</em> signal within their range, analyze its raw digital representation (IQ data), generate <em>arbitrary</em> digital waveforms, and then transmit them. This is <em>exactly</em> what we need to build and test adversarial RF attacks.</li>\n</ul>\n<h4><strong>4. Overview of AI Applications in RF: Spectrum Sensing, Signal Classification, Anomaly Detection, Jamming Detection</strong></h4>\n<p>The RF spectrum is becoming incredibly crowded and dynamic. Manually monitoring and understanding everything happening is impossible. This is where AI shines.</p>\n<ul>\n<li><strong>Spectrum Sensing:</strong> AI can quickly scan wide swaths of spectrum to detect the presence of signals, identify occupied frequencies, and find available &quot;white space.&quot; Used in cognitive radio to dynamically hop frequencies.</li>\n<li><strong>Signal Classification (Modulation Recognition):</strong> Given a slice of captured RF data, AI can analyze its characteristics (modulation type, bandwidth, symbol rate) to identify <em>what kind</em> of signal it is. Is it Wi-Fi? Bluetooth? LoRa? FM broadcast? A military signal? <em>This is the primary target for our adversarial attacks.</em></li>\n<li><strong>Anomaly Detection:</strong> AI can learn what &quot;normal&quot; spectrum activity looks like and flag anything unusual – a signal appearing where it shouldn&#39;t, a signal with strange characteristics, or unexpected interference.</li>\n<li><strong>Jamming Detection:</strong> A specific type of anomaly detection focused on identifying intentional interference designed to disrupt communication. AI can potentially differentiate jamming from natural noise or interference.</li>\n</ul>\n<p><strong>Case Study Idea:</strong> Imagine a battlefield scenario. AI is used to monitor enemy radio communications. It classifies signals to identify different units or types of activity. Accurate classification is critical for situational awareness.</p>\n<h4><strong>5. Why AI in RF is a Target: Reliance on Data Patterns, Potential for Deception, Impact of Failure in Critical Applications</strong></h4>\n<p>This is the core motivation for the course! Why would someone <em>want</em> to attack an AI system analyzing RF?</p>\n<ul>\n<li><strong>Reliance on Data Patterns:</strong> AI models, especially deep learning models, learn complex patterns from the data they are trained on. They become very good at identifying signals that look like their training data. However, they can be surprisingly brittle when presented with inputs that are slightly, but specifically, different.</li>\n<li><strong>Potential for Deception (Adversarial Examples):</strong> Just as you can craft an image that looks like a cat to a human but is classified as a dog by an AI, you can potentially craft an RF signal that sounds/looks like one thing but is classified as another by an AI.<ul>\n<li><strong>Example Scenario:</strong> Make a malicious signal look like benign noise. Make an enemy signal look like a friendly signal. Make a critical signal look like something unimportant.</li>\n</ul>\n</li>\n<li><strong>Impact of Failure in Critical Applications:</strong> As AI is deployed in more critical RF systems (military comms, air traffic control, critical infrastructure monitoring, autonomous systems), fooling the AI has increasingly severe consequences. Misclassifying a signal could lead to missed threats, incorrect decisions, or system failures. This raises the stakes significantly compared to fooling an image classifier on social media.</li>\n</ul>\n<h4><strong>6. Introduction to Adversarial Machine Learning (AML) Concept: The Idea of Crafted Inputs to Fool AI</strong></h4>\n<p>We touched on this, but let&#39;s formalize it slightly.</p>\n<ul>\n<li><strong>The Core Idea:</strong> AML is the study of vulnerabilities in machine learning models to malicious inputs designed to trick them. An &quot;adversarial example&quot; is a specially crafted input (like an image, text, or in our case, an RF signal representation) that is very similar to a legitimate input but causes the model to make a wrong prediction.</li>\n<li><strong>Not Random Noise:</strong> This is key. Adversarial perturbations are <em>calculated</em>. They often exploit the linearity or high dimensionality of the model&#39;s decision boundaries. Adding random noise might <em>reduce</em> accuracy, but an adversarial perturbation is designed to achieve a <em>specific</em> misclassification or evasion with minimal changes to the original input.</li>\n<li><strong>Our Goal:</strong> In this course, we want to generate adversarial <em>RF signals</em> that, when processed by an AI classifier, cause it to misidentify the signal type.</li>\n</ul>\n<h4><strong>7. Ethical and Legal Considerations: Responsible Disclosure, Legal Boundaries of RF Transmission</strong></h4>\n<p><strong>This is non-negotiable.</strong> We are working with the physical airwaves, which are a regulated public resource.</p>\n<ul>\n<li><strong>Legality:</strong> Transmitting radio signals is heavily regulated by government bodies (like the FCC in the US, Ofcom in the UK, etc.).<ul>\n<li>You <em>must</em> operate within legal limits. This means:<ul>\n<li>Using authorized frequencies (e.g., ISM bands like 2.4 GHz or 915 MHz, <em>if</em> your SDR supports them and you follow power restrictions).</li>\n<li>Operating at legal power levels (often very low for unlicensed bands).</li>\n<li><em>Never</em> interfering with licensed or critical services (emergency services, air traffic control, commercial broadcasts, etc.).</li>\n</ul>\n</li>\n<li><strong>Jamming is Illegal:</strong> Intentionally interfering with communications is against the law in most places. Our goal is <em>not</em> to jam, but to <em>deceive</em> an AI classifier. While generating adversarial signals involves transmission, it must be done in a way that <em>only</em> affects your controlled test environment and does not cause harmful interference.</li>\n</ul>\n</li>\n<li><strong>Ethical Responsibility:</strong><ul>\n<li><strong>Responsible Research:</strong> Your work should aim to understand vulnerabilities to improve security, not to cause harm.</li>\n<li><strong>Controlled Environment:</strong> Whenever possible, use cables, attenuators, or shielded enclosures (like a Faraday cage) for transmission testing to prevent signals from radiating and potentially causing interference.</li>\n<li><strong>Low Power:</strong> When over-the-air testing is necessary and legal, use the lowest possible power.</li>\n<li><strong>Responsible Disclosure:</strong> If you find significant vulnerabilities in real-world systems, follow responsible disclosure practices (contact the vendor/owner privately) rather than making them public immediately.</li>\n</ul>\n</li>\n<li><strong>In this course:</strong> We will emphasize using <em>controlled test environments</em> (cables, attenuators) or operating strictly within legal, low-power, unlicensed bands for transmission exercises. We are building a <em>functional clone</em> of the attack <em>concept</em>, not building a tool for malicious activity.</li>\n</ul>\n<h4><strong>8. Setting up the Development Environment: SDR Drivers, Python, Essential Libraries</strong></h4>\n<p>Alright, let&#39;s get our hands dirty and set up our workspace.</p>\n<ul>\n<li><strong>Hardware:</strong> You need an SDR.<ul>\n<li><strong>RTL-SDR:</strong> Excellent and cheap for RX only. Great for starting with capture and analysis.</li>\n<li><strong>HackRF One, LimeSDR, ADALM-PLUTO:</strong> More expensive, but capable of both RX and TX. You&#39;ll need one of these (or similar) for the later TX modules. <em>For Module 1, an RTL-SDR is sufficient.</em></li>\n</ul>\n</li>\n<li><strong>Operating System:</strong> Linux (Ubuntu, Kali, etc.) is generally the most common and best-supported environment for SDR work, especially with open-source tools. macOS also works well. Windows is possible but can sometimes require more effort for driver setup.</li>\n<li><strong>Python:</strong> We&#39;ll use Python for scripting our SDR control, data processing, and AI model interaction. Install Python 3 if you don&#39;t have it. Use a virtual environment (<code>venv</code> or <code>conda</code>) to keep your project dependencies clean.</li>\n<li><strong>Essential Libraries:</strong><ul>\n<li><code>numpy</code>: For numerical operations, especially handling IQ data arrays and performing FFTs.</li>\n<li><code>scipy</code>: Contains various scientific tools, including signal processing functions that might be useful.</li>\n<li><code>matplotlib</code>: For plotting and visualizing spectrum data, spectrograms, etc.</li>\n<li><code>pyrtlsdr</code> (or <code>pyhackrf</code>, <code>pysdr</code>): Python libraries to interface with your specific SDR hardware. <code>pyrtlsdr</code> is the go-to for RTL-SDR. <code>pysdr</code> offers a potentially cleaner abstraction layer over different SDRs.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Step-by-Step Setup Guide:</strong></p>\n<ol>\n<li><strong>Install Python 3 and pip:</strong> Most modern OS have Python 3 pre-installed. Check with <code>python3 --version</code> and <code>pip3 --version</code>. If not installed, follow guides for your specific OS.</li>\n<li><strong>Create a Virtual Environment (Recommended):</strong><pre><code class=\"language-bash\">python3 -m venv sdr_env\nsource sdr_env/bin/activate # On Linux/macOS\n# On Windows command prompt: sdr_env\\Scripts\\activate.bat\n# On Windows PowerShell: sdr_env\\Scripts\\Activate.ps1\n</code></pre>\nYou should see <code>(sdr_env)</code> at the start of your command prompt. This means you&#39;re inside the virtual environment.</li>\n<li><strong>Install SDR Drivers:</strong> This step is OS-dependent.<ul>\n<li><strong>Linux (Ubuntu/Debian):</strong> <code>sudo apt update &amp;&amp; sudo apt install librtlsdr-dev</code> (for RTL-SDR). You might need to blacklist the default DVB-T drivers so <code>librtlsdr</code> can access the device. Create a file like <code>/etc/modprobe.d/blacklist-rtl.conf</code> and add <code>blacklist dvb_usb_rtl28xxu</code> and <code>blacklist rtl2832</code>. Run <code>sudo rmmod dvb_usb_rtl28xxu rtl2832</code> and unplug/replug the SDR.</li>\n<li><strong>macOS:</strong> Use Homebrew: <code>brew install librtlsdr</code>.</li>\n<li><strong>Windows:</strong> This is trickier. You usually need <code>zadig</code> to replace the default Windows driver with a WinUSB driver. Search for guides specific to your SDR and Zadig.</li>\n<li><em>Verification:</em> Once drivers are installed, plug in your SDR and try a command-line tool like <code>rtl_test</code> (if using RTL-SDR). If it runs and shows stats without errors, your drivers are likely working.</li>\n</ul>\n</li>\n<li><strong>Install Python Libraries:</strong> While <em>inside</em> your virtual environment:<pre><code class=\"language-bash\">pip install numpy matplotlib scipy pyrtlsdr # Add pyhackrf or pysdr if needed for your hardware\n</code></pre>\n</li>\n<li><strong>Verify Python Libraries:</strong> Open a Python interpreter or script <em>within your virtual environment</em> and try importing the libraries:<pre><code class=\"language-python\">import numpy\nimport matplotlib.pyplot as plt\nimport scipy\nfrom rtlsdr import RtlSdr # Or from hackrf import Hackrf, etc.\n\nprint(&quot;Libraries imported successfully!&quot;)\n</code></pre>\nIf no errors appear, you&#39;re good to go!</li>\n</ol>\n<hr>\n<h3><strong>Suggested Resources/Prerequisites:</strong></h3>\n<ul>\n<li><strong>Basic Python:</strong> You should be comfortable writing simple scripts, using variables, loops, lists, and functions. Online tutorials like Codecademy, Coursera, or the official Python documentation are great.</li>\n<li><strong>Command Line:</strong> Basic navigation (<code>cd</code>, <code>ls</code>/<code>dir</code>), running scripts, installing packages (<code>pip</code>, <code>apt</code>, <code>brew</code>).</li>\n<li><strong>Access to an SDR:</strong> As mentioned, RTL-SDR (RX only) is the minimum for this module. A TX-capable SDR is needed for later modules.</li>\n<li><strong>Recommended Reading:</strong><ul>\n<li>&quot;RF &amp; Wireless Technologies&quot; basics: Search for introductory articles or videos on frequency, wavelength, amplitude, phase, and modulation. The ARRL Handbook has great foundational chapters, though it&#39;s extensive.</li>\n<li>Introductory SDR guides: The documentation for your specific SDR (e.g., the <code>pyrtlsdr</code> documentation, HackRF wiki) often has setup and basic usage examples. Michael Ossmann&#39;s SDR tutorials are excellent.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>Module Project/Exercise: Capture, Visualize, and Identify</strong></h3>\n<p>This exercise is your first hands-on interaction with the RF spectrum using your SDR and Python. It confirms your setup works and gives you a taste of seeing real signals.</p>\n<p><strong>Objective:</strong> Use your SDR and Python to capture a segment of the RF spectrum, perform a basic frequency analysis, and visualize the results.</p>\n<p><strong>Steps:</strong></p>\n<ol>\n<li><strong>Ensure your SDR is connected</strong> and drivers are working (e.g., run <code>rtl_test</code> if using RTL-SDR).</li>\n<li><strong>Activate your Python virtual environment</strong> (<code>source sdr_env/bin/activate</code>).</li>\n<li><strong>Write a Python script</strong> using <code>pyrtlsdr</code> (or your SDR library) to:<ul>\n<li>Import necessary libraries (<code>RtlSdr</code>, <code>numpy</code>, <code>matplotlib.pyplot</code>).</li>\n<li>Create an SDR object (<code>sdr = RtlSdr()</code>).</li>\n<li>Configure the SDR:<ul>\n<li>Set the center frequency (<code>sdr.center_freq = ...</code>). Choose a frequency range likely to have signals, like the FM broadcast band (around 88-108 MHz), or a known ISM band (like 915 MHz or 2.4 GHz, though 2.4 GHz needs a different SDR than RTL-SDR). For RTL-SDR, FM broadcast or airband (108-137 MHz) are good starting points. Let&#39;s target FM broadcast around 98 MHz.</li>\n<li>Set the sample rate (<code>sdr.sample_rate = ...</code>). A common value for RTL-SDR is 2.048 MS/s (Mega Samples per second). This gives you a bandwidth of about 2 MHz.</li>\n<li>Set the gain (<code>sdr.gain = &#39;auto&#39;</code> or a specific value). Start with auto or a moderate value.</li>\n</ul>\n</li>\n<li>Capture a block of samples (<code>samples = sdr.read_samples(...)</code>). Start with a reasonable number, maybe 256k or 512k samples.</li>\n<li>Close the SDR (<code>sdr.close()</code>).</li>\n<li>Perform a Fast Fourier Transform (FFT) on the samples to move from the time domain to the frequency domain. This shows the strength of different frequencies within the captured bandwidth.<ul>\n<li>Use <code>numpy.fft.fft(samples)</code>.</li>\n<li>Use <code>numpy.fft.fftshift</code> to center the zero frequency component.</li>\n</ul>\n</li>\n<li>Calculate the power spectral density (strength) from the FFT output (e.g., <code>power_spectrum = 20*np.log10(np.abs(fft_result))</code>). The <code>20*log10</code> converts to dB, a common unit for signal strength.</li>\n<li>Create a frequency axis corresponding to the FFT results. This maps the FFT bin index to an actual frequency. <code>freqs = np.fft.fftshift(np.fft.fftfreq(len(samples), 1/sdr.sample_rate)) + sdr.center_freq</code>.</li>\n<li>Plot the frequency spectrum using <code>matplotlib</code>. Plot <code>freqs</code> on the x-axis and <code>power_spectrum</code> on the y-axis. Label your axes!</li>\n<li>Add a title to your plot indicating the center frequency and sample rate.</li>\n<li>Display the plot (<code>plt.show()</code>).</li>\n</ul>\n</li>\n<li><strong>Run your script.</strong></li>\n<li><strong>Observe the plot:</strong><ul>\n<li>Can you see a peak around your chosen center frequency?</li>\n<li>Do you see other peaks? If you tuned to the FM band, you should see several peaks corresponding to radio stations.</li>\n<li>What does the noise floor look like? (The relatively flat, lower part of the graph).</li>\n<li>Try changing the center frequency and sample rate (staying within your SDR&#39;s limits) and see how the plot changes.</li>\n</ul>\n</li>\n</ol>\n<p>Here&#39;s a basic Python code template to get you started:</p>\n<pre><code class=\"language-python\">import numpy as np\nimport matplotlib.pyplot as plt\nfrom rtlsdr import RtlSdr # Make sure this matches your installed library\n\n# --- SDR Configuration ---\n# Adjust these values based on your SDR and desired capture\ncenter_freq = 98e6  # Example: 98 MHz (FM broadcast band)\nsample_rate = 2.048e6 # Example: 2.048 MS/s\nnum_samples = 256 * 1024 # Number of samples to capture (e.g., 256k)\n\n# --- SDR Setup and Capture ---\nsdr = None\ntry:\n    sdr = RtlSdr() # Or your SDR class (e.g., Hackrf())\n\n    sdr.center_freq = center_freq\n    sdr.sample_rate = sample_rate\n    # sdr.gain = &#39;auto&#39; # You can try &#39;auto&#39; or a fixed value like 20, 30, etc.\n    # If using HackRF, gain settings are different (vga_gain, lna_gain)\n    # sdr.vga_gain = 30\n    # sdr.lna_gain = 8\n\n    print(f&quot;Capturing {num_samples} samples at {center_freq/1e6:.2f} MHz with {sample_rate/1e6:.2f} MS/s...&quot;)\n    samples = sdr.read_samples(num_samples)\n    print(&quot;Capture complete.&quot;)\n\nexcept Exception as e:\n    print(f&quot;Error during SDR setup or capture: {e}&quot;)\n    print(&quot;Ensure your SDR is connected and drivers are correctly installed.&quot;)\n    samples = None # Ensure samples is None if capture failed\n\nfinally:\n    if sdr:\n        sdr.close()\n        print(&quot;SDR closed.&quot;)\n\n# --- Analysis and Visualization (if capture was successful) ---\nif samples is not None:\n    # Perform FFT\n    fft_result = np.fft.fft(samples)\n    fft_result_shifted = np.fft.fftshift(fft_result)\n\n    # Calculate power spectrum (in dB)\n    power_spectrum = 20 * np.log10(np.abs(fft_result_shifted))\n\n    # Create frequency axis\n    freqs = np.fft.fftshift(np.fft.fftfreq(len(samples), 1/sample_rate)) + center_freq\n\n    # Plot the spectrum\n    plt.figure(figsize=(12, 6))\n    plt.plot(freqs / 1e6, power_spectrum) # Plot frequency in MHz\n    plt.xlabel(&quot;Frequency (MHz)&quot;)\n    plt.ylabel(&quot;Power (dB)&quot;)\n    plt.title(f&quot;RF Spectrum around {center_freq/1e6:.2f} MHz (Sample Rate: {sample_rate/1e6:.2f} MS/s)&quot;)\n    plt.grid(True)\n    plt.show()\nelse:\n    print(&quot;Skipping analysis and visualization due to capture error.&quot;)\n</code></pre>\n<p><strong>Contribution to Capstone:</strong> Successfully completing this exercise confirms that your SDR is working with your Python environment and libraries. You&#39;ve demonstrated the ability to capture raw RF data and perform a basic frequency analysis, which is a fundamental building block for</p>\n\n                </div>\n             </div>\n         ",
    "module-2": "\n             <div class=\"card main-content-card\"> <!-- Added main-content-card class -->\n                <h1>Module 2: module_2</h1> <!-- Use module title here -->\n                <div class=\"markdown-content\">\n                    <p>Okay, class! Welcome back. If you&#39;ve just finished Module 1, you should have your SDR environment set up and have dipped your toes into capturing some raw RF data. You understand the <em>why</em> – why AI is a target in the spectrum, and why SDRs are our tool.</p>\n<p>Now, in Module 2, we shift gears squarely into the <em>how</em>. This is where we get our hands dirty with the fundamental skills of using Software-Defined Radios programmatically. Think of this as boot camp for your SDR. By the end, you&#39;ll be comfortable capturing signals, looking at them in different ways, creating your own signals, and even transmitting them (responsibly, of course!). These are the absolute core mechanics we&#39;ll build upon for every subsequent module, especially when we start generating and transmitting those sneaky adversarial signals.</p>\n<p>Let&#39;s dive in!</p>\n<hr>\n<h2>Module 2: SDR Mastery: Capture, Analysis, and Generation</h2>\n<p><strong>Module Objective:</strong> Become proficient in using SDRs to capture, analyze, and generate arbitrary RF signals, laying the practical groundwork for handling RF data and transmitting crafted signals.</p>\n<p><strong>Contribution to Capstone:</strong> Develops the core SDR data handling and transmission skills required for the attack system.</p>\n<h3>2.1 The SDR Software Ecosystem: Our Tools of the Trade</h3>\n<p>While Gnu Radio Companion (GRC) is a fantastic graphical tool for building complex signal processing flows, our focus in this course is on programmatic control using Python. This allows us to integrate SDR operations directly with data analysis, machine learning libraries, and eventually, our adversarial attack generation code.</p>\n<p>We&#39;ll primarily leverage powerful Python libraries built on top of SDR drivers:</p>\n<ul>\n<li><strong><code>pyrtlsdr</code>:</strong> A Python wrapper for <code>librtlsdr</code>, specifically for the ubiquitous and inexpensive RTL-SDR dongles. These are receive-only but excellent for spectrum sensing and signal capture. A great starting point!</li>\n<li><strong><code>pyhackrf</code>:</strong> A Python wrapper for <code>libhackrf</code>, for the HackRF One. This is a more capable, transmit-and-receive (half-duplex) SDR, essential for our signal generation and adversarial transmission later.</li>\n<li><strong><code>pysdr</code>:</strong> While less a driver wrapper and more a collection of DSP and SDR utilities <em>in</em> Python, <code>pysdr</code> provides helpful functions for signal analysis, modulation, and simulation that complement the driver libraries. It&#39;s great for understanding the math behind the operations.</li>\n</ul>\n<p><strong>Setup Check (from Module 1):</strong> Ensure you have the necessary drivers (<code>librtlsdr</code>, <code>libhackrf</code>) installed and the corresponding Python libraries (<code>pip install pyrtlsdr pyhackrf pysdr</code>). Make sure your SDR is plugged in and recognized by your system (e.g., check <code>lsusb</code> on Linux/macOS, or device manager on Windows).</p>\n<h3>2.2 Working with IQ Data: The Language of SDRs</h3>\n<p>This is <em>the</em> most fundamental concept when working with SDRs. SDRs don&#39;t give you a simple &quot;voltage vs. time&quot; like an oscilloscope on a low-frequency signal. They give you <strong>IQ data</strong>.</p>\n<p>Why? Because RF signals are oscillating at very high frequencies. Digitizing the raw RF signal directly is incredibly difficult and requires impossibly fast Analog-to-Digital Converters (ADCs) for most bands. Instead, SDRs use a technique called <strong>quadrature downconversion</strong>.</p>\n<p>Imagine your signal is at 100 MHz. The SDR mixes this signal with a local oscillator (LO) signal at or near 100 MHz. This mixing process shifts the signal&#39;s frequency down to a much lower frequency, centered around 0 Hz (baseband). Crucially, to preserve <em>all</em> the information (amplitude, frequency, <em>and phase</em>), the mixing is done twice, once with the LO signal directly (producing the <strong>In-phase, I</strong> component) and once with the LO signal phase-shifted by 90 degrees (producing the <strong>Quadrature, Q</strong> component).</p>\n<ul>\n<li><strong>I component:</strong> Represents the signal&#39;s component in phase with the LO.</li>\n<li><strong>Q component:</strong> Represents the signal&#39;s component 90 degrees out of phase with the LO.</li>\n</ul>\n<p>Together, I and Q form a <strong>complex number</strong>: <code>Signal(t) = I(t) + jQ(t)</code>, where <code>j</code> is the imaginary unit (<code>sqrt(-1)</code>).</p>\n<p>This complex number at any given time sample represents the signal&#39;s <strong>magnitude</strong> and <strong>phase</strong> relative to the center frequency the SDR is tuned to.</p>\n<ul>\n<li><strong>Magnitude:</strong> <code>Amplitude = sqrt(I^2 + Q^2)</code></li>\n<li><strong>Phase:</strong> <code>Phase = atan2(Q, I)</code> (using the <code>atan2</code> function which handles quadrants correctly)</li>\n</ul>\n<p><strong>Why is this important?</strong></p>\n<ol>\n<li><strong>Full Information:</strong> IQ data preserves the complete state of the signal at baseband, allowing you to reconstruct or analyze <em>any</em> aspect of the original RF signal digitally.</li>\n<li><strong>Digital Processing:</strong> Once you have the IQ data, all further signal processing (filtering, demodulation, analysis, <em>and adversarial perturbation</em>) can be done efficiently in software.</li>\n<li><strong>Adversarial Attacks:</strong> Our adversarial perturbations will directly manipulate this IQ data to try and fool the AI classifier. Understanding its structure is paramount.</li>\n</ol>\n<p>When you capture data with an SDR, you get a sequence of these complex IQ samples over time. In Python, this is typically represented as a NumPy array of complex numbers (<code>dtype=&#39;complex64&#39;</code> or <code>complex128</code>).</p>\n<p>Let&#39;s visualize some simple IQ data:</p>\n<pre><code class=\"language-python\">import numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate some simple IQ data representing a tone slightly off center frequency\n# A complex exponential e^(j*theta) represents a signal rotating in the IQ plane\n# If theta = 2 * pi * f * t, it&#39;s a tone at frequency f relative to the center frequency\nsample_rate = 2.048e6 # Samples per second\nduration = 1e-3       # 1 millisecond\nfrequency_offset = 10e3 # 10 kHz tone relative to center frequency\n\nt = np.arange(int(duration * sample_rate)) / sample_rate\n# Create the complex tone: exp(j * 2 * pi * f * t)\niq_data = np.exp(1j * 2 * np.pi * frequency_offset * t)\n\n# Separate I and Q components\ni_data = iq_data.real\nq_data = iq_data.imag\n\n# Plot the I and Q components over time\nplt.figure(figsize=(12, 6))\nplt.plot(t, i_data, label=&#39;I (In-phase)&#39;)\nplt.plot(t, q_data, label=&#39;Q (Quadrature)&#39;)\nplt.xlabel(&#39;Time (s)&#39;)\nplt.ylabel(&#39;Amplitude&#39;)\nplt.title(&#39;IQ Data Components over Time (10 kHz Tone)&#39;)\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the IQ data on the complex plane (constellation plot for a single tone)\nplt.figure(figsize=(6, 6))\nplt.scatter(i_data, q_data, s=1, alpha=0.5)\nplt.xlabel(&#39;I Component&#39;)\nplt.ylabel(&#39;Q Component&#39;)\nplt.title(&#39;IQ Constellation Plot (10 kHz Tone)&#39;)\nplt.axis(&#39;equal&#39;) # Important for constellation plots\nplt.grid(True)\nplt.show()\n</code></pre>\n<p><strong>Explanation:</strong></p>\n<ul>\n<li>The first plot shows the sinusoidal nature of the I and Q components over time. They are 90 degrees out of phase, as expected.</li>\n<li>The second plot, the &quot;constellation plot&quot; or IQ plot, shows the points <code>(I, Q)</code> in the complex plane. For a single, clean tone, this forms a perfect circle centered at the origin. The radius is the amplitude, and the speed of rotation around the circle corresponds to the frequency offset from the center.</li>\n</ul>\n<p>Understanding this representation is key because this is the raw material we&#39;ll capture, analyze, and manipulate.</p>\n<h3>2.3 Capturing RF Signals: Bringing Data from the Air</h3>\n<p>Now let&#39;s use our SDR library to capture real data. We&#39;ll start with <code>pyrtlsdr</code> as it&#39;s widely accessible.</p>\n<p><strong>Prerequisites:</strong></p>\n<ul>\n<li>An RTL-SDR dongle plugged in.</li>\n<li><code>pyrtlsdr</code> installed (<code>pip install pyrtlsdr</code>).</li>\n</ul>\n<pre><code class=\"language-python\">from rtlsdr import RtlSdr\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- Configuration ---\ncenter_freq = 100e6  # Tune to 100 MHz (example: near FM broadcast band)\nsample_rate = 2.048e6 # 2.048 MSps is a common rate for RTL-SDR\ngain = &#39;auto&#39;        # Or a specific gain value, e.g., 40\nnum_samples = 2**20  # Number of samples to capture (a power of 2 is good for FFT)\n\n# --- SDR Setup and Capture ---\nsdr = None\ntry:\n    # List available devices and pick the first one\n    sdr = RtlSdr(0)\n\n    print(f&quot;Configuring SDR:&quot;)\n    print(f&quot;  Center Freq: {center_freq/1e6} MHz&quot;)\n    print(f&quot;  Sample Rate: {sample_rate/1e6} MSps&quot;)\n    print(f&quot;  Gain: {gain}&quot;)\n    print(f&quot;  Number of Samples: {num_samples}&quot;)\n\n    sdr.sample_rate = sample_rate\n    sdr.center_freq = center_freq\n    sdr.gain = gain\n\n    print(&quot;Capturing data...&quot;)\n    # Capture samples. This returns a NumPy array of complex numbers (IQ data).\n    samples = sdr.read_samples(num_samples)\n    print(f&quot;Captured {len(samples)} samples.&quot;)\n\nexcept Exception as e:\n    print(f&quot;Error during SDR capture: {e}&quot;)\n    print(&quot;Ensure your SDR is plugged in and drivers are installed correctly.&quot;)\n    print(&quot;If using Linux, check udev rules for permissions.&quot;)\n    print(&quot;You might need to run this script with sudo if permissions are an issue (use with caution!).&quot;)\n    sdr = None # Ensure sdr variable is None if setup fails\n\nfinally:\n    # Always close the SDR connection when done\n    if sdr:\n        sdr.close()\n        print(&quot;SDR closed.&quot;)\n\n# --- Optional: Save the captured data ---\nif sdr and len(samples) &gt; 0:\n    file_name = f&quot;capture_{int(center_freq/1e6)}MHz_{int(sample_rate/1e6)}MSps_{num_samples}_samples.iq&quot;\n    samples.tofile(file_name)\n    print(f&quot;Data saved to {file_name}&quot;)\n\n# --- Proceed to analysis in the next section using the &#39;samples&#39; variable ---\n# If capture failed, &#39;samples&#39; will not be defined or will be empty.\n# For analysis examples, you can load a pre-saved file if capture fails.\n</code></pre>\n<p><strong>Explanation:</strong></p>\n<ol>\n<li>We import the necessary library (<code>RtlSdr</code>).</li>\n<li>We define our capture parameters: <code>center_freq</code> (where we tune), <code>sample_rate</code> (how wide a band we capture around the center frequency - the bandwidth is <code>sample_rate</code>), <code>gain</code> (amplification level), and <code>num_samples</code> (how much data to grab).</li>\n<li>We create an <code>RtlSdr</code> object, typically using the device index <code>0</code> if you only have one.</li>\n<li>We configure the SDR with our chosen parameters.</li>\n<li><code>sdr.read_samples(num_samples)</code> performs the capture. This is a blocking call that waits until <code>num_samples</code> are received.</li>\n<li>The captured data <code>samples</code> is a NumPy array of complex numbers (our IQ data!).</li>\n<li>We include error handling and ensure the SDR is closed using a <code>try...finally</code> block.</li>\n<li>Saving to a binary file (<code>.iq</code>) is shown, which is useful for later analysis or creating datasets.</li>\n</ol>\n<p><strong>Important Notes on Capture:</strong></p>\n<ul>\n<li><strong>Center Frequency vs. Signal Frequency:</strong> The <code>center_freq</code> is the middle of the captured bandwidth. A signal at, say, <code>center_freq + 500e3</code> Hz will appear at <code>+500e3</code> Hz relative to the <em>baseband</em> (0 Hz) in your captured IQ data. A signal at <code>center_freq - 200e3</code> Hz will appear at <code>-200e3</code> Hz.</li>\n<li><strong>Sample Rate:</strong> This determines the <em>bandwidth</em> you capture. An SDR capturing at <code>sample_rate</code> can capture signals within <code>center_freq ± sample_rate/2</code>. Higher sample rates capture wider swaths of spectrum but generate more data.</li>\n<li><strong>Gain:</strong> Too low, and faint signals are lost in the noise. Too high, and strong signals can be distorted (saturation). Often requires experimentation or using &#39;auto&#39; if available.</li>\n<li><strong>Antenna:</strong> The antenna matters! Ensure it&#39;s appropriate for the frequency band you&#39;re tuning to.</li>\n<li><strong>Permissions (Linux):</strong> You might need to add a udev rule or run with <code>sudo</code> to access the SDR device, although this is generally discouraged for security reasons if not necessary.</li>\n</ul>\n<h3>2.4 Signal Analysis with SDRs: Making Sense of IQ Data</h3>\n<p>Raw IQ data isn&#39;t very intuitive. We need ways to visualize and extract meaningful information. The most common tools are spectrum plots and spectrograms.</p>\n<p><strong>Spectrum Analysis (FFT):</strong></p>\n<p>The Fast Fourier Transform (FFT) is our key tool here. It transforms the signal from the time domain (IQ samples over time) to the frequency domain (amplitude/power at different frequencies).</p>\n<pre><code class=\"language-python\"># Assuming &#39;samples&#39; variable contains the captured IQ data from the previous step\n\nif &#39;samples&#39; in locals() and len(samples) &gt; 0:\n    # --- Spectrum Analysis (FFT) ---\n    # Calculate the power spectral density (PSD)\n    # NFFT is the number of points used in each FFT block.\n    # A power of 2 is efficient. num_samples is often a power of 2.\n    NFFT = len(samples)\n    # Ensure NFFT is a power of 2 for optimal performance, though not strictly required by np.fft\n    # NFFT = 2**int(np.ceil(np.log2(len(samples)))) # Alternative if you need a specific NFFT\n\n    # Use Matplotlib&#39;s specgram for a quick PSD plot\n    # It handles windowing, overlapping, and FFT calculation\n    plt.figure(figsize=(12, 6))\n    # The first argument is the signal (IQ samples)\n    # NFFT is the number of data points in each block for the FFT\n    # Fs is the sampling frequency\n    # Fc is the center frequency (used for the x-axis scale)\n    Pxx, freqs, bins, im = plt.specgram(samples, NFFT=NFFT, Fs=sample_rate, Fc=center_freq, noverlap=NFFT//2, cmap=&#39;viridis&#39;)\n\n    # Calculate the frequency axis correctly\n    # The FFT output covers frequencies from -sample_rate/2 to +sample_rate/2 relative to center_freq\n    freqs_fft = np.fft.fftshift(np.fft.fftfreq(NFFT, 1/sample_rate)) + center_freq\n\n    # Calculate the power spectrum manually (optional, specgram does it)\n    # power_spectrum = np.abs(np.fft.fftshift(np.fft.fft(samples)))**2\n    # plt.figure(figsize=(12, 6))\n    # plt.plot(freqs_fft/1e6, 10 * np.log10(power_spectrum)) # Plot in dB\n    # plt.xlabel(&quot;Frequency (MHz)&quot;)\n    # plt.ylabel(&quot;Power (dB)&quot;)\n    # plt.title(&quot;Power Spectrum&quot;)\n    # plt.grid(True)\n    # plt.show()\n\n\n    # Plotting the result from specgram (it&#39;s primarily for spectrogram, but returns PSD data too)\n    # Let&#39;s just use a simple plot of the average spectrum if specgram output isn&#39;t clear for single spectrum\n    # A more direct way:\n    spectrum = np.fft.fftshift(np.fft.fft(samples))\n    power_spectrum_db = 10 * np.log10(np.abs(spectrum)**2)\n\n    plt.figure(figsize=(12, 6))\n    plt.plot(freqs_fft/1e6, power_spectrum_db)\n    plt.xlabel(&quot;Frequency (MHz)&quot;)\n    plt.ylabel(&quot;Power (dB)&quot;)\n    plt.title(f&quot;Power Spectrum around {center_freq/1e6} MHz&quot;)\n    plt.grid(True)\n    plt.show()\n\nelse:\n    print(&quot;No samples available for analysis. Please run the capture step first or load a file.&quot;)\n</code></pre>\n<p><strong>Explanation:</strong></p>\n<ol>\n<li>We use <code>np.fft.fft()</code> to perform the Fourier Transform on our <code>samples</code> array.</li>\n<li><code>np.fft.fftshift()</code> rearranges the output so that the 0 Hz component is in the center, matching the <code>center_freq</code> of our SDR tune. Frequencies range from <code>-sample_rate/2</code> to <code>+sample_rate/2</code> relative to the center.</li>\n<li><code>np.fft.fftfreq()</code> calculates the corresponding frequency bins for the FFT output.</li>\n<li>We calculate the magnitude (absolute value) and often the power (<code>magnitude**2</code>) or power in dB (<code>10 * log10(power)</code>) for plotting.</li>\n<li>The plot shows frequency on the x-axis and power/amplitude on the y-axis. Peaks indicate where signals are present.</li>\n</ol>\n<p><strong>Spectrogram (Waterfall Plot):</strong></p>\n<p>A spectrogram shows how the spectrum changes over time. It&#39;s a sequence of FFTs calculated on small, overlapping chunks of the data. This is invaluable for analyzing signals that change frequency or have intermittent activity.</p>\n<pre><code class=\"language-python\"># Assuming &#39;samples&#39; variable is available\n\nif &#39;samples&#39; in locals() and len(samples) &gt; 0:\n    # --- Spectrogram Analysis ---\n    # NFFT: size of the FFT window (determines frequency resolution)\n    # noverlap: number of samples overlapping between windows\n    # Fs: sampling frequency\n    # Fc: center frequency\n\n    NFFT_spectrogram = 1024 # Common FFT size for spectrograms\n    noverlap_spectrogram = NFFT_spectrogram // 2 # 50% overlap is typical\n\n    plt.figure(figsize=(12, 8))\n    # The specgram function is ideal for this\n    Pxx, freqs, bins, im = plt.specgram(samples,\n                                        NFFT=NFFT_spectrogram,\n                                        Fs=sample_rate,\n                                        Fc=center_freq,\n                                        noverlap=noverlap_spectrogram,\n                                        cmap=&#39;viridis&#39;, # Color map (e.g., &#39;plasma&#39;, &#39;inferno&#39;, &#39;cividis&#39;)\n                                        xextent=(0, len(samples)/sample_rate)) # Show time on x-axis\n\n    plt.xlabel(&quot;Time (s)&quot;)\n    plt.ylabel(&quot;Frequency (MHz)&quot;)\n    plt.title(f&quot;Spectrogram around {center_freq/1e6} MHz&quot;)\n    plt.colorbar(label=&quot;Power (dB)&quot;) # Add a color bar to show power scale\n    plt.ylim(center_freq/1e6 - sample_rate/2e6, center_freq/1e6 + sample_rate/2e6) # Set frequency limits\n    plt.show()\n\nelse:\n     print(&quot;No samples available for spectrogram analysis.&quot;)\n</code></pre>\n<p><strong>Explanation:</strong></p>\n<ol>\n<li>We use <code>matplotlib.pyplot.specgram()</code>. This function is designed for spectrograms.</li>\n<li><code>NFFT</code> now determines the frequency resolution (number of points in each mini-FFT). A larger <code>NFFT</code> gives better frequency resolution but poorer time resolution (wider windows).</li>\n<li><code>noverlap</code> controls how much the windows overlap. Overlapping helps capture transient signals and makes the spectrogram smoother.</li>\n<li>The x-axis typically represents time, the y-axis represents frequency (relative to the center frequency), and the color intensity represents the power at that time/frequency point.</li>\n<li>This plot is crucial for seeing how signals appear and disappear, change frequency (like FM), or have complex time-frequency characteristics.</li>\n</ol>\n<p><strong>Other Analysis Techniques:</strong></p>\n<ul>\n<li><strong>Time-domain plots:</strong> Plotting I and Q over time (as shown in 2.2) or calculating instantaneous amplitude/phase can reveal modulation patterns.</li>\n<li><strong>Constellation Plots:</strong> Plotting <code>Q</code> vs <code>I</code> for modulated signals shows clusters of points corresponding to different symbols (e.g., PSK, QAM). This is less useful for broadband signals but vital for digital modulation analysis.</li>\n</ul>\n<p><strong>Exercise:</strong></p>\n<ol>\n<li>Capture data from a known signal source near you (e.g., a strong local FM radio station, or if you have a simple FSK transmitter like a wireless doorbell or weather station sensor, capture that).</li>\n<li>Use the spectrum plot to find the exact center frequency and bandwidth of the signal.</li>\n<li>Use the spectrogram to observe how the signal changes over time. Can you see the characteristics of the modulation? (e.g., FM will show frequency deviations).</li>\n</ol>\n<h3>2.5 Basic Signal Generation: Putting Bits to Air</h3>\n<p>Now for the exciting part: creating our <em>own</em> RF signals using code and transmitting them. This requires a TX-capable SDR like the HackRF One, LimeSDR, or ADALM-PLUTO.</p>\n<p><strong>Prerequisites:</strong></p>\n<ul>\n<li>A TX-capable SDR plugged in.</li>\n<li>The corresponding Python library installed (e.g., <code>pyhackrf</code>).</li>\n<li><strong>A controlled environment (e.g., using attenuators, connecting TX directly to RX with a cable and attenuator, or operating in a shielded room) and confirmation you are operating within legal power limits and frequency bands.</strong> <strong>Seriously, don&#39;t cause interference!</strong></li>\n</ul>\n<p>Let&#39;s generate a simple, continuous tone slightly offset from our center frequency.</p>\n<pre><code class=\"language-python\">from hackrf import HackRF\nimport numpy as np\nimport time\n\n# --- Configuration ---\ncenter_freq = 433.92e6 # A common ISM band frequency (check local regulations!)\nsample_rate = 2.0e6    # Sample rate for TX\ntx_gain = 0            # Transmit gain (0-47 dB for HackRF)\namplitude = 0.5        # Amplitude of the generated signal (affects output power)\nduration = 5           # Seconds to transmit\n\n# --- Signal Generation (Creating IQ data) ---\n# Generate a tone at 50 kHz above the center frequency\ntone_freq_offset = 50e3 # 50 kHz offset\n\n# Number of samples needed\nnum_samples = int(duration * sample_rate)\n\n# Generate the time vector\nt = np.arange(num_samples) / sample_rate\n\n# Create the complex tone waveform: amplitude * exp(j * 2 * pi * f * t)\n# Ensure the maximum value is within the SDR&#39;s digital range (-1 to 1 for IQ components)\n# The HackRF expects float32 IQ pairs\niq_data_float32 = (amplitude * np.exp(1j * 2 * np.pi * tone_freq_offset * t)).astype(np.complex64)\n\nprint(f&quot;Generated {len(iq_data_float32)} samples for transmission.&quot;)\n\n# --- SDR Setup and Transmission ---\nsdr = None\ntry:\n    sdr = HackRF()\n\n    print(f&quot;Configuring SDR for TX:&quot;)\n    print(f&quot;  Center Freq: {center_freq/1e6} MHz&quot;)\n    print(f&quot;  Sample Rate: {sample_rate/1e6} MSps&quot;)\n    print(f&quot;  TX Gain: {tx_gain} dB&quot;)\n    print(f&quot;  Amplitude: {amplitude}&quot;)\n    print(f&quot;  Duration: {duration} seconds&quot;)\n\n    sdr.sample_rate = sample_rate\n    sdr.center_freq = center_freq\n    sdr.tx_gain = tx_gain # Use tx_gain for transmit power\n\n    # --- Transmission ---\n    print(&quot;Starting transmission...&quot;)\n    # transmit() takes the IQ data (NumPy array)\n    # It&#39;s often blocking for the duration of the data provided\n    sdr.transmit(iq_data_float32, repeat=False) # Set repeat=True for continuous loop\n\n    print(&quot;Transmission finished.&quot;)\n\nexcept Exception as e:\n    print(f&quot;Error during SDR transmission: {e}&quot;)\n    print(&quot;Ensure your TX-capable SDR is plugged in and drivers are installed correctly.&quot;)\n    print(&quot;Check permissions if necessary.&quot;)\n    print(&quot;Are you sure your SDR supports TX (e.g., HackRF, not RTL-SDR)?&quot;)\n\nfinally:\n    # Always stop and close the SDR connection\n    if sdr:\n        sdr.stop_tx() # Explicitly stop transmission\n        sdr.close()\n        print(&quot;SDR stopped and closed.&quot;)\n</code></pre>\n<p><strong>Explanation:</strong></p>\n<ol>\n<li>We import the necessary library (<code>HackRF</code>).</li>\n<li></li>\n</ol>\n\n                </div>\n             </div>\n         ",
    "module-3": "\n             <div class=\"card main-content-card\"> <!-- Added main-content-card class -->\n                <h1>Module 3: module_3</h1> <!-- Use module title here -->\n                <div class=\"markdown-content\">\n                    <p>Alright team! Welcome back. We&#39;ve set the stage, got our SDRs humming, and captured some raw RF goodness. Now, it&#39;s time to bridge the gap between that raw data and the intelligent systems that try to make sense of it. This is where Artificial Intelligence steps into our RF world, and frankly, it&#39;s where some fascinating vulnerabilities start to emerge.</p>\n<p>In <strong>Module 3: AI/ML for Spectrum: Classification &amp; Feature Engineering</strong>, we&#39;re going to understand <em>how</em> those messy RF signals get transformed into something an AI can chew on, and we&#39;ll build our very first simple AI model designed specifically to classify these signals. This isn&#39;t just theory; the model we build here is the very <em>target</em> system we&#39;ll be attacking later. So, let&#39;s make it real!</p>\n<hr>\n<h2><strong>Module 3: AI/ML for Spectrum: Classification &amp; Feature Engineering</strong></h2>\n<ul>\n<li><strong>Module Objective:</strong> Understand how RF signals are processed and represented for Artificial Intelligence models, and learn the fundamentals of training a basic AI model for signal classification. By the end, you&#39;ll have built the core AI component that our adversarial attacks will target.</li>\n</ul>\n<h3><strong>Introduction: Why AI Needs Data in a Specific Format</strong></h3>\n<p>Think about how <em>you</em> recognize things. You don&#39;t process raw light waves hitting your retina as complex electromagnetic field equations. Your brain processes them into shapes, colors, patterns. Similarly, AI models, especially neural networks, need data presented in a structured, understandable format.</p>\n<p>RF signals, in their raw form (like the IQ data we captured in Module 2), are sequences of complex numbers changing over time. While powerful, this format isn&#39;t always the most intuitive for standard AI architectures designed for things like images or text. This module is all about turning that RF complexity into AI-friendly inputs.</p>\n<h3><strong>Essential Subtopics Deep Dive:</strong></h3>\n<h4><strong>1. Review of Relevant ML Concepts</strong></h4>\n<p>Before we apply ML to RF, let&#39;s quickly recap the core ideas we&#39;ll rely on:</p>\n<ul>\n<li><strong>Supervised Learning:</strong> This is our primary mode. We have input data (our RF signals) and corresponding correct outputs (the signal type, e.g., &quot;WiFi&quot;, &quot;LTE&quot;, &quot;FM&quot;). The AI learns a mapping from input to output based on this labeled data.</li>\n<li><strong>Classification:</strong> Our specific task. We want the AI to assign an input RF signal to one of several predefined categories or classes (the signal types).</li>\n<li><strong>Training Data:</strong> The set of input-output pairs used to <em>teach</em> the model. The model adjusts its internal parameters to minimize errors on this data.</li>\n<li><strong>Labels:</strong> The correct output category associated with each piece of training data.</li>\n<li><strong>Features:</strong> The specific, quantifiable pieces of information extracted from the raw data that the AI model uses to make its decision. This is a critical step in RF.</li>\n<li><strong>Models:</strong> The mathematical structure (e.g., a neural network) that learns the relationship between features and labels.</li>\n</ul>\n<h4><strong>2. Representing RF Data for ML</strong></h4>\n<p>This is arguably the most crucial step. How do we take our IQ data and make it digestible for an AI? Several common approaches exist:</p>\n<ul>\n<li><p><strong>a) Raw IQ Data:</strong></p>\n<ul>\n<li><strong>Concept:</strong> Feed the sequence of complex IQ samples directly into the model.</li>\n<li><strong>Pros:</strong> Preserves all information, no manual feature engineering needed initially.</li>\n<li><strong>Cons:</strong> High dimensionality (each sample is complex, often need many samples for a window), sensitive to timing shifts, requires models good with sequences (like LSTMs). Can be computationally intensive.</li>\n<li><strong>Example Data Shape:</strong> A sequence of <code>N</code> complex samples might be represented as a tensor of shape <code>(N, 2)</code> for real and imaginary parts, or <code>(N,)</code> for complex numbers if the framework supports it.</li>\n</ul>\n</li>\n<li><p><strong>b) Time-Series Features:</strong></p>\n<ul>\n<li><strong>Concept:</strong> Calculate statistical features over a window of IQ data.</li>\n<li><strong>Pros:</strong> Reduces dimensionality significantly, relatively easy to compute.</li>\n<li><strong>Cons:</strong> Loses fine-grained temporal structure, potentially discarding discriminative information.</li>\n<li><strong>Examples:</strong> Mean amplitude, variance, skewness, kurtosis, peak-to-average power ratio (PAPR), zero-crossing rate, etc. Calculated over segments of the IQ stream.</li>\n</ul>\n</li>\n<li><p><strong>c) Frequency-Domain Features:</strong></p>\n<ul>\n<li><strong>Concept:</strong> Apply a Fast Fourier Transform (FFT) to convert the time-domain IQ data into the frequency domain. Calculate features from the resulting spectrum.</li>\n<li><strong>Pros:</strong> Robust to time shifts, highlights spectral characteristics (bandwidth, center frequency, presence of carriers/harmonics).</li>\n<li><strong>Cons:</strong> Loses temporal information within the window.</li>\n<li><strong>Examples:</strong> Peak frequency, occupied bandwidth, spectral flatness, spectral entropy. Often calculated from the Power Spectral Density (PSD), which is the magnitude-squared of the FFT.</li>\n</ul>\n</li>\n<li><p><strong>d) Time-Frequency Representations (Spectrograms):</strong></p>\n<ul>\n<li><p><strong>Concept:</strong> Apply the Short-Time Fourier Transform (STFT) repeatedly over overlapping windows of IQ data. This generates a 2D representation showing how the frequency content of the signal changes over time. This is a spectrogram!</p>\n</li>\n<li><p><strong>Pros:</strong> Captures both time and frequency characteristics, visually intuitive, can leverage powerful image-based AI models (like CNNs). Very popular in RF signal classification.</p>\n</li>\n<li><p><strong>Cons:</strong> Requires careful tuning of STFT parameters (window size, overlap), loses some phase information (though magnitude is usually sufficient).</p>\n</li>\n<li><p><strong>Example Data Shape:</strong> A spectrogram is essentially an image. If you generate a spectrogram with <code>T</code> time bins and <code>F</code> frequency bins, the shape is <code>(T, F)</code>. If you treat it as a single channel image, it&#39;s <code>(T, F, 1)</code>.</p>\n</li>\n<li><p><strong>Let&#39;s focus on Spectrograms for our project.</strong> They are a widely used representation that works well with common neural network architectures (CNNs), making the ML part more straightforward for this course&#39;s scope.</p>\n</li>\n</ul>\n<pre><code class=\"language-python\">import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import spectrogram\nfrom scipy.io import loadmat # Often used for RF datasets\n\n# --- Example: Generating a Spectrogram ---\n\n# Assume &#39;iq_data&#39; is a numpy array of complex numbers (from Module 2 capture)\n# Example: Create some dummy IQ data (a simple tone plus noise)\nsample_rate = 1e6 # 1 MHz\nduration = 0.01   # 10 ms\nt = np.linspace(0, duration, int(sample_rate * duration), endpoint=False)\n# Tone at 100 kHz\ntone_freq = 100e3\niq_data = 0.5 * np.exp(2j * np.pi * tone_freq * t)\n# Add some noise\niq_data += np.random.randn(len(iq_data)) + 1j * np.random.randn(len(iq_data))\n\n# Spectrogram parameters (need tuning based on signal type and desired resolution)\nnperseg = 256   # Samples per segment (FFT size)\nnoverlap = nperseg // 2 # Overlap between segments\n\n# Generate the spectrogram\n# fs = sample rate, noverlap = overlap, nperseg = segment length\nf, t_spec, Sxx = spectrogram(iq_data, fs=sample_rate, noverlap=noverlap, nperseg=nperseg)\n\n# Sxx is the power spectral density (magnitude squared).\n# Often, we use log scale for better visualization and sometimes for ML input\nSxx_log = 10 * np.log10(Sxx + 1e-10) # Add small value to avoid log(0)\n\n# Visualize the spectrogram\nplt.figure(figsize=(10, 6))\nplt.pcolormesh(t_spec, f, Sxx_log, shading=&#39;gouraud&#39;)\nplt.ylabel(&#39;Frequency [Hz]&#39;)\nplt.xlabel(&#39;Time [sec]&#39;)\nplt.title(&#39;Spectrogram of IQ Data&#39;)\nplt.colorbar(label=&#39;Power/Frequency [dB/Hz]&#39;)\nplt.show()\n\nprint(f&quot;Raw IQ data shape: {iq_data.shape}&quot;)\nprint(f&quot;Spectrogram shape (freq bins, time bins): {Sxx.shape}&quot;)\n</code></pre>\n<ul>\n<li><strong>Note:</strong> The output <code>Sxx</code> has shape <code>(n_frequencies, n_times)</code>. For a CNN input (like an image), we might reshape it to <code>(n_times, n_frequencies, 1)</code> to represent it as a single-channel image, or just use the <code>(n_times, n_frequencies)</code> shape if the framework supports it.</li>\n</ul>\n</li>\n</ul>\n<h4><strong>3. Selecting and Engineering Features for RF Signals</strong></h4>\n<p>If we weren&#39;t using Spectrograms, we&#39;d spend time here deciding which statistical or frequency-domain features are most discriminative for the signal types we care about. This often involves domain knowledge about the signals (e.g., FM has constant amplitude, PSK has constant envelope but phase changes).</p>\n<p>Since we&#39;re focusing on Spectrograms for the project, our &quot;feature engineering&quot; is primarily the <em>process</em> of generating the spectrograms themselves and deciding on parameters like <code>nperseg</code> and <code>noverlap</code>. These parameters affect the time-frequency resolution trade-off.</p>\n<h4><strong>4. Introduction to Neural Networks suitable for RF</strong></h4>\n<ul>\n<li><p><strong>Convolutional Neural Networks (CNNs):</strong> These are powerhouses for image recognition and are excellent for processing grid-like data like Spectrograms.</p>\n<ul>\n<li><strong>Why they work:</strong> They use convolutional layers to automatically learn hierarchical features (edges, patterns, textures) from the input data, regardless of where they appear spatially. In a spectrogram, this means learning to recognize specific frequency patterns or time-varying structures characteristic of a modulation type.</li>\n<li><strong>Architecture:</strong> Typically involve convolutional layers, pooling layers (to reduce dimensionality and provide translation invariance), and dense (fully connected) layers at the end for classification.</li>\n</ul>\n</li>\n<li><p><strong>Recurrent Neural Networks (RNNs), particularly LSTMs (Long Short-Term Memory):</strong> These are designed for sequential data.</p>\n<ul>\n<li><p><strong>Why they work:</strong> They have internal memory that allows them to process sequences piece by piece, making them suitable for raw IQ data streams where temporal order is important.</p>\n</li>\n<li><p><strong>Architecture:</strong> Often involve LSTM layers followed by dense layers.</p>\n</li>\n<li><p><strong>For our project, we will use a simple CNN, as Spectrograms are our chosen representation.</strong></p>\n</li>\n</ul>\n</li>\n</ul>\n<h4><strong>5. Using ML Frameworks: TensorFlow or PyTorch basics</strong></h4>\n<p>These frameworks provide the tools to build, train, and evaluate neural networks efficiently, often leveraging GPU acceleration. We&#39;ll use TensorFlow with its Keras API for simplicity in our examples.</p>\n<p>Basic steps in Keras:</p>\n<ol>\n<li><strong>Load and Preprocess Data:</strong> Get your Spectrograms (or other features) and labels into NumPy arrays or a framework-specific data format. Ensure shapes are correct. Split into training and testing sets.</li>\n<li><strong>Define the Model:</strong> Stack layers (e.g., <code>Conv2D</code>, <code>MaxPooling2D</code>, <code>Flatten</code>, <code>Dense</code>).</li>\n<li><strong>Compile the Model:</strong> Specify the optimizer (how the model learns), the loss function (how errors are measured), and metrics (what to track during training, like accuracy).</li>\n<li><strong>Train the Model:</strong> Feed the training data to the model (<code>model.fit()</code>).</li>\n<li><strong>Evaluate the Model:</strong> Check performance on unseen test data (<code>model.evaluate()</code>).</li>\n<li><strong>Make Predictions:</strong> Use the trained model to classify new data (<code>model.predict()</code>).</li>\n</ol>\n<pre><code class=\"language-python\"># --- Example: Basic CNN using TensorFlow/Keras ---\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer # To one-hot encode labels\n\n# Assume you have:\n# spectrograms_data: numpy array of spectrograms, shape (num_samples, height, width)\n# labels_data: numpy array of strings or integers, shape (num_samples,)\n# num_classes: integer, number of distinct signal types\n\n# --- Dummy Data Generation (Replace with your actual data loading) ---\n# Let&#39;s simulate having 100 samples of 3 classes, spectrogram size 128x128\nnum_samples = 300\nheight, width = 128, 128\nnum_classes = 3\nclass_names = [&#39;FM&#39;, &#39;AM&#39;, &#39;Noise&#39;] # Example class names\n\nspectrograms_data = np.random.rand(num_samples, height, width).astype(np.float32) * 100 # Simulate some data\nlabels_data = np.random.choice(class_names, num_samples) # Simulate labels\n\n# Add a channel dimension (required for Conv2D)\n# Input shape for CNN is (height, width, channels)\nspectrograms_data = np.expand_dims(spectrograms_data, axis=-1) # Shape becomes (num_samples, height, width, 1)\n\n# Convert labels to one-hot encoding (e.g., &#39;FM&#39; -&gt; [1, 0, 0])\nlabel_binarizer = LabelBinarizer()\nlabels_one_hot = label_binarizer.fit_transform(labels_data)\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    spectrograms_data, labels_one_hot, test_size=0.2, random_state=42, stratify=labels_one_hot\n)\n\nprint(f&quot;Training data shape: {X_train.shape}&quot;)\nprint(f&quot;Testing data shape: {X_test.shape}&quot;)\nprint(f&quot;Training labels shape: {y_train.shape}&quot;)\nprint(f&quot;Testing labels shape: {y_test.shape}&quot;)\n\n# --- Define the CNN Model ---\nmodel = Sequential([\n    Conv2D(32, (3, 3), activation=&#39;relu&#39;, input_shape=(height, width, 1)),\n    MaxPooling2D((2, 2)),\n    Conv2D(64, (3, 3), activation=&#39;relu&#39;),\n    MaxPooling2D((2, 2)),\n    Conv2D(128, (3, 3), activation=&#39;relu&#39;),\n    MaxPooling2D((2, 2)),\n    Flatten(), # Flatten the 3D output to 1D\n    Dense(128, activation=&#39;relu&#39;),\n    Dropout(0.5), # Helps prevent overfitting\n    Dense(num_classes, activation=&#39;softmax&#39;) # Output layer with softmax for classification\n])\n\n# --- Compile the Model ---\nmodel.compile(optimizer=&#39;adam&#39;,\n              loss=&#39;categorical_crossentropy&#39;, # Suitable for multi-class classification with one-hot labels\n              metrics=[&#39;accuracy&#39;])\n\nmodel.summary()\n\n# --- Train the Model ---\n# Use a small number of epochs for demonstration\nepochs = 10\nbatch_size = 32\n\nprint(&quot;\\nStarting model training...&quot;)\nhistory = model.fit(X_train, y_train,\n                    epochs=epochs,\n                    batch_size=batch_size,\n                    validation_split=0.2, # Use a portion of training data for validation during training\n                    verbose=1) # Show training progress\n\nprint(&quot;Training finished.&quot;)\n\n# --- Evaluate the Model ---\nprint(&quot;\\nEvaluating model on test data...&quot;)\nloss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n\nprint(f&quot;Test Loss: {loss:.4f}&quot;)\nprint(f&quot;Test Accuracy: {accuracy:.4f}&quot;)\n\n# --- Save the Model (for use in Module 4) ---\nmodel_save_path = &quot;rf_classifier_model.h5&quot; # Keras HDF5 format\nmodel.save(model_save_path)\nprint(f&quot;Model saved to {model_save_path}&quot;)\n\n# To load the model later:\n# loaded_model = tf.keras.models.load_model(model_save_path)\n</code></pre>\n<h4><strong>6. Evaluating Classifier Performance</strong></h4>\n<p>Accuracy is the most common metric, but it can be misleading, especially with imbalanced datasets. A classifier that always predicts the majority class might have high accuracy but be useless.</p>\n<ul>\n<li><p><strong>Accuracy:</strong> (Number of correct predictions) / (Total number of predictions). Simple, but potentially misleading.</p>\n</li>\n<li><p><strong>Confusion Matrix:</strong> A table showing the counts of True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN) for each class.</p>\n<ul>\n<li><strong>TP:</strong> Predicted class X, actual class X.</li>\n<li><strong>FP:</strong> Predicted class X, actual class Y (Type I error).</li>\n<li><strong>FN:</strong> Predicted class Y, actual class X (Type II error).</li>\n<li><strong>TN:</strong> Predicted not class X, actual not class X.</li>\n<li>From the confusion matrix, you can derive other metrics like Precision, Recall (Sensitivity), F1-score, and Specificity, which give a more nuanced view of performance, especially per class.</li>\n</ul>\n<pre><code class=\"language-python\"># --- Example: Confusion Matrix using scikit-learn ---\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns # For pretty plots\n\n# Assume you have the trained &#39;model&#39; and test data X_test, y_test from the previous example\n\n# Get predictions for the test set\ny_pred_one_hot = model.predict(X_test)\ny_pred_labels = label_binarizer.inverse_transform(y_pred_one_hot) # Convert one-hot back to original labels\ny_true_labels = label_binarizer.inverse_transform(y_test)\n\n# Generate the confusion matrix\ncm = confusion_matrix(y_true_labels, y_pred_labels, labels=class_names) # Specify labels to ensure order\n\nprint(&quot;\\nConfusion Matrix:&quot;)\nprint(cm)\n\n# Visualize the confusion matrix (more intuitive)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=&#39;d&#39;, cmap=&#39;Blues&#39;, xticklabels=class_names, yticklabels=class_names)\nplt.xlabel(&#39;Predicted Label&#39;)\nplt.ylabel(&#39;True Label&#39;)\nplt.title(&#39;Confusion Matrix&#39;)\nplt.show()\n\n# Print a classification report (includes precision, recall, f1-score)\nprint(&quot;\\nClassification Report:&quot;)\nprint(classification_report(y_true_labels, y_pred_labels, target_names=class_names))\n</code></pre>\n</li>\n</ul>\n<h4><strong>7. Case Study: How AI is used in military/intelligence signal identification (SIGINT)</strong></h4>\n<p>This is a huge and fascinating area where AI is rapidly being deployed.</p>\n<ul>\n<li><strong>Task:</strong> Automatically monitor vast swaths of the RF spectrum.</li>\n<li><strong>Goal:</strong><ul>\n<li>Identify known communication signals (voice, data, radar, etc.).</li>\n<li>Classify their modulation type, protocol, or even specific device.</li>\n<li>Detect unknown or anomalous signals.</li>\n<li>Geolocate signal sources.</li>\n</ul>\n</li>\n<li><strong>Why AI?</strong> The sheer volume and complexity of modern spectrum make manual analysis impossible. AI can process data at machine speed, identify subtle patterns, and adapt (if trained properly) to new signal types.</li>\n<li><strong>Vulnerability:</strong> If an adversary knows (or can figure out) the AI models being used for classification, they can potentially craft signals that mimic friendly communications, evade detection, or appear as something benign, disrupting intelligence gathering or enabling deception. This is <em>exactly</em> the type of scenario our adversarial attacks explore.</li>\n</ul>\n<h3><strong>Module Project/Exercise:</strong></h3>\n<p>Alright, let&#39;s build the heart of our target system!</p>\n<ol>\n<li><p><strong>Obtain RF Signal Data:</strong></p>\n<ul>\n<li><strong>Option A (Recommended for repeatability):</strong> Use a publicly available, pre-labeled dataset like the <a href=\"https://www.deepsig.io/datasets\">RML2016.10a dataset</a>. This dataset contains IQ samples for various modulation types (8PSK, AM, FM, GFSK, etc.) at different Signal-to-Noise Ratios (SNRs). It&#39;s structured specifically for ML tasks. You&#39;ll need to download it.</li>\n<li><strong>Option B (More challenging, but uses your own data):</strong> Use the data you captured in Module 2. <em>Challenge:</em> This data is likely unlabeled noise or unknown signals. You would need to generate a few known signals (e.g., using GNU Radio or another SDR) and capture them, carefully labeling each capture. This is harder to make repeatable and get enough diverse data for training.</li>\n<li><em>Let&#39;s proceed assuming you&#39;ll use a structured dataset like RML2016.10a for the code examples.</em></li>\n</ul>\n</li>\n<li><p><strong>Process Data into Spectrograms:</strong></p>\n<ul>\n<li>Load the IQ data for a selected subset of modulation types (start with 2-3 distinct ones, e.g., FM, AM, BPSK) at a reasonable SNR (e.g., 0dB or higher).</li>\n<li>For each IQ sample (which is a time series), compute its spectrogram using <code>scipy.signal.spectrogram</code>. Experiment with <code>nperseg</code> and <code>noverlap</code> parameters. A common approach is to make the spectrogram image size reasonable (e.g., 64x64, 128x128). Remember to use the magnitude or log-magnitude.</li>\n<li>Store the resulting spectrograms and their corresponding labels.</li>\n</ul>\n</li>\n<li><p><strong>Create a Labeled Dataset for ML:</strong></p>\n<ul>\n<li>Organize your processed spectrograms and their labels into NumPy arrays.</li>\n<li>Ensure your labels are consistently represented (e.g., strings like &#39;FM&#39;, &#39;AM&#39;, &#39;BPSK&#39;).</li>\n<li>Split your dataset into training and testing sets using <code>train_test_split</code> from <code>sklearn.model_selection</code>.</li>\n</ul>\n</li>\n<li><p><strong>Train a Simple Neural Network Classifier:</strong></p>\n<ul>\n<li>Define a simple CNN model using TensorFlow/Keras, similar to the example code provided above. Adjust the input shape to match your spectrogram dimensions.</li>\n<li>Compile the model using <code>adam</code> optimizer and <code>categorical_crossentropy</code> loss.</li>\n<li>Train the model on your training data for a sufficient number of epochs (start with 10-20, monitor validation accuracy).</li>\n<li><em>Computational Note:</em> Training can take time depending on dataset size and your hardware. Start with a small dataset subset if needed.</li>\n</ul>\n</li>\n<li><p><strong>Evaluate the Trained Model:</strong></p>\n<ul>\n<li>Evaluate the model&#39;s performance on the <em>unseen</em> test set using <code>model.evaluate</code>. Note the test accuracy.</li>\n<li>Generate and display the confusion matrix using <code>sklearn.metrics.confusion_matrix</code> and <code>seaborn.heatmap</code>. Analyze which signal types are being confused.</li>\n</ul>\n</li>\n</ol>\n<p><strong>Code Sketch for Project Steps 1-3 (using a simplified RML-like structure):</strong></p>\n<pre><code class=\"language-python\">import numpy as np\nfrom scipy.signal import spectrogram\nfrom scipy.io import loadmat\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\n\n# --- Project Step 1 &amp; 2: Load Data and Generate Spectrograms ---\n# Assuming you downloaded RML2016.10a.tar.gz and extracted it.\n# The .mat file structure is usually a dictionary.\n# Data is typically indexed by SNR and Modulation Type.\n\n# Example: Load a small subset of data (e.g., 3 modulation types at 0dB SNR)\n# Note: The actual RML dataset structure might require adaptation of loading code.\n# This is a conceptual example. A helper function is often used for RML loading.\n\n# FAKE DATA STRUCTURE FOR DEMO - REPLACE WITH REAL LOADING\n# In a real scenario, you&#39;d load the .mat file and iterate through its structure\n# to get IQ samples and labels.\n# Example structure: data[&#39;SNR&#39;][&#39;ModType&#39;] = np.array([...IQ samples...])\nfake_data = {}\nfake_data[0] = {} # SNR 0dB\nfake_data[0][&#39;8PSK&#39;] = np.random.randn(100, 128) + 1j*np.random.randn(100, 128) # 100 samples, 128 points each\nfake_data[0][&#39;FM&#39;] = np.random.randn(100, 128) + 1j*np.random.randn(100, 128)\nfake_data[0][&#39;WFM&#39;] = np.random.randn(100, 128) + 1j*np.random.randn(100, 128)\n\n# --- Real RML data loading is more involved ---\n# You&#39;d typically load the .mat file:\n# data = loadmat(&#39;RML2016.10a.mat&#39;)[&#39;RML2016_10a&#39;] # Adjust key based on file structure\n# Then iterate through SNRs and modulations:\n# classes = [&#39;8PSK&#39;, &#39;AM&#39;, &#39;CPFSK&#39;, ...]\n# snrs = [-20, -18, ..., 18]\n# X = []\n# Y = []\n# for snr in snrs:\n#     for mod in classes:\n#         iq_samples_list = data[mod][snr] # Get list/array of IQ sequences for this mod/snr\n#         for iq_seq in iq_samples_list:\n#             X.append(iq_seq)\n#             Y.append(mod)\n# X = np.array(X) # Shape (num_samples, iq_points)\n# Y = np.array(Y) # Shape (num_samples,)\n# --- End Real RML loading sketch ---\n\n\n# --- Let&#39;s use the fake data for the spectrogram part ---\nall_spectrograms = []\nall_labels = []\nspectrogram_height = 64 # Desired height of spectrogram image\nspectrogram_width = 64  # Desired width of spectrogram image\n\n# Spectrogram parameters - adjust these!\nnperseg = 32 # Affects frequency resolution\nnoverlap = nperseg // 2 # Affects time resolution\n\nprint(&quot;Generating spectrograms...&quot;)\nfor snr, mods in fake_data.items(): # Iterate through SNR levels\n    for mod_type, iq_samples_array in mods.items(): # Iterate through modulation types\n        print(f&quot;  Processing {mod_type} at {snr} dB SNR...&quot;)\n        for iq_sample in iq_samples_array: # Iterate through each IQ sequence\n            # Calculate spectrogram\n            # fs=sample_rate (need to know dataset&#39;s sample rate, let&#39;s assume 1.0 for simplicity here)\n            # Sxx will have shape (n_freq_bins, n_time_bins)\n            f, t_spec, Sxx = spectrogram(iq_sample, fs=1.0, noverlap=noverlap, nperseg=nperseg)\n\n            # Convert to log scale and resize/crop if needed to get consistent dimensions\n            Sxx_log = 10 * np.log10(Sxx + 1e-10)\n\n            # --- Resizing/Cropping (Important for consistent input size) ---\n            # This is a common challenge. Spectrogram output dimensions depend on input length\n            # and STFT parameters. We need a fixed size for CNNs.\n            # A simple approach is cropping or padding. Let&#39;s crop for this example.\n            # Ensure spectrogram_height &lt;= Sxx_log.shape[0] and spectrogram_width &lt;= Sxx_log.shape[1]\n            # In a real scenario, you&#39;d handle varying input IQ lengths.\n            # For standard datasets like RML, IQ lengths are fixed, so Sxx dimensions will be consistent.\n            # Let&#39;s assume Sxx_log is already large enough or we resize properly.\n            # For this demo, let&#39;s just take a slice if it&#39;s too big,\n</code></pre>\n\n                </div>\n             </div>\n         ",
    "module-4": "\n             <div class=\"card main-content-card\"> <!-- Added main-content-card class -->\n                <h1>Module 4: module_4</h1> <!-- Use module title here -->\n                <div class=\"markdown-content\">\n                    <p>Okay, let&#39;s dive deep into Module 4: Building the Target: An AI-Driven RF Classifier System. This module is where we bring together the SDR skills from Module 2 and the AI/ML knowledge from Module 3 to create the core system that we will later attack. Think of this as building the &#39;lock&#39; before we learn to pick it in subsequent modules.</p>\n<hr>\n<h2><strong>Module 4: Building the Target: An AI-Driven RF Classifier System</strong></h2>\n<p><strong>Module Objective:</strong> Integrate SDR data capture with the trained AI model to build a functional, albeit basic, real-time or near-real-time RF signal classification system. This system will serve as the primary target for adversarial attacks.</p>\n<p><strong>Contribution to Capstone:</strong> Completes the <em>target</em> system component of the functional clone. By the end of this module, you will have a running system that listens to RF, processes it, and attempts to identify signals using your previously trained AI model.</p>\n<hr>\n<h3><strong>Introduction: From Digital Dream to RF Reality</strong></h3>\n<p>In Module 3, you trained an AI model to classify RF signals based on processed data (like spectrograms or IQ features). That model was trained on static datasets. Now, we face the challenge of making that model useful in a dynamic RF environment. How do we feed live data from an SDR into our trained model? How do we handle the continuous stream of RF energy? This module is about building the bridge between the digital realm of your trained model and the physical world of radio waves captured by your SDR.</p>\n<p>Our goal isn&#39;t necessarily a production-ready, perfectly optimized real-time system. The goal is a <em>functional clone</em> – a system that works well enough in a controlled environment that we can reliably test adversarial attacks against it. This system will be our primary target in Modules 6 and 7.</p>\n<h3><strong>4.1 Connecting SDR Capture Pipeline to ML Inference Pipeline</strong></h3>\n<p>This is the core architectural challenge. We have two distinct processes:</p>\n<ol>\n<li><strong>SDR Capture:</strong> Reading raw IQ data from the SDR hardware.</li>\n<li><strong>ML Inference:</strong> Taking processed data and feeding it through the trained AI model to get a classification.</li>\n</ol>\n<p>We need a way to connect these two. The simplest approach is a sequential pipeline:</p>\n<pre><code>SDR -&gt; Read Samples -&gt; Process Samples -&gt; Prepare ML Input -&gt; Load Model -&gt; Run Inference -&gt; Get Prediction -&gt; Output Result\n</code></pre>\n<p>This process needs to happen repeatedly to classify a continuous signal or monitor a frequency band.</p>\n<p><strong>Data Flow:</strong></p>\n<ul>\n<li>The SDR captures a chunk of IQ samples over a specific duration.</li>\n<li>This raw IQ data needs to be transformed into the format your Module 3 model expects. If your model used spectrograms, you&#39;ll need to compute a spectrogram from the IQ chunk. If it used raw IQ or time-series features, you&#39;ll process accordingly.</li>\n<li>This processed data (e.g., a NumPy array representing a spectrogram image or IQ features) becomes the input to your loaded ML model.</li>\n<li>The model outputs probabilities for each class.</li>\n<li>You interpret these probabilities (usually by taking the class with the highest probability) and display the result.</li>\n</ul>\n<p><strong>Libraries Involved:</strong></p>\n<ul>\n<li><strong>SDR Interaction:</strong> <code>pyrtlsdr</code>, <code>pyhackrf</code>, <code>pysdr</code>, or similar libraries depending on your SDR.</li>\n<li><strong>Numerical Processing:</strong> <code>NumPy</code> for array manipulation, <code>SciPy</code> for signal processing functions (like FFT, windowing).</li>\n<li><strong>Spectrogram Generation (if needed):</strong> <code>Matplotlib</code> (specifically its backend) or custom <code>NumPy</code>/<code>SciPy</code> code.</li>\n<li><strong>ML Framework:</strong> <code>TensorFlow</code> or <code>PyTorch</code> for loading and running the model.</li>\n</ul>\n<h3><strong>4.2 Data Buffering and Processing for Continuous Classification</strong></h3>\n<p>RF signals don&#39;t arrive in neat little packages perfectly sized for your model&#39;s input. They are continuous. Your SDR reads data in chunks (buffers).</p>\n<p><strong>Handling Chunks:</strong></p>\n<ul>\n<li>Your SDR library will have a function to read a specified number of samples (e.g., <code>sdr.read_samples(num_samples)</code>).</li>\n<li>The <code>num_samples</code> you read will depend on:<ul>\n<li>The sample rate of your SDR.</li>\n<li>The duration of the signal segment you want to classify at a time.</li>\n<li>The input size required by your ML model <em>after</em> processing.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Example: Spectrogram Input</strong></p>\n<p>If your model takes a spectrogram of a certain size (e.g., 128x128 pixels) representing a 10ms signal segment, you need to calculate how many IQ samples correspond to 10ms at your SDR&#39;s sample rate.</p>\n<p><code>Number of Samples = Duration (seconds) * Sample Rate (samples/second)</code></p>\n<p>For a 10ms duration and 2.048 Msps sample rate:<br><code>Number of Samples = 0.010 * 2,048,000 = 20,480 samples</code></p>\n<p>You would read 20,480 IQ samples. Then, you&#39;d process these 20,480 samples to generate a spectrogram. This might involve:</p>\n<ol>\n<li>Splitting the chunk into smaller overlapping segments (e.g., 256 samples with 50% overlap).</li>\n<li>Applying a window function (e.g., Hann window) to each segment.</li>\n<li>Performing an FFT on each windowed segment.</li>\n<li>Taking the magnitude (power spectrum) of the FFT results.</li>\n<li>Stacking these power spectra side-by-side to form the spectrogram image data.</li>\n<li>Resizing or interpolating this data to match the 128x128 input size expected by your model.</li>\n</ol>\n<p><strong>Code Snippet (Illustrative: IQ Capture &amp; Basic Spectrogram Generation Logic)</strong></p>\n<pre><code class=\"language-python\">import numpy as np\nfrom scipy.signal import spectrogram\nimport matplotlib.pyplot as plt\n# Assuming sdr object is initialized and configured (center_freq, sample_rate)\n# from Module 2 setup\n\n# --- Configuration ---\nsample_rate = 2.048e6 # Example sample rate (2.048 Msps)\ncenter_freq = 433e6   # Example center frequency (433 MHz ISM band)\nclassification_duration = 0.01 # Classify every 10 ms of signal\nchunk_size = int(sample_rate * classification_duration) # Number of samples per classification chunk\n\n# Spectrogram parameters (adjust based on desired resolution and model input size)\nnperseg = 256 # Samples per segment for FFT\nnoverlap = nperseg // 2 # 50% overlap\nnfft = nperseg # Number of FFT points\n\n# --- Capture and Processing Loop (Conceptual) ---\n# In a real system, this would run continuously or triggered\nprint(f&quot;Capturing {chunk_size} samples ({classification_duration*1000} ms) at {center_freq/1e6} MHz...&quot;)\n\ntry:\n    # Capture a chunk of complex IQ data\n    # NOTE: The actual read function depends on your SDR library (pyrtlsdr, pyhackrf, etc.)\n    # Example using a hypothetical sdr.read_samples function:\n    # iq_samples = sdr.read_samples(chunk_size)\n\n    # For demonstration, let&#39;s create some dummy IQ data\n    t = np.arange(chunk_size) / sample_rate\n    # Simple tone + noise\n    iq_samples = 0.5 * np.exp(1j * 2 * np.pi * 100000 * t) + \\\n                 (np.random.randn(chunk_size) + 1j * np.random.randn(chunk_size)) * 0.1\n\n\n    # --- Processing Step: Generate Spectrogram ---\n    # f: Array of sample frequencies\n    # t_spec: Array of segment times\n    # Sxx: Spectrogram (power spectral density)\n    f, t_spec, Sxx = spectrogram(iq_samples, fs=sample_rate, nperseg=nperseg,\n                                 noverlap=noverlap, nfft=nfft, mode=&#39;psd&#39;)\n\n    # Convert power spectral density to dB for better visualization/ML input\n    # Add a small epsilon to avoid log(0)\n    Sxx_db = 10 * np.log10(Sxx + 1e-9)\n\n    # Sxx_db now represents your spectrogram data.\n    # Its shape will be (number_of_frequency_bins, number_of_time_segments)\n    # Example: (nfft/2 + 1, (chunk_size - noverlap) // (nperseg - noverlap))\n\n    print(f&quot;Generated spectrogram data with shape: {Sxx_db.shape}&quot;)\n\n    # --- Further Processing for ML Input ---\n    # Your model expects a specific input shape (e.g., 128x128 image-like).\n    # You might need to resize, crop, or pad Sxx_db.\n    # You also need to normalize/scale the values (e.g., between 0 and 1 or -1 and 1)\n    # Example: Simple resizing (this is often lossy, training data processing should match)\n    from skimage.transform import resize # requires scikit-image installed\n    target_height, target_width = 128, 128 # Assuming your model expects 128x128\n    Sxx_resized = resize(Sxx_db, (target_height, target_width), anti_aliasing=True)\n\n    # Normalize (e.g., min-max scaling)\n    Sxx_normalized = (Sxx_resized - Sxx_resized.min()) / (Sxx_resized.max() - Sxx_resized.min())\n\n    # Your model might expect a channel dimension (e.g., for CNNs)\n    # If grayscale spectrogram, add a channel dimension: (height, width, 1)\n    ml_input = np.expand_dims(Sxx_normalized, axis=-1)\n\n    # Add batch dimension (model expects input shape like (batch_size, height, width, channels))\n    ml_input = np.expand_dims(ml_input, axis=0) # Batch size 1\n\n    print(f&quot;Prepared ML input data with shape: {ml_input.shape}&quot;)\n\n    # --- Visualization (Optional, for debugging/display) ---\n    plt.figure(figsize=(10, 4))\n    plt.imshow(Sxx_db, aspect=&#39;auto&#39;, origin=&#39;lower&#39;,\n               extent=[t_spec[0], t_spec[-1], f[0]/1e6, f[-1]/1e6],\n               cmap=&#39;viridis&#39;)\n    plt.colorbar(label=&#39;Power/Frequency (dB/Hz)&#39;)\n    plt.xlabel(&#39;Time (s)&#39;)\n    plt.ylabel(&#39;Frequency (MHz)&#39;)\n    plt.title(&#39;Spectrogram of Captured RF Data Chunk&#39;)\n    plt.show()\n\nexcept Exception as e:\n    print(f&quot;An error occurred during capture or processing: {e}&quot;)\n    # Handle SDR disconnection, permission issues, etc.\n\n# NOTE: In a real system, this capture and processing would be inside a loop.\n</code></pre>\n<p><strong>Teaching Tip:</strong> Emphasize that the exact processing steps (windowing, FFT size, overlap, normalization, resizing) <em>must</em> match the processing steps used when creating the training data for the Module 3 model. Inconsistent processing will lead to poor performance.</p>\n<h3><strong>4.3 Handling Real-World RF Data Variability</strong></h3>\n<p>The clean, synthetic signals or carefully curated datasets used for training in M3 don&#39;t perfectly represent the chaos of the real RF environment.</p>\n<ul>\n<li><strong>Noise:</strong> The noise floor varies. Your model was likely trained on data with a relatively consistent noise level. Real noise fluctuates.</li>\n<li><strong>Interference:</strong> Other signals might bleed into your target frequency band.</li>\n<li><strong>Signal Strength:</strong> The power of the signal you&#39;re trying to classify will change depending on distance, obstacles, antenna orientation, etc.</li>\n<li><strong>Timing:</strong> Signals might start or stop mid-chunk.</li>\n</ul>\n<p><strong>Impact on Classification:</strong> These variations can significantly degrade the performance of a model trained on clean data. A signal that was perfectly classifiable during training might look very different to the model when captured live with real-world noise and varying power levels.</p>\n<p><strong>Mitigation (for building a robust target, though not strictly required for the <em>basic</em> target):</strong></p>\n<ul>\n<li><strong>Normalization:</strong> Ensure your processing pipeline includes robust normalization (e.g., min-max scaling, z-score normalization) that can handle variations in signal strength and noise floor. Apply the <em>same</em> normalization used during training.</li>\n<li><strong>Data Augmentation:</strong> (This would ideally be done in M3 training) Train your model on data augmented with realistic noise, fading, and interference.</li>\n<li><strong>Thresholding:</strong> You might implement logic to only attempt classification if the signal power in the chunk exceeds a certain threshold, avoiding classifying pure noise.</li>\n</ul>\n<p>For the initial <em>target</em> system, focus on getting the pipeline working with <em>some</em> variability. We will exploit this variability and the model&#39;s potential lack of robustness in later modules.</p>\n<h3><strong>4.4 Loading Pre-trained Models for Inference</strong></h3>\n<p>This is straightforward using your chosen ML framework. You saved your trained model in Module 3. Now you load it.</p>\n<p><strong>Code Snippet (TensorFlow/Keras)</strong></p>\n<pre><code class=\"language-python\">import tensorflow as tf\n\n# --- Configuration ---\nmodel_path = &#39;path/to/your/saved_model&#39; # Path where you saved your model in M3\n\n# --- Load Model ---\nprint(f&quot;Loading model from: {model_path}&quot;)\ntry:\n    model = tf.keras.models.load_model(model_path)\n    print(&quot;Model loaded successfully.&quot;)\n    model.summary() # Print model architecture to verify\nexcept Exception as e:\n    print(f&quot;Error loading model: {e}&quot;)\n    print(&quot;Make sure the model path is correct and the model was saved correctly.&quot;)\n    model = None # Ensure model is None if loading fails\n\n# You can now use this &#39;model&#39; object to make predictions\n# model.predict(ml_input) # Assuming ml_input is prepared as in 4.2\n</code></pre>\n<p><strong>Code Snippet (PyTorch)</strong></p>\n<pre><code class=\"language-python\">import torch\nimport torch.nn as nn # Needed if your model architecture is defined in a class\n\n# Assuming your model architecture class (e.g., SimpleCNN) is defined\n# from your Module 3 training code\n# from your_module3_code import SimpleCNN # Example import\n\n# --- Configuration ---\nmodel_path = &#39;path/to/your/saved_model.pth&#39; # Path where you saved your model in M3\n\n# --- Load Model ---\nprint(f&quot;Loading model from: {model_path}&quot;)\ntry:\n    # If you saved the state_dict\n    # model = SimpleCNN(...) # Instantiate your model architecture first\n    # model.load_state_dict(torch.load(model_path))\n\n    # If you saved the entire model object (less recommended but simpler for demos)\n    model = torch.load(model_path)\n\n    model.eval() # Set model to evaluation mode (important for dropout, batchnorm)\n    print(&quot;Model loaded successfully.&quot;)\n    # print(model) # Print model architecture (less detailed than Keras summary)\nexcept Exception as e:\n    print(f&quot;Error loading model: {e}&quot;)\n    print(&quot;Make sure the model path is correct and the model was saved correctly.&quot;)\n    model = None # Ensure model is None if loading fails\n\n# You can now use this &#39;model&#39; object to make predictions\n# with torch.no_grad(): # Inference should be done without tracking gradients\n#     output = model(ml_input) # Assuming ml_input is a PyTorch tensor\n</code></pre>\n<p><strong>Teaching Tip:</strong> Remind learners that the model architecture <em>must</em> be compatible with the saved weights/state. If they saved only the state dictionary in PyTorch, they need to instantiate the model class first. Saving the whole model object is simpler for this course&#39;s purpose if using PyTorch.</p>\n<h3><strong>4.5 Implementing the Classification Logic based on SDR Input</strong></h3>\n<p>Once you have the <code>ml_input</code> data ready (processed chunk of SDR data in the correct format) and the <code>model</code> loaded, the next step is running inference and interpreting the output.</p>\n<p><strong>Inference:</strong></p>\n<p>The model will output a tensor/array of probabilities (if using softmax in the output layer) or raw scores (logits) for each class.</p>\n<ul>\n<li><strong>TensorFlow/Keras:</strong> <code>predictions = model.predict(ml_input)</code></li>\n<li><strong>PyTorch:</strong> <code>with torch.no_grad(): output = model(ml_input)</code></li>\n</ul>\n<p><strong>Interpretation:</strong></p>\n<p>Assuming the model outputs probabilities (summing to 1 across classes), the predicted class is the one with the highest probability.</p>\n<ul>\n<li>Use <code>np.argmax()</code> to find the index of the maximum probability.</li>\n<li>Map this index back to the corresponding class label (e.g., using a list or dictionary created during M3 training).</li>\n</ul>\n<p><strong>Code Snippet (Classification Logic)</strong></p>\n<pre><code class=\"language-python\">import numpy as np\n\n# Assuming &#39;model&#39; is loaded (TensorFlow or PyTorch)\n# Assuming &#39;ml_input&#39; is prepared (NumPy array for TF, PyTorch tensor for PyTorch)\n# Assuming you have a list mapping index to label from M3\nlabel_map = [&#39;ClassA&#39;, &#39;ClassB&#39;, &#39;ClassC&#39;] # Example: replace with your actual labels\n\n# --- Run Inference ---\nif model is not None:\n    try:\n        # --- TensorFlow ---\n        if isinstance(model, tf.keras.Model):\n             # model.predict expects a batch, ml_input should have batch dimension\n             predictions = model.predict(ml_input)\n             # predictions shape will be (batch_size, num_classes)\n             # Assuming batch_size = 1\n             probabilities = predictions[0] # Get probabilities for the single input\n\n             predicted_index = np.argmax(probabilities)\n             predicted_label = label_map[predicted_index]\n             confidence = probabilities[predicted_index]\n\n             print(f&quot;Prediction: {predicted_label} (Confidence: {confidence:.4f})&quot;)\n\n        # --- PyTorch ---\n        elif isinstance(model, torch.nn.Module):\n            # PyTorch model expects a tensor\n            if not isinstance(ml_input, torch.Tensor):\n                 # Convert NumPy array to PyTorch tensor if needed\n                 ml_input = torch.from_numpy(ml_input).float() # Adjust dtype if necessary\n\n            with torch.no_grad():\n                 output = model(ml_input) # output shape will be (batch_size, num_classes)\n\n            # Assuming batch_size = 1\n            output = output.squeeze(0) # Remove batch dimension\n\n            # If model output is logits, apply softmax to get probabilities\n            if not torch.isclose(output.sum(), torch.tensor(1.0)): # Simple check if it&#39;s likely not probabilities\n                 probabilities = torch.softmax(output, dim=0)\n            else:\n                 probabilities = output # Assume it&#39;s already probabilities\n\n            predicted_index = torch.argmax(probabilities).item()\n            predicted_label = label_map[predicted_index]\n            confidence = probabilities[predicted_index].item()\n\n            print(f&quot;Prediction: {predicted_label} (Confidence: {confidence:.4f})&quot;)\n\n        else:\n            print(&quot;Model is of an unknown type (not TF Keras or PyTorch Module).&quot;)\n\n    except Exception as e:\n        print(f&quot;Error during inference: {e}&quot;)\n        print(&quot;Check input data shape and model compatibility.&quot;)\n\nelse:\n    print(&quot;Model not loaded, cannot perform inference.&quot;)\n</code></pre>\n<p><strong>Teaching Tip:</strong> Stress the importance of the <code>label_map</code> matching the order of output classes from the model trained in M3.</p>\n<h3><strong>4.6 Visualization of Classification Results</strong></h3>\n<p>Seeing is believing! Simply printing the prediction is useful, but visualizing it alongside the spectrum data provides valuable context and helps debug.</p>\n<p><strong>Visualization Options:</strong></p>\n<ul>\n<li><strong>Text Overlay on Plot:</strong> If you are displaying a real-time spectrum or waterfall plot (using <code>matplotlib.animation</code>), you can update a text annotation on the plot with the current prediction.</li>\n<li><strong>Dedicated Display Area:</strong> A simple GUI (using <code>Tkinter</code>, <code>PyQt</code>, or <code>Streamlit</code> for a web interface) could show the current spectrum chunk, the classification result, and maybe a confidence score.</li>\n<li><strong>Command Line Output:</strong> For simplicity, just printing the prediction and confidence is sufficient for the core functionality of the target system.</li>\n</ul>\n<p><strong>Code Snippet (Basic Matplotlib Plot Update - Illustrative)</strong></p>\n<p>This requires setting up an animated plot, which is more complex. Here&#39;s a simplified idea of updating a title or text:</p>\n<pre><code class=\"language-python\"># This is conceptual and requires a Matplotlib animation loop setup\n# See Matplotlib documentation for real-time plotting/animation examples\n\n# Assuming &#39;ax&#39; is a Matplotlib axes object displaying the spectrum/spectrogram\n# Assuming &#39;predicted_label&#39; and &#39;confidence&#39; are available from 4.5\n\n# Option 1: Update plot title\nax.set_title(f&quot;Classified as: {predicted_label} ({confidence:.2f})&quot;)\n\n# Option 2: Update a text annotation\n# You would need to create a text object initially and update it\n# text_annotation = ax.text(x_pos, y_pos, &quot;&quot;, color=&#39;white&#39;)\n# text_annotation.set_text(f&quot;Prediction: {predicted_label}\\nConfidence: {confidence:.2f}&quot;)\n\n# Remember to redraw the canvas in the animation loop\n# fig.canvas.draw()\n# fig.canvas.flush_events()\n</code></pre>\n<p><strong>Teaching Tip:</strong> For the Module Project, simple command-line output is perfectly acceptable. Real-time plotting adds complexity that isn&#39;t core to the adversarial attack concept itself. Focus on getting the capture -&gt; process -&gt; classify loop working reliably first.</p>\n<h3><strong>4.7 Setting up a Controlled Test Environment</strong></h3>\n<p>This is absolutely critical for developing and testing adversarial attacks. The unpredictable nature of the real RF environment makes repeatable experiments very difficult.</p>\n<p><strong>Why Controlled?</strong></p>\n<ul>\n<li><strong>Repeatability:</strong> You need to be able to feed the &quot;same&quot; signal to the target system multiple times and observe consistent results (or consistent misclassifications from your attack).</li>\n<li><strong>Isolation:</strong> Minimize interference from external sources (WiFi, Bluetooth, local broadcasts) that could affect the signal or confuse your target classifier.</li>\n<li><strong>Known Signal Source:</strong> You need a reliable way to generate the clean signals that your target system is supposed to classify correctly before you add adversarial perturbations.</li>\n<li><strong>Controlled Signal Strength:</strong> Adversarial attacks are often sensitive to the signal-to-noise ratio (SNR). Controlling the signal strength reaching the target RX antenna is important.</li>\n</ul>\n<p><strong>How to Build a Controlled Test Environment:</strong></p>\n<ol>\n<li><strong>Two SDRs:</strong> You ideally need at least two SDRs.<ul>\n<li>One <strong>Transmitter (TX) SDR:</strong> To generate the &quot;clean&quot; signal you want to classify (or later, the adversarial signal). Requires a TX-capable SDR like HackRF One, LimeSDR, ADALM-PLUTO.</li>\n<li>One <strong>Receiver (RX) SDR:</strong> Connected to your target classification system. Can be an RTL-SDR if your TX SDR covers the same frequency range, or another TX/RX SDR.</li>\n</ul>\n</li>\n<li><strong>Cables and Attenuators:</strong> The <em>most</em> controlled environment uses cables to directly connect the TX SDR&#39;s output to the RX SDR&#39;s input. <strong>WARNING:</strong> SDRs are sensitive! You <em>must</em> use appropriate in-line attenuators to reduce the TX power significantly before it reaches the RX input. Transmitting directly into an RX port without attenuation can damage the receiver. Attenuators (e.g., 30dB, 40dB, 50dB pads) are essential.</li>\n<li><strong>Antennas (Less Controlled):</strong> If direct cabling isn&#39;t feasible or you want a slightly more realistic &quot;over-the-air&quot; feel, use antennas. Place the TX and RX antennas very close together (e.g., inches or a few feet apart).</li>\n<li><strong>Shielding (Ideal but Hard):</strong> A Faraday cage or shielded box can significantly reduce external interference. This is often impractical for a home lab setup but is the gold standard for controlled RF testing.</li>\n<li><strong>Fixed Parameters:</strong> Keep SDR parameters (center frequency, sample rate, gain) constant during experiments.</li>\n</ol>\n<p><strong>Setup Diagram (Conceptual - Cabling Method):</strong></p>\n<pre><code>[TX SDR] --&gt; [Attenuator Bank (e.g., 40dB + 10dB)] --&gt; [RX SDR] --&gt; [Computer Running Target System]\n</code></pre>\n<p><strong>Setup Diagram (Conceptual - Close Antennas):</strong></p>\n<pre><code>[TX SDR] -- Antenna ]))) ((([ Antenna -- [RX SDR] --&gt; [Computer Running Target System]\n</code></pre>\n<p><strong>Teaching Tip:</strong> Strongly advise learners to start with the cabled setup with <em>significant</em> attenuation. It removes the variability of the air interface and makes debugging the core capture/process/classify pipeline much easier. Emphasize starting with very low transmit power on the TX SDR as well.</p>\n<h3><strong>Module Project/Exercise: Building the Target System</strong></h3>\n<p>Your goal is to combine the pieces discussed into a functional script or program.</p>\n<p><strong>Steps:</strong></p>\n<ol>\n<li><p><strong>Review and Prepare:</strong></p>\n<ul>\n<li>Ensure you have a trained ML model file saved from Module 3.</li>\n<li>Have your SDR setup working (tested in Module 2).</li>\n<li>Make sure all necessary libraries are installed (<code>pyrtlsdr</code>/<code>pyhackrf</code>/<code>pysdr</code>, <code>numpy</code>, <code>scipy</code>, <code>matplotlib</code>, <code>tensorflow</code>/<code>pytorch</code>, <code>scikit-image</code> if using <code>resize</code>).</li>\n</ul>\n</li>\n<li><p><strong>Load the Model:</strong> Write the code to load your trained model using the path where you saved it. Include error handling.</p>\n</li>\n<li><p><strong>Initialize the RX SDR:</strong></p>\n<ul>\n<li>Write code to initialize your SDR in receive mode.</li>\n<li>Set the center frequency and sample rate. Choose a frequency you can control (e.g., an ISM band frequency like 433 MHz or 915 MHz if allowed in your region, or a frequency you&#39;ll generate with your TX SDR).</li>\n<li>Set the gain (experiment to find a good level that isn&#39;t saturated but captures signals).</li>\n</ul>\n</li>\n<li><p><strong>Implement the Main Loop:</strong> Create a loop that runs continuously (or for a set duration).</p>\n</li>\n<li><p><strong>Inside the Loop:</strong></p>\n<ul>\n<li><strong>Capture Data:</strong> Read a chunk of IQ samples from the SDR. Calculate the required chunk size based on your desired classification duration and sample rate (as discussed in 4.2).</li>\n<li><strong>Process Data:</strong> Apply the <em>exact same</em> processing steps used to prepare your training data in Module 3. If using spectrograms:<ul>\n<li>Generate the spectrogram data (<code>scipy.signal.spectrogram</code>).</li>\n<li>Convert to dB (optional but common).</li>\n<li>Resize/reshape to the model&#39;s input dimensions (<code>skimage.transform.resize</code> is an option, or custom slicing/padding).</li>\n<li>Normalize/scale the data.</li>\n<li>Add batch and channel dimensions if required by your model (<code>numpy.expand_dims</code>).</li>\n</ul>\n</li>\n<li><strong>Run Inference:</strong> Feed the processed data (<code>ml_input</code>) into the loaded model (<code>model.predict()</code> or `model(</li>\n</ul>\n</li>\n</ol>\n\n                </div>\n             </div>\n         ",
    "module-5": "\n             <div class=\"card main-content-card\"> <!-- Added main-content-card class -->\n                <h1>Module 5: module_5</h1> <!-- Use module title here -->\n                <div class=\"markdown-content\">\n                    <p>Okay class, welcome to Module 5! We&#39;ve built our target system – a foundational AI classifier for RF signals. Now, it&#39;s time to switch gears and think like an adversary. This module is where we delve into the fascinating, sometimes counter-intuitive, world of Adversarial Machine Learning (AML). We&#39;ll learn <em>why</em> AI models, despite their impressive capabilities, can be surprisingly fragile and susceptible to carefully crafted inputs designed to deceive them.</p>\n<p>Think of it like this: You&#39;ve built a highly skilled guard dog (your AI classifier) trained to recognize specific scents (RF signal patterns). Now, we&#39;re going to learn how to create a synthetic scent that smells <em>just slightly</em> different from a recognized friendly scent, but is different <em>enough</em> to fool the dog into thinking it&#39;s smelling something else entirely, or perhaps nothing at all.</p>\n<p>This module lays the theoretical and practical groundwork for generating the adversarial signals we&#39;ll eventually transmit over the air. While our primary goal is RF security, the concepts of AML are universal. For simplicity and to leverage readily available tools and datasets, our hands-on coding exercises in this module will focus on generating adversarial examples for image classifiers. This is a standard practice in learning AML, as the principles transfer directly to other data types like RF signals, and visualizing image perturbations is often more intuitive initially.</p>\n<p>Let&#39;s dive in!</p>\n<hr>\n<h2><strong>Module 5: The Adversarial Mindset: Introduction to AI Attacks</strong></h2>\n<ul>\n<li><strong>Module Objective:</strong> Dive into the theory and common techniques of Adversarial Machine Learning, understanding <em>why</em> AI models are vulnerable and the different goals of an attacker.</li>\n</ul>\n<h3><strong>5.1 What are Adversarial Examples? Small Perturbations, Large Impacts</strong></h3>\n<ul>\n<li><p><strong>Concept:</strong> Adversarial examples are inputs to an AI model that have been intentionally modified to cause the model to make a specific error (usually a misclassification). The key characteristic is that these modifications are often <em>imperceptible</em> or <em>insignious</em> to humans, but drastically alter the model&#39;s output.</p>\n</li>\n<li><p><strong>Why is this surprising?</strong> We often think of AI models, especially deep learning ones, as learning robust, high-level features similar to how humans perceive things. The existence of adversarial examples shows that they sometimes rely on subtle, brittle patterns that don&#39;t align with human intuition.</p>\n</li>\n<li><p><strong>Classic Example (Image Domain):</strong> The most famous example is modifying an image of a panda by adding a small amount of carefully calculated noise. To a human, it still looks clearly like a panda. But to a trained image classifier, it might confidently classify the image as a gibbon.</p>\n<ul>\n<li><em>Original Image:</em> Panda (classified correctly)</li>\n<li><em>Perturbation:</em> Small, almost invisible noise pattern.</li>\n<li><em>Adversarial Image:</em> Original Image + Perturbation = Still looks like a panda to you and me.</li>\n<li><em>Model Output:</em> Gibbon (misclassified with high confidence).</li>\n</ul>\n</li>\n<li><p><strong>Relevance to RF:</strong> Imagine our RF classifier is trained to distinguish between different modulation types (FM, FSK, QPSK, etc.). An adversarial RF attack would involve taking a legitimate signal (say, a standard FM signal) and adding a carefully crafted, low-power noise or perturbation. To a standard RF receiver or even a human looking at a spectrogram, it might still look like FM, but the AI classifier might misinterpret it as FSK, or perhaps just noise.</p>\n</li>\n</ul>\n<h3><strong>5.2 The Geometry of Adversarial Attacks: Decision Boundaries</strong></h3>\n<ul>\n<li><p><strong>Concept:</strong> Machine learning classifiers work by learning complex <em>decision boundaries</em> in a high-dimensional feature space. These boundaries separate inputs belonging to different classes.</p>\n</li>\n<li><p><strong>How it Works:</strong> During training, the model adjusts its parameters (weights and biases) to define these boundaries based on the training data. For a new input, the model determines which side of the boundary it falls on to make a prediction.</p>\n</li>\n<li><p><strong>Vulnerability:</strong> Deep neural networks often learn decision boundaries that are complex and, crucially, can be relatively close to the training data points. Adversarial attacks exploit this. By adding a small perturbation, the attacker pushes the input data point across a decision boundary into a region that the model associates with a different class.</p>\n</li>\n<li><p><strong>Linearity:</strong> Another factor is the (piecewise) linear nature of many neural network operations. While the overall function is non-linear, locally, the gradients are well-defined. Gradient-based attacks (like FGSM or PGD) work by calculating the direction of the steepest increase in the loss function (which corresponds to moving the data point towards a misclassification) <em>with respect to the input data</em>. A small step in this direction can quickly cross a decision boundary, especially in high-dimensional spaces.</p>\n</li>\n</ul>\n<h3><strong>5.3 Attack Goals: What Does the Adversary Want?</strong></h3>\n<p>The goal of an adversarial attack isn&#39;t just to break the model; it&#39;s usually a specific kind of failure.</p>\n<ul>\n<li><p><strong>Untargeted Misclassification:</strong> The simplest goal. The attacker wants the model to misclassify the input into <em>any</em> wrong class.</p>\n<ul>\n<li><em>RF Example:</em> Make the AI classifier think a legitimate WiFi signal is <em>anything other than WiFi</em>.</li>\n</ul>\n</li>\n<li><p><strong>Targeted Misclassification:</strong> A more sophisticated goal. The attacker wants the model to misclassify the input into a <em>specific</em> target class chosen by the attacker.</p>\n<ul>\n<li><em>RF Example:</em> Make the AI classifier think a legitimate WiFi signal is specifically an LTE signal. This could be used to trick a cognitive radio system into allocating spectrum incorrectly.</li>\n</ul>\n</li>\n<li><p><strong>Evasion:</strong> The attacker wants the input to be classified as an &quot;unknown&quot; or &quot;noise&quot; class, or simply to avoid detection entirely by appearing benign.</p>\n<ul>\n<li><em>RF Example:</em> Make a malicious or unauthorized signal appear as background noise or a known, harmless signal type to avoid triggering alarms in a spectrum monitoring system.</li>\n</ul>\n</li>\n</ul>\n<p>In the context of our course, we&#39;ll primarily focus on demonstrating targeted or untargeted misclassification of known signal types as a proof of concept. Evasion attacks are also highly relevant but can sometimes require more complex techniques.</p>\n<h3><strong>5.4 Threat Models: What Does the Attacker Know?</strong></h3>\n<p>The attacker&#39;s knowledge about the target model significantly impacts the types of attacks they can perform.</p>\n<ul>\n<li><p><strong>White-box Attack:</strong> The attacker has full knowledge of the target model. This includes:</p>\n<ul>\n<li>Model architecture (layers, activation functions, etc.).</li>\n<li>Model parameters (weights and biases).</li>\n<li>Training data (or at least the distribution it came from).</li>\n<li>The specific machine learning framework used.</li>\n<li><em>Advantage:</em> Attackers can calculate exact gradients and craft highly effective, minimal perturbations.</li>\n<li><em>Realism:</em> Less common in real-world black-box systems, but crucial for <em>research</em> to understand vulnerabilities and potential defenses. Our initial attacks will be white-box.</li>\n</ul>\n</li>\n<li><p><strong>Black-box Attack:</strong> The attacker has limited or no knowledge of the internal workings of the target model. They can only interact with the model by sending inputs and observing the outputs (e.g., predicted class, confidence scores, or raw logits).</p>\n<ul>\n<li><em>Challenge:</em> Cannot directly calculate gradients with respect to the input data using the target model&#39;s parameters.</li>\n<li><em>Techniques:</em><ul>\n<li><strong>Transferability:</strong> Exploit the fact that adversarial examples often transfer between different models. An attacker might train a <em>surrogate</em> model (that they control) to mimic the target model&#39;s behavior and generate adversarial examples using a white-box attack on the surrogate. These examples might then successfully fool the real black-box target.</li>\n<li><strong>Query-based Attacks:</strong> Iteratively probe the target model with many inputs and observe the outputs to estimate the gradient or build a local approximation of the decision boundary. This requires many queries and can be computationally expensive.</li>\n</ul>\n</li>\n<li><em>Realism:</em> More representative of real-world scenarios where attackers don&#39;t have access to model internals (e.g., attacking a deployed system via its API or RF interface).</li>\n</ul>\n</li>\n</ul>\n<p>For this course, we will focus on white-box attacks initially because they are the easiest to understand and implement. Module 8 might briefly touch upon the concept of transferability as a potential extension.</p>\n<h3><strong>5.5 Common White-box Attack Techniques</strong></h3>\n<p>These techniques leverage the attacker&#39;s knowledge of the model&#39;s internal structure and parameters, particularly using gradients.</p>\n<ul>\n<li><p><strong>Fast Gradient Sign Method (FGSM):</strong></p>\n<ul>\n<li><strong>Idea:</strong> A simple, one-step attack. Calculate the gradient of the loss function with respect to the input data. The sign of this gradient indicates the direction in the input space that maximally increases the loss (i.e., makes the correct class less likely). Take a step in this direction, scaled by a small value <code>epsilon</code>.</li>\n<li><strong>Formula:</strong><br><code>adversarial_example = original_input + epsilon * sign(gradient(Loss(original_input, true_label), original_input))</code></li>\n<li><strong>Explanation:</strong><ul>\n<li><code>Loss(...)</code>: The model&#39;s loss function (e.g., cross-entropy).</li>\n<li><code>gradient(Loss, original_input)</code>: The gradient of the loss with respect to the input data. This tells us how much the loss changes if we slightly change each element of the input.</li>\n<li><code>sign(...)</code>: Takes the sign (+1 or -1) of each element in the gradient. This gives us the <em>direction</em> to push the input data to increase the loss.</li>\n<li><code>epsilon</code>: A small scalar value that controls the magnitude of the perturbation. A larger <code>epsilon</code> usually results in a more effective attack but a more noticeable perturbation.</li>\n</ul>\n</li>\n<li><strong>Pros:</strong> Simple, fast to compute.</li>\n<li><strong>Cons:</strong> Often results in noticeable perturbations, less effective than iterative methods, less likely to achieve targeted misclassification reliably.</li>\n</ul>\n</li>\n<li><p><strong>Projected Gradient Descent (PGD):</strong></p>\n<ul>\n<li><strong>Idea:</strong> An iterative attack. Instead of one large step, PGD takes multiple small steps. It starts with a small random perturbation and then repeatedly applies a small gradient step (like FGM, the non-sign version) and <em>projects</em> the result back into an allowed perturbation region (e.g., an L-infinity ball around the original input). This projection ensures the total perturbation remains bounded.</li>\n<li><strong>Process:</strong><ol>\n<li>Start with <code>adversarial_example = original_input + random_noise</code> (where noise is within a small bound).</li>\n<li>Repeat for <code>N</code> iterations:<ul>\n<li>Calculate <code>gradient = gradient(Loss(adversarial_example, true_label), adversarial_example)</code>.</li>\n<li>Update <code>adversarial_example = adversarial_example + step_size * sign(gradient)</code>. (Sometimes the sign is omitted, using just the gradient).</li>\n<li>Project <code>adversarial_example</code> back onto the allowed perturbation set (e.g., ensure <code>abs(adversarial_example - original_input) &lt;= epsilon</code>).</li>\n</ul>\n</li>\n</ol>\n</li>\n<li><strong>Pros:</strong> Generally more effective and produces less noticeable perturbations than FGSM, considered a strong first-order adversary.</li>\n<li><strong>Cons:</strong> More computationally expensive than FGSM due to iterations.</li>\n</ul>\n</li>\n<li><p><strong>DeepFool:</strong></p>\n<ul>\n<li><strong>Idea:</strong> Finds the <em>minimum</em> perturbation needed to cross a decision boundary. It iteratively linearizes the classifier around the current point and moves the point orthogonally to the nearest hyperplane until it crosses a boundary.</li>\n<li><strong>Pros:</strong> Aims for minimal perturbation, often more efficient than PGD in finding <em>any</em> misclassification boundary.</li>\n<li><strong>Cons:</strong> Can be more complex to implement than FGSM/PGD, primarily designed for untargeted attacks.</li>\n</ul>\n</li>\n</ul>\n<p>For our practical exercise, we will focus on implementing FGSM due to its simplicity, making it an excellent starting point to grasp the core concept of gradient-based adversarial generation.</p>\n<h3><strong>5.6 Common Black-box Attack Techniques (Briefly)</strong></h3>\n<p>As mentioned under Threat Models, black-box attacks are harder as they can&#39;t directly use gradients of the target model.</p>\n<ul>\n<li><strong>Transferability:</strong> Generate adversarial examples using a white-box attack on a <em>different</em> model (a surrogate) trained on similar data or performing the same task. Then, use these examples against the black-box target. This works surprisingly well because different models often learn similar decision boundaries or rely on similar features.</li>\n<li><strong>Query-based (e.g., Boundary Attack, HopSkipJumpAttack):</strong> These methods interact with the model by sending queries and observing the response. They use this information to estimate gradients or find the shortest path from a misclassified point back to the boundary of the original class, then step just across it. These require many interactions with the target model.</li>\n</ul>\n<p>We won&#39;t implement these in detail, but it&#39;s important to know they exist and are relevant for real-world scenarios where the attacker doesn&#39;t have white-box access.</p>\n<h3><strong>5.7 Metrics for Adversarial Attacks</strong></h3>\n<p>How do we measure the success and quality of an adversarial attack?</p>\n<ul>\n<li><strong>Success Rate:</strong> The percentage of adversarial examples that successfully fool the target model into making the desired misclassification (targeted) or any misclassification (untargeted).</li>\n<li><strong>Perturbation Size:</strong> How &quot;big&quot; is the added noise? We want the perturbation to be minimal (ideally imperceptible) to be stealthy. Common ways to measure the size of the perturbation vector (<code>delta = adversarial_example - original_input</code>) are using Lp norms:<ul>\n<li><strong>L-infinity norm (<code>||delta||_inf</code>):</strong> The maximum absolute value of any element in the perturbation vector. This measures the largest change applied to any single feature (e.g., pixel value, IQ sample). It&#39;s often used because it&#39;s easy to constrain (e.g., ensure no pixel changes by more than <code>epsilon</code>).</li>\n<li><strong>L-2 norm (<code>||delta||_2</code>):</strong> The Euclidean distance of the perturbation vector (<code>sqrt(sum(delta^2))</code>). This measures the overall &quot;energy&quot; or magnitude of the perturbation across all features.</li>\n<li><strong>L-0 norm (<code>||delta||_0</code>):</strong> The number of non-zero elements in the perturbation vector. This measures how sparse the perturbation is (how many features were changed).</li>\n</ul>\n</li>\n</ul>\n<p>For RF signals, these norms will eventually need to be related to physical constraints like power limits or bandwidth limits, which we&#39;ll discuss in Module 6. But for now, thinking about L-infinity (maximum change per sample) and L-2 (total energy of the perturbation) is useful.</p>\n<h3><strong>5.8 Case Study: Adversarial Attacks in the Image Domain</strong></h3>\n<p>We touched on the panda/gibbon example. Let&#39;s consider others and their implications:</p>\n<ul>\n<li><strong>Traffic Signs:</strong> Researchers have shown that small stickers or graffiti on stop signs can cause autonomous vehicle vision systems to misclassify them as speed limit signs. The implications for safety are obvious and severe.</li>\n<li><strong>Facial Recognition:</strong> Perturbations added to glasses or makeup can cause facial recognition systems to fail to identify a person or misidentify them as someone else entirely.</li>\n<li><strong>Object Detection:</strong> Adding adversarial patterns to clothing or objects can make them &quot;invisible&quot; to object detection systems.</li>\n</ul>\n<p>These examples highlight that adversarial vulnerabilities are not just theoretical curiosities; they pose real security and safety risks in deployed AI systems across various domains. Our focus is RF, but the underlying principles of manipulating input data to exploit model weaknesses are the same.</p>\n<h3><strong>5.9 Ethical and Legal Considerations (Revisited)</strong></h3>\n<p>As we move into generating attacks, it&#39;s crucial to reiterate the ethical responsibilities. Learning these techniques is for understanding vulnerabilities and developing defenses. Any practical application <em>must</em> be confined to controlled environments, with explicit permission, and within legal boundaries. Unauthorized RF transmission is illegal and can interfere with critical services. Always check local regulations (e.g., FCC rules in the US). The goal is responsible research and disclosure, not malicious activity.</p>\n<h3><strong>5.10 Setting up the Environment (AML Specific)</strong></h3>\n<p>We&#39;ll continue using our Python environment from Module 1, but we&#39;ll add libraries specifically useful for Machine Learning and potentially Adversarial Machine Learning research.</p>\n<ul>\n<li><strong>ML Frameworks:</strong> TensorFlow or PyTorch. We&#39;ll use TensorFlow for our examples as it integrates well with Keras and has built-in functions for gradient calculation.</li>\n<li><strong>Standard Libraries:</strong> NumPy, Matplotlib (already covered).</li>\n</ul>\n<p>Make sure you have TensorFlow installed:</p>\n<pre><code class=\"language-bash\">pip install tensorflow matplotlib numpy\n</code></pre>\n<p><em>(Note: If you have a CUDA-enabled GPU, install <code>tensorflow-gpu</code> for faster training, though for our simple MNIST example, CPU is fine).</em></p>\n<h3><strong>5.11 Practical Implementation: Implementing FGSM on an Image Classifier</strong></h3>\n<p>Now for the hands-on part! We will implement the FGSM attack on a simple Convolutional Neural Network (CNN) trained on the MNIST dataset. MNIST consists of small (28x28 pixel) grayscale images of handwritten digits (0-9). This is a common &quot;hello world&quot; for image classification and adversarial attacks due to its simplicity and speed.</p>\n<p><strong>Goal:</strong> Take a correctly classified MNIST image, apply the FGSM perturbation, and show that the model misclassifies the resulting adversarial image.</p>\n<p><strong>Steps:</strong></p>\n<ol>\n<li>Load the MNIST dataset.</li>\n<li>Build and train a simple CNN classifier (or load a pre-trained one).</li>\n<li>Select a test image that the model classifies correctly.</li>\n<li>Implement the FGSM logic using TensorFlow&#39;s gradient capabilities.</li>\n<li>Generate the adversarial example.</li>\n<li>Visualize the original, perturbation, and adversarial image.</li>\n<li>Show the model&#39;s prediction on the original and adversarial images.</li>\n<li>(Optional) Evaluate the attack success rate on a small batch of test images.</li>\n</ol>\n<p>Let&#39;s walk through the code.</p>\n<pre><code class=\"language-python\">import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Ensure reproducibility\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nprint(f&quot;TensorFlow version: {tf.__version__}&quot;)\n\n# --- Step 1: Load and Prepare Data ---\n# Load MNIST dataset\nmnist = tf.keras.datasets.mnist\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n\n# Normalize pixel values to [0, 1]\ntrain_images = train_images.astype(&#39;float32&#39;) / 255.0\ntest_images = test_images.astype(&#39;float32&#39;) / 255.0\n\n# Add a channel dimension for CNN input (MNIST is grayscale, so 1 channel)\ntrain_images = train_images[..., tf.newaxis]\ntest_images = test_images[..., tf.newaxis]\n\n# Select a subset for faster testing if needed\n# test_images = test_images[:1000]\n# test_labels = test_labels[:1000]\n\n# --- Step 2: Build and Train a Simple CNN Classifier ---\n# We&#39;ll build a small, simple CNN\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(32, (3, 3), activation=&#39;relu&#39;, input_shape=(28, 28, 1)),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Conv2D(64, (3, 3), activation=&#39;relu&#39;),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation=&#39;relu&#39;),\n    tf.keras.layers.Dense(10, activation=&#39;softmax&#39;) # 10 classes for digits 0-9\n])\n\n# Compile the model\nmodel.compile(optimizer=&#39;adam&#39;,\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), # Use from_logits=False with softmax output\n              metrics=[&#39;accuracy&#39;])\n\n# Train the model (for simplicity, we&#39;ll train for a short time)\nprint(&quot;Training the model...&quot;)\nmodel.fit(train_images, train_labels, epochs=3, validation_data=(test_images, test_labels))\nprint(&quot;Model training complete.&quot;)\n\n# Evaluate the model on the test set\nloss, acc = model.evaluate(test_images, test_labels, verbose=0)\nprint(f&quot;Model accuracy on test set: {acc:.4f}&quot;)\n\n# --- Step 3: Select a Test Image ---\n# Find a test image that the model classifies correctly\n# We need the image, its true label, and the model&#39;s correct prediction\ncorrectly_classified_index = -1\nfor i in range(len(test_images)):\n    img = test_images[i:i+1] # Get image as a batch of 1\n    true_label = test_labels[i]\n    prediction = model.predict(img)\n    predicted_label = np.argmax(prediction[0])\n    if predicted_label == true_label:\n        correctly_classified_index = i\n        break\n\nif correctly_classified_index == -1:\n    print(&quot;Could not find a correctly classified image in the first batch. Check model accuracy.&quot;)\n    exit()\n\noriginal_image = test_images[correctly_classified_index:correctly_classified_index+1]\ntrue_label = test_labels[correctly_classified_index]\n\nprint(f&quot;\\nSelected image index: {correctly_classified_index}&quot;)\nprint(f&quot;True label: {true_label}&quot;)\ninitial_prediction = model.predict(original_image)\ninitial_predicted_label = np.argmax(initial_prediction[0])\nprint(f&quot;Initial model prediction: {initial_predicted_label}&quot;)\n\n\n# --- Step 4 &amp; 5: Implement FGSM and Generate Adversarial Example ---\n\n# Function to generate adversarial example using FGSM\ndef create_adversarial_example_fgsm(model, original_image, true_label, epsilon):\n    # Convert image and label to TensorFlow tensors with correct data types\n    image_tensor = tf.cast(original_image, tf.float32)\n    label_tensor = tf.cast([true_label], tf.int64) # Ensure label is a batch of 1\n\n    # Use tf.GradientTape to record operations for automatic differentiation\n    with tf.GradientTape() as tape:\n        tape.watch(image_tensor) # Tell the tape to watch the input image tensor\n        prediction = model(image_tensor) # Get the model&#39;s prediction\n\n        # Calculate the loss. We want to maximize the loss for the *true* class\n        # to push the prediction away from the true class.\n        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(label_tensor, prediction)\n\n    # Get the gradients of the loss with respect to the input image\n    gradient = tape.gradient(loss, image_tensor)\n\n    # Get the sign of the gradients\n    signed_grad = tf.sign(gradient)\n\n    # Create the adversarial example by adding the signed gradient scaled by epsilon\n    adversarial_image = image_tensor + epsilon * signed_grad\n\n    # Clip the adversarial image to ensure pixel values are still in [0, 1]\n    # This is important for image data, less so for raw RF IQ, but good practice\n    adversarial_image = tf.clip_by_value(adversarial_image, 0, 1)\n\n    return adversarial_image, signed_grad # Also return the perturbation for visualization\n\n# Choose an epsilon value. This controls the strength of the attack.\n# Start small and increase if needed. For MNIST, 0.1 to 0.3 often works.\nepsilon = 0.15 # Experiment with this value!\n\nprint(f&quot;\\nGenerating adversarial example with epsilon = {epsilon}&quot;)\nadversarial_image, perturbation = create_adversarial_example_fgsm(model, original_image, true_label, epsilon)\nprint(&quot;Adversarial example generated.&quot;)\n\n# --- Step 6 &amp; 7: Visualize and Evaluate ---\n\n# Function to display images and predictions\ndef display_images_and_predictions(original_img, adv_img, perturbation_img, model, true_label):\n    original_pred = model.predict(original_img)\n    original_pred_label = np.argmax(original_pred[0])\n    original_pred_confidence = np.max(original_pred[0])\n\n    adv_pred = model.predict(adv_img)\n    adv_pred_label = np.argmax(adv_pred[0])\n    adv_pred_confidence = np.max(adv_pred[0])\n\n    # Remove the channel dimension for plotting grayscale images\n    original_img_plot = original_img[0, ..., 0]\n    adv_img_plot = adv_img[0, ..., 0]\n    # The perturbation can be bipolar, so we might need to scale it for visualization\n    perturbation_img_plot = perturbation_img[0, ..., 0]\n\n\n    plt.figure(figsize=(12, 4))\n\n    # Original Image\n    plt.subplot(1, 3, 1)\n    plt.imshow(original_img_plot, cmap=&#39;gray&#39;)\n    plt.title(f&quot;Original\\nTrue: {true_label}, Pred: {original_pred_label}\\nConfidence: {original_pred_confidence:.2f}&quot;)\n    plt.axis(&#39;off&#39;)\n\n    # Perturbation (scaled for visibility)\n    plt.subplot(1, 3, 2)\n    # Use a diverging colormap to show positive/negative changes\n    # vmin/vmax symmetric around 0 makes 0 white/gray\n    abs_max_pert = np.max(np.abs(perturbation_img_plot))\n    plt.imshow(perturbation_img_plot, cmap=&#39;coolwarm&#39;, vmin=-abs_max_pert, vmax=abs_max_pert)\n    plt.title(f&quot;Perturbation (epsilon={epsilon:.2f})\\nL-inf norm: {np.max(np.abs(original_img - adv_img)):.4f}&quot;)\n    plt.axis(&#39;off&#39;)\n    plt.colorbar() # Add a colorbar to show the scale of perturbation\n\n    # Adversarial Image\n    plt.subplot(1, 3, 3)\n    plt.imshow(adv_img_plot, cmap=&#39;gray&#39;)\n    plt.title(f&quot;Adversarial\\nTrue: {true_label}, Pred: {adv_pred_label}\\nConfidence: {adv_pred_confidence:.2f}&quot;)\n    plt.axis(&#39;off&#39;)\n\n    plt.tight_layout()\n    plt.show()\n\n# Display the results for our single example\ndisplay_images_and_predictions(original_image, adversarial_image, perturbation, model, true_label)\n\n# --- Step 8 (Optional): Evaluate on a Batch ---\n# This is more robust than just one example\ndef evaluate_fgsm_attack(model, test_images, test_labels, epsilon):\n    successful_attacks = 0\n    total_attempts = 0\n\n    # We will only attack images that the model initially classifies correctly\n    for i in range(len(test_images)):\n        img = test_images[i:i+1]\n        true_label = test_labels[i]\n\n        initial_prediction = model.predict(img, verbose=0)\n        initial_predicted_label = np.argmax(initial_prediction[0])\n\n        if initial_predicted_label == true_label: # Only attack if initially correct\n            total\n</code></pre>\n\n                </div>\n             </div>\n         ",
    "module-6": "\n             <div class=\"card main-content-card\"> <!-- Added main-content-card class -->\n                <h1>Module 6: module_6</h1> <!-- Use module title here -->\n                <div class=\"markdown-content\">\n                    <p>Alright team, welcome back! We&#39;ve built our foundation in RF and SDRs (Modules 1 &amp; 2), we&#39;ve understood how AI plays in the spectrum space and even built our own basic classifier (Modules 3 &amp; 4). In Module 5, we peered into the fascinating, and frankly, a little unsettling, world of Adversarial Machine Learning – understanding <em>why</em> AI can be fooled and the core concepts like gradient ascent.</p>\n<p>Now, in Module 6, we take that adversarial theory and smack it right onto our RF classification problem. This is where we bridge the gap between the abstract math of AML and the concrete data of RF signals. Our goal isn&#39;t to transmit anything <em>yet</em> – that&#39;s Module 7. Here, we focus on generating the <em>digital</em> adversarial examples, the specially crafted data inputs that, when fed into our trained AI model (from M4), cause it to misclassify.</p>\n<p>This is a crucial step. We need to confirm our adversarial generation <em>works</em> in the pristine digital domain before we introduce the messiness of over-the-air transmission. Think of it as crafting the perfect blueprint before heading to the construction site.</p>\n<p>Let&#39;s dive in!</p>\n<hr>\n<h2>Module 6: Crafting RF Adversarial Examples (Digital)</h2>\n<p><strong>Module Objective:</strong> Adapt general adversarial machine learning techniques to the RF domain, focusing on generating adversarial <em>data</em> (IQ or spectrograms) that can deceive the target RF classifier <em>before</em> over-the-air transmission.</p>\n<p><strong>Contribution to Capstone:</strong> Creates the core adversarial signal <em>generation</em> component (digital stage).</p>\n<hr>\n<h3>6.1 Recapping Adversarial Attacks &amp; The Gradient Idea</h3>\n<p>Remember from Module 5, the core idea behind many white-box adversarial attacks (like FGSM and PGD) is to find a small perturbation to the input data that maximizes the model&#39;s loss for the correct class (untargeted attack) or minimizes the loss for the correct class while maximizing it for a target class (targeted attack).</p>\n<p>How do we find this perturbation? Using the model&#39;s gradients! If we know how the model&#39;s loss changes with respect to small changes in the input data, we can take a &quot;step&quot; in the direction that increases the loss the fastest. This is essentially gradient ascent on the loss function, but applied to the <em>input data</em> rather than the model weights.</p>\n<p>The formula we saw was something like:</p>\n<p><code>adversarial_example = original_example + epsilon * sign(gradient_of_loss_wrt_input)</code></p>\n<p>Where:</p>\n<ul>\n<li><code>original_example</code>: Your clean RF data (e.g., a spectrogram image, or an IQ data block).</li>\n<li><code>epsilon</code>: A small scalar value controlling the magnitude of the perturbation. This is <em>critical</em> for RF, as it relates to power constraints later.</li>\n<li><code>gradient_of_loss_wrt_input</code>: How much the loss changes as each element of the input data changes.</li>\n<li><code>sign()</code>: Takes the sign of each element in the gradient (either +1 or -1). This ensures we only care about the <em>direction</em> of the maximum increase, not the magnitude of the gradient itself (for FGSM).</li>\n</ul>\n<p>Our task now is to apply this concept when the &quot;input&quot; is RF data.</p>\n<h3>6.2 Challenges of RF Perturbations (Digital Stage)</h3>\n<p>While the mathematical concept is the same, applying AML to RF data presents unique challenges, even before we go over the air:</p>\n<ol>\n<li><p><strong>Data Representation:</strong> RF data can be raw IQ samples (complex numbers over time) or processed features like spectrograms (images). The attack needs to be crafted for the specific input format your M4 classifier uses.</p>\n<ul>\n<li><strong>Attacking Spectrograms:</strong> This is often easier as spectrograms are 2D arrays, similar to images. Many existing image AML techniques and libraries can be adapted. The perturbation is applied to the pixel values of the spectrogram.</li>\n<li><strong>Attacking Raw IQ:</strong> This is closer to the physical reality but harder. IQ data is a sequence of complex numbers. Perturbing IQ directly means modifying the amplitude and phase at specific time points. This requires models that handle sequential complex data (e.g., 1D CNNs, LSTMs) and the perturbation constraints need careful consideration (e.g., perturbing phase might be easier/less noticeable than perturbing amplitude, which directly impacts power).</li>\n<li><strong>Decision for this Module:</strong> We&#39;ll primarily focus on attacking the <strong>Spectrogram</strong> representation, as it aligns well with CNN classifiers often used for spectrograms (like the one likely built in M4) and allows us to leverage standard AML library functions more directly. We&#39;ll discuss the implications for IQ data.</li>\n</ul>\n</li>\n<li><p><strong>Physical Constraints (Mentally applied in digital):</strong> Even in the digital stage, we must be mindful of the physical world.</p>\n<ul>\n<li><strong>Power:</strong> The total power of the transmitted signal is crucial. An adversarial perturbation shouldn&#39;t require infinite power. The <code>epsilon</code> value directly relates to the magnitude of the perturbation, which in turn affects the power of the resulting adversarial signal. We need ways to constrain this.</li>\n<li><strong>Bandwidth:</strong> The perturbation shouldn&#39;t drastically change the signal&#39;s bandwidth unless that&#39;s the specific attack goal (e.g., making a narrow signal look wide). FGSM/PGD applied naively can spread the signal in the frequency domain.</li>\n<li><strong>Signal Structure:</strong> Ideally, the perturbed signal should <em>still look somewhat like</em> the original signal in ways the AI <em>doesn&#39;t</em> use, but different in ways it <em>does</em> use. This is very hard to guarantee with simple gradient attacks.</li>\n</ul>\n</li>\n<li><p><strong>Maintaining Signal Validity (Digital Stage):</strong> For some signal types, the relationship between IQ samples isn&#39;t arbitrary (e.g., phase changes in PSK, frequency jumps in FSK). A random perturbation might break the fundamental structure of the signal, making it look like generic noise rather than a crafted adversarial example of a specific class. Simple attacks like FGSM don&#39;t inherently respect these structural constraints.</p>\n</li>\n</ol>\n<h3>6.3 Applying Adversarial Generation to RF Data (Spectrograms)</h3>\n<p>Let&#39;s focus on the Spectrogram approach. Our target model from M4 takes spectrogram images as input. We want to generate a slightly modified spectrogram that fools the classifier.</p>\n<p><strong>Steps to Generate an Adversarial Spectrogram (using FGSM):</strong></p>\n<ol>\n<li><strong>Load the Target Model:</strong> Get the <code>tf.keras.Model</code> object saved from Module 4.</li>\n<li><strong>Load a Clean RF Data Sample:</strong> Take a spectrogram image of a known signal type (e.g., WiFi). We also need its true class label.</li>\n<li><strong>Prepare Data for Gradient Calculation:</strong> The ML framework needs to track operations to compute gradients. In TensorFlow, this is done with <code>tf.GradientTape</code>. The input data also needs to be a <code>tf.Tensor</code> and often requires a batch dimension (even if it&#39;s just one sample).</li>\n<li><strong>Compute the Gradient:</strong><ul>\n<li>Inside the <code>tf.GradientTape</code> context, feed the prepared data through the model to get predictions.</li>\n<li>Calculate the loss between the predictions and the true label. For an <em>untargeted</em> attack, you want to maximize the loss for the <em>true</em> class. For a <em>targeted</em> attack, you want to minimize the loss for the <em>target</em> class you want it to be misclassified as.</li>\n<li>Use <code>tape.gradient()</code> to compute the gradient of the loss with respect to the input data tensor.</li>\n</ul>\n</li>\n<li><strong>Calculate the Perturbation:</strong><ul>\n<li>Get the <code>sign()</code> of the computed gradient.</li>\n<li>Multiply the signed gradient by your chosen <code>epsilon</code> value. This is your raw perturbation.</li>\n</ul>\n</li>\n<li><strong>Create the Adversarial Example:</strong><ul>\n<li>Add the perturbation to the original clean data sample.</li>\n<li><strong>Crucially:</strong> Clip the resulting values to the valid range of your spectrogram data (e.g., if your spectrograms were normalized to [0, 1], clip the perturbed values to stay within [0, 1]). This ensures the generated data is still a valid &quot;image&quot; representation.</li>\n</ul>\n</li>\n<li><strong>Evaluate Digitally:</strong> Feed the resulting adversarial spectrogram into the <em>same</em> loaded model and see what it predicts. If it predicts a class different from the true class (for untargeted) or predicts the specific target class (for targeted), your digital attack was successful!</li>\n</ol>\n<h3>6.4 Implementing FGSM for RF Spectrograms (Code Example)</h3>\n<p>Let&#39;s write some Python code using TensorFlow to demonstrate this. We&#39;ll need:</p>\n<ul>\n<li>TensorFlow (<code>tensorflow</code>)</li>\n<li>NumPy (<code>numpy</code>)</li>\n<li>Matplotlib (<code>matplotlib</code>) for visualization (optional but helpful)</li>\n</ul>\n<p>Assume:</p>\n<ul>\n<li>Your model is saved as <code>rf_classifier_model.h5</code>.</li>\n<li>You have a dataset of spectrograms as NumPy arrays, and corresponding labels. Let&#39;s assume a single sample <code>clean_spectrogram</code> and its <code>true_label</code>.</li>\n<li>Spectrogram data is normalized between 0 and 1.</li>\n</ul>\n<pre><code class=\"language-python\">import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Assume model path and data loading\nMODEL_PATH = &#39;rf_classifier_model.h5&#39;\n# Assume you have loaded your spectrogram dataset and labels\n# clean_spectrogram: a numpy array representing one spectrogram (e.g., shape [height, width, 1] for grayscale)\n# true_label: the integer label for the clean_spectrogram\n# target_label: (only for targeted attack) the integer label you want the model to predict\n\n# --- 1. Load the Target Model ---\ntry:\n    model = tf.keras.models.load_model(MODEL_PATH)\n    print(f&quot;Successfully loaded model from {MODEL_PATH}&quot;)\n    model.summary() # Print model summary to confirm\nexcept Exception as e:\n    print(f&quot;Error loading model: {e}&quot;)\n    print(&quot;Please ensure &#39;rf_classifier_model.h5&#39; exists and is a valid Keras model.&quot;)\n    # Exit or handle error appropriately if model loading fails\n    exit() # Or raise an exception\n\n# --- Assume you have a clean sample and its label ---\n# Replace with your actual data loading logic\n# Example dummy data creation (replace with your actual data)\nspectrogram_height = 128 # Example dimensions from M4\nspectrogram_width = 128\nnum_classes = 10 # Example number of classes from M4\n\n# Create a dummy clean spectrogram (e.g., random noise for demonstration)\n# In reality, this would be loaded from your captured/processed RF data\nclean_spectrogram = np.random.rand(spectrogram_height, spectrogram_width, 1).astype(np.float32)\ntrue_label = 2 # Example: Let&#39;s say this is a &#39;WiFi&#39; signal (class 2)\n\n# For targeted attack, define the target label (e.g., make WiFi look like LTE, class 5)\n# targeted_attack = True\n# target_label = 5\n\nprint(f&quot;Clean spectrogram shape: {clean_spectrogram.shape}&quot;)\nprint(f&quot;True label: {true_label}&quot;)\n\n# --- 2. Define Attack Parameters ---\nepsilon = 0.05 # Controls the magnitude of the perturbation. Tune this!\n# A smaller epsilon means a smaller, less noticeable perturbation,\n# but potentially less likely to succeed.\n\n# --- 3. Prepare Data for Gradient Calculation ---\n# Add batch dimension: (1, height, width, channels)\nclean_spectrogram_tensor = tf.convert_to_tensor(clean_spectrogram[np.newaxis, ...])\n# Prepare label as a tensor\ntrue_label_tensor = tf.convert_to_tensor([true_label])\n# For targeted attack, use target label tensor\n# target_label_tensor = tf.convert_to_tensor([target_label])\n\n# --- 4. Compute the Gradient using GradientTape ---\nwith tf.GradientTape() as tape:\n    tape.watch(clean_spectrogram_tensor) # Tell tape to record operations on this tensor\n\n    # Get model predictions\n    predictions = model(clean_spectrogram_tensor)\n\n    # Calculate Loss\n    # Use SparseCategoricalCrossentropy if your labels are integers (like true_label)\n    # Use CategoricalCrossentropy if your labels are one-hot encoded\n    loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n\n    # --- Determine the target for the loss calculation ---\n    # For UNTARGETED attack: Maximize loss for the TRUE class\n    loss = loss_object(true_label_tensor, predictions)\n\n    # For TARGETED attack: Minimize loss for the TARGET class\n    # if targeted_attack:\n    #     loss_object_targeted = tf.keras.losses.SparseCategoricalCrossentropy()\n    #     loss = -loss_object_targeted(target_label_tensor, predictions) # Minimize loss for target class = Maximize negative loss\n\n    # --- 5. Compute Gradient of Loss WRT Input ---\n    gradient = tape.gradient(loss, clean_spectrogram_tensor)\n\n# --- 6. Calculate the Perturbation ---\n# Get the sign of the gradient\nsigned_gradient = tf.sign(gradient)\n\n# Multiply by epsilon\nperturbation = epsilon * signed_gradient\n\n# --- 7. Create the Adversarial Example ---\nadversarial_spectrogram_tensor = clean_spectrogram_tensor + perturbation\n\n# Crucially, clip the values to the valid data range (e.g., [0, 1])\nadversarial_spectrogram_tensor = tf.clip_by_value(adversarial_spectrogram_tensor, 0, 1)\n\n# Convert back to numpy array (remove batch dimension)\nadversarial_spectrogram = adversarial_spectrogram_tensor.numpy().squeeze() # squeeze removes the batch dim and channel dim if it was 1\n\nprint(f&quot;Generated adversarial spectrogram shape: {adversarial_spectrogram.shape}&quot;)\n\n# --- 8. Evaluate Digital Adversarial Example ---\nprint(&quot;\\n--- Digital Evaluation ---&quot;)\n\n# Evaluate clean spectrogram\nclean_prediction = model.predict(clean_spectrogram_tensor)\nclean_predicted_class = np.argmax(clean_prediction)\nprint(f&quot;Prediction for clean spectrogram: Class {clean_predicted_class} (Confidence: {np.max(clean_prediction):.2f})&quot;)\nif clean_predicted_class == true_label:\n    print(&quot;Clean sample classified correctly.&quot;)\nelse:\n    print(&quot;Warning: Clean sample misclassified by the model itself!&quot;)\n\n# Evaluate adversarial spectrogram\nadversarial_prediction = model.predict(adversarial_spectrogram_tensor)\nadversarial_predicted_class = np.argmax(adversarial_prediction)\nprint(f&quot;Prediction for adversarial spectrogram: Class {adversarial_predicted_class} (Confidence: {np.max(adversarial_prediction):.2f})&quot;)\n\n# Check if the attack was successful (digitally)\nattack_successful_digital = False\nif adversarial_predicted_class != true_label:\n    if &#39;targeted_attack&#39; in locals() and targeted_attack:\n        if adversarial_predicted_class == target_label:\n            print(&quot;Targeted attack SUCCESSFUL (digitally)!&quot;)\n            attack_successful_digital = True\n        else:\n            print(f&quot;Targeted attack FAILED (digitally): Misclassified as {adversarial_predicted_class} instead of target {target_label}.&quot;)\n    else: # Untargeted attack\n         print(&quot;Untargeted attack SUCCESSFUL (digitally)!&quot;)\n         attack_successful_digital = True\nelse:\n    print(&quot;Attack FAILED (digitally): Adversarial sample still classified as the true class.&quot;)\n\n# --- Optional: Visualize the Spectrograms and Perturbation ---\n# This is mainly for debugging and understanding\nif spectrogram_height &lt;= 64 and spectrogram_width &lt;= 64: # Avoid plotting massive arrays\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    axes[0].imshow(clean_spectrogram.squeeze(), aspect=&#39;auto&#39;, origin=&#39;lower&#39;, cmap=&#39;viridis&#39;)\n    axes[0].set_title(f&#39;Clean Spectrogram (True Class: {true_label})&#39;)\n    axes[1].imshow(adversarial_spectrogram.squeeze(), aspect=&#39;auto&#39;, origin=&#39;lower&#39;, cmap=&#39;viridis&#39;)\n    axes[1].set_title(f&#39;Adversarial Spectrogram (Predicted: {adversarial_predicted_class})&#39;)\n    # Plot the perturbation itself (difference between adversarial and clean)\n    perturbation_viz = (adversarial_spectrogram - clean_spectrogram).squeeze()\n    # Use a diverging colormap for the perturbation\n    max_val = np.max(np.abs(perturbation_viz))\n    axes[2].imshow(perturbation_viz, aspect=&#39;auto&#39;, origin=&#39;lower&#39;, cmap=&#39;coolwarm&#39;, vmin=-max_val, vmax=max_val)\n    axes[2].set_title(f&#39;Perturbation (Scaled by {epsilon})&#39;)\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(&quot;\\nSpectrogram too large to visualize easily. Skipping plot.&quot;)\n    print(&quot;You can inspect min/max values of perturbation:&quot;,\n          np.min(adversarial_spectrogram - clean_spectrogram),\n          np.max(adversarial_spectrogram - clean_spectrogram))\n\n# --- Saving the adversarial example ---\n# You&#39;ll likely want to save this generated adversarial spectrogram\n# for use in the Module 7 over-the-air transmission.\n# np.save(&#39;adversarial_spectrogram.npy&#39;, adversarial_spectrogram)\n# print(&quot;Adversarial spectrogram saved as &#39;adversarial_spectrogram.npy&#39;&quot;)\n</code></pre>\n<p><strong>Explanation of the Code:</strong></p>\n<ol>\n<li>We load the Keras model we trained in Module 4.</li>\n<li>We define <code>epsilon</code>, the critical parameter controlling the <em>strength</em> of the perturbation. Tuning this is key!</li>\n<li>We take a clean spectrogram sample (<code>clean_spectrogram</code>) and its true <code>true_label</code>.</li>\n<li><code>tf.GradientTape()</code> is used to record the forward pass (<code>model(clean_spectrogram_tensor)</code>) so TensorFlow knows how to calculate the gradient of the output (loss) with respect to the input (<code>clean_spectrogram_tensor</code>).</li>\n<li>We calculate the loss. For an <em>untargeted</em> attack, we calculate the standard loss between the model&#39;s prediction and the <em>true</em> label. The gradient will then point in the direction that increases this loss.</li>\n<li><code>tape.gradient(loss, clean_spectrogram_tensor)</code> computes the gradient. This is the core step.</li>\n<li><code>tf.sign(gradient)</code> gives us the direction of the gradient for each &quot;pixel&quot; in the spectrogram.</li>\n<li>We scale this signed gradient by <code>epsilon</code> to get the perturbation.</li>\n<li>We add the perturbation to the original clean spectrogram.</li>\n<li><code>tf.clip_by_value(..., 0, 1)</code> is crucial. It ensures that the resulting adversarial spectrogram&#39;s values stay within the valid range (0 to 1 in this example). This is a simple form of constraint. More complex constraints (like L2 norm limits on the <em>perturbation itself</em> or frequency domain constraints) would be implemented here or in the perturbation calculation.</li>\n<li>Finally, we evaluate the generated <code>adversarial_spectrogram</code> by feeding it back into the model and checking the prediction. This confirms the attack&#39;s success <em>in the digital domain</em>.</li>\n</ol>\n<h3>6.5 Targeted vs. Untargeted Attacks (Code Variation)</h3>\n<p>The code snippet above shows an untargeted attack (maximize loss for the true class). To make it a <em>targeted</em> attack (make the model predict a specific <code>target_label</code>), you simply change how the loss is calculated inside the <code>GradientTape</code>:</p>\n<pre><code class=\"language-python\"># ... (previous code setting up model, data, epsilon) ...\n\n# For TARGETED attack:\ntarget_label = 5 # Example: Make WiFi (class 2) look like LTE (class 5)\ntarget_label_tensor = tf.convert_to_tensor([target_label])\n\nwith tf.GradientTape() as tape:\n    tape.watch(clean_spectrogram_tensor)\n    predictions = model(clean_spectrogram_tensor)\n\n    # --- Change the loss calculation for targeted attack ---\n    # We want to minimize the loss between the prediction and the TARGET label.\n    # Minimizing loss is equivalent to maximizing negative loss.\n    loss_object_targeted = tf.keras.losses.SparseCategoricalCrossentropy()\n    loss = -loss_object_targeted(target_label_tensor, predictions) # Note the negative sign!\n\n    gradient = tape.gradient(loss, clean_spectrogram_tensor)\n\n# ... (rest of the code is the same - calculating perturbation, creating adversarial example, clipping, evaluation) ...\n</code></pre>\n<p>By minimizing the loss with respect to the <em>target</em> label, the gradient points in the direction that makes the prediction <em>closer</em> to the target class.</p>\n<h3>6.6 Constraining the Adversarial Perturbation (Beyond Clipping)</h3>\n<p>Clipping to the valid data range [0, 1] is a basic constraint. More sophisticated attacks and evaluations often constrain the <em>magnitude of the perturbation itself</em> using Lp norms.</p>\n<ul>\n<li><strong>L-infinity norm (<code>L_inf</code>):</strong> Limits the <em>maximum absolute value</em> of any single element in the perturbation. <code>||perturbation||_inf &lt;= epsilon</code>. This is what our FGSM code with clipping <em>approximates</em> if the data range is [0,1] and epsilon is small. The code <code>adversarial = clean + epsilon * sign(gradient)</code> and then clipping ensures <code>|adversarial - clean| &lt;= epsilon</code> (assuming clean is 0-1 and perturbation doesn&#39;t push it beyond 0 or 1).</li>\n<li><strong>L-2 norm (<code>L_2</code>):</strong> Limits the square root of the sum of the squares of all elements in the perturbation. <code>||perturbation||_2 &lt;= epsilon</code>. This corresponds to the &quot;energy&quot; or &quot;power&quot; of the perturbation.</li>\n</ul>\n<p>Implementing L2 constrained FGSM or PGD requires normalizing the gradient differently or projecting the perturbation back onto an L2 ball. PGD (Projected Gradient Descent) is essentially an iterative FGSM where, after each small gradient step, the result is &quot;projected&quot; back into the allowed epsilon-ball (defined by L-inf or L2 norm).</p>\n<p>For this module&#39;s project, starting with the simple FGSM and clipping is sufficient. Understanding Lp norms is key for reading AML papers and implementing more robust attacks/defenses.</p>\n<h3>6.7 Applying to Raw IQ Data (Conceptual)</h3>\n<p>If your M4 classifier worked directly on raw IQ data (e.g., a 1D CNN or LSTM taking complex samples over time), the process would be conceptually similar:</p>\n<ol>\n<li>Your input data would be a tensor of complex numbers (or two tensors, one for I and one for Q).</li>\n<li>Your model would be designed to process this sequence.</li>\n<li>You would use <code>tf.GradientTape</code> to compute the gradient of the loss with respect to the <em>IQ input tensor</em>.</li>\n<li>The perturbation would be calculated on this IQ tensor.</li>\n<li><strong>Constraints are harder here:</strong> Simple clipping [0, 1] doesn&#39;t make sense for IQ. Constraints would likely be on:<ul>\n<li>The magnitude of the perturbation vector at each time step (<code>|p_i + j*q_i| &lt;= epsilon_t</code>).</li>\n<li>The total power of the perturbed signal over a time window (<code>Sum(|clean_i + pert_i|^2) &lt;= MaxPower</code>).</li>\n<li>Potentially frequency domain constraints (e.g., limiting out-of-band emissions).</li>\n</ul>\n</li>\n<li>The output of this process would be an <em>adversarial IQ sequence</em>.</li>\n</ol>\n<p>This is more complex because the relationship between a change in a complex IQ sample and its physical manifestation (amplitude/phase) is direct, and physical constraints (like peak or average power) translate differently than simple image pixel clipping.</p>\n<h3>6.8 Module Project/Exercise</h3>\n<p>Alright, time to get your hands dirty and build the digital adversarial generator!</p>\n<p><strong>Steps:</strong></p>\n<ol>\n<li><strong>Ensure Prerequisites:</strong> Make sure your Module 4 target classifier (<code>rf_classifier_model.h5</code>) is saved and accessible. Have a small dataset of clean RF spectrograms (or whatever format your M4 model used) and their true labels ready.</li>\n<li><strong>Choose a Framework:</strong> Use the ML framework you used in M4 (TensorFlow or PyTorch). The code example above is for TensorFlow. If using PyTorch, the concepts (gradient calculation via <code>requires_grad=True</code> and backpropagation, applying perturbation, clipping) are analogous, just the syntax differs (<code>torch.autograd.backward</code>, <code>tensor.grad</code>, <code>torch.sign</code>, <code>torch.clamp</code>).</li>\n<li><strong>Implement the Attack:</strong> Write a Python script that:<ul>\n<li>Loads your trained RF classifier model from M4.</li>\n<li>Loads at least one clean sample of RF data (spectrogram recommended) and its true label.</li>\n<li>Implements the FGSM attack algorithm as shown in the TensorFlow example (or its PyTorch equivalent).</li>\n<li><strong>Experiment with <code>epsilon</code>:</strong> Start with a small <code>epsilon</code> (e.g., 0.01) and gradually increase it (e.g., 0.05, 0.1, 0.2) until the digital attack is successful. Note how the perturbation visually changes (if you plot it) and how the model&#39;s prediction confidence changes.</li>\n<li><strong>Choose Targeted or Untargeted:</strong> Decide if you want to try an untargeted attack first (simpler) or a targeted one (more challenging, requires picking a target class). Implement the corresponding loss calculation.</li>\n<li>Generate the adversarial version of your clean sample.</li>\n<li>Digitally evaluate the adversarial sample by feeding it back into the loaded model.</li>\n<li>Print the model&#39;s prediction for both the clean and adversarial samples.</li>\n<li>Verify and report if the digital attack was successful (i.e., the model misclassified the adversarial sample as intended).</li>\n</ul>\n</li>\n<li><strong>Save the Adversarial Sample:</strong> Once you have a digital adversarial example that successfully fools the model, save it to a file (e.g., using <code>numpy.save</code>). This saved data is the input you will feed into your SDR&#39;s transmission buffer in Module 7.</li>\n<li><strong>Document:</strong> Note down the <code>epsilon</code> value used, the true class, the predicted class for the clean sample, and the predicted class for the adversarial sample. Describe the attack goal (targeted/untargeted) and the data representation used (spectrogram).</li>\n</ol>\n<p><strong>Tips for the Project:</strong></p>\n<ul>\n<li>Start simple! Get the untargeted FGSM working first.</li>\n<li>Use a single, well-understood clean sample from your M4 dataset for initial testing.</li>\n<li>Debug by checking the shapes of your tensors and numpy arrays at each step.</li>\n<li>If the digital attack isn&#39;t working, double-check:<ul>\n<li>Is the model loading correctly?</li>\n<li>Is the data format correct (e.g., batch dimension, data type float32)?</li>\n<li>Is <code>tape.watch()</code> being called correctly (TensorFlow)?</li>\n<li>Is the loss function appropriate for your labels (sparse vs. categorical)?</li>\n<li>Is <code>epsilon</code> large enough? (But don&#39;t make it <em>too</em> large; the goal is a <em>small</em> perturbation).</li>\n<li>Are you clipping correctly?</li>\n</ul>\n</li>\n<li>If using a targeted attack, ensure the <code>target_label</code> is different from the <code>true_label</code>.</li>\n</ul>\n<hr>\n<h3>6.9 Summary</h3>\n<p>In this module, we&#39;ve taken the adversarial concepts from Module 5</p>\n\n                </div>\n             </div>\n         ",
    "module-7": "\n             <div class=\"card main-content-card\"> <!-- Added main-content-card class -->\n                <h1>Module 7: module_7</h1> <!-- Use module title here -->\n                <div class=\"markdown-content\">\n                    <p>Okay, RF, Offensive Security, AI, and coding enthusiasts! Welcome to Module 7. We&#39;ve journeyed from the fundamentals of RF and SDRs, through building an AI classifier, to crafting digital adversarial examples. Now, we stand at the exciting, challenging, and ethically sensitive threshold of taking our digital creations and pushing them into the physical world – over the airwaves.</p>\n<p>This module is where the rubber meets the road (or where the electrons meet the antenna). We&#39;ll confront the harsh realities of the physical layer and learn how to use our SDRs not just for listening, but for actively transmitting our carefully crafted adversarial signals.</p>\n<p>Let&#39;s dive deep.</p>\n<hr>\n<h2>Module 7: Over-the-Air Attacks: Transmitting Adversarial RF</h2>\n<p><strong>Module Objective:</strong> Bridge the gap between digital adversarial examples and physical RF transmission, addressing the challenges of over-the-air effects and executing adversarial attacks using SDRs in a controlled environment.</p>\n<p><strong>Recap from Module 6:</strong> In Module 6, we learned the theory of Adversarial Machine Learning and, crucially, developed the code to generate <em>digital</em> adversarial examples. We took clean IQ data or spectrograms and calculated small perturbations that, when added, fooled our target AI classifier <em>in simulation</em>. We verified their effectiveness by feeding them directly into the loaded model.</p>\n<p><strong>The Challenge Ahead:</strong> The digital world is perfect. The physical RF world is messy. Our carefully calculated digital perturbation, effective in isolation, must now survive conversion to analog, transmission through a noisy, unpredictable channel, and reception by the target system, all while ideally remaining &quot;small&quot; or inconspicuous in the physical domain. This is the core challenge of Over-the-Air (OTA) Adversarial ML.</p>\n<h3>7.1 The Chasm: Why Over-the-Air AML is Hard</h3>\n<p>Taking a digital adversarial example and transmitting it isn&#39;t as simple as just loading the IQ data into the SDR&#39;s transmit buffer. Several factors conspire against the attacker:</p>\n<ul>\n<li><strong>Channel Effects:</strong><ul>\n<li><strong>Fading:</strong> Signal strength varies significantly over time and space due to multipath propagation. A perturbation calculated for a strong signal might be ineffective if the signal fades.</li>\n<li><strong>Multipath:</strong> Signals bounce off objects, arriving at the receiver via multiple paths with different delays and phases. This causes constructive and destructive interference, distorting the signal waveform and potentially wiping out or altering the delicate adversarial perturbation.</li>\n<li><strong>Doppler Shift:</strong> If the transmitter or receiver is moving, the frequency of the signal appears to shift. This can disrupt synchronization and signal structure.</li>\n</ul>\n</li>\n<li><strong>Noise and Interference:</strong> The real world is full of thermal noise and signals from other devices. The adversarial perturbation is often designed to be low-power. Can it survive being buried in the noise floor or masked by other signals?</li>\n<li><strong>Hardware Non-linearities:</strong> SDR transmitters, especially amplifiers, are not perfectly linear. They can introduce distortion (e.g., spectral regrowth, intermodulation distortion) that changes the transmitted waveform from the ideal digital IQ data. This distortion can alter or destroy the adversarial perturbation.</li>\n<li><strong>Synchronization:</strong> For many signal types, correct demodulation and classification require precise timing and frequency synchronization. The attacker&#39;s signal might not be perfectly synchronized with the target receiver or the signal it&#39;s trying to impersonate/attack.</li>\n<li><strong>Power Constraints:</strong> Adversarial perturbations are often designed to be small relative to the clean signal power. This is an attacker&#39;s constraint for stealth or plausibility. Can the perturbation still be effective at realistic power levels after channel loss and noise? The attacker&#39;s <em>total</em> transmission power is also often limited by regulations or operational constraints.</li>\n<li><strong>Bandwidth and Filtering:</strong> The adversarial perturbation might introduce frequency components outside the intended signal&#39;s bandwidth. These can be filtered out by the transmitter&#39;s hardware, the channel, or the receiver&#39;s front end, again destroying the attack.</li>\n</ul>\n<p><strong>Key Takeaway:</strong> Over-the-Air AML is an active area of research because the physical layer acts as a powerful, unpredictable defense mechanism against digitally crafted attacks. Success requires careful consideration of these real-world effects.</p>\n<h3>7.2 Translating Digital Adversarial Examples to Analog Waveforms (SDR TX)</h3>\n<p>Our adversarial examples from Module 6 are typically in the form of modified IQ data arrays (NumPy arrays of complex numbers) or derived features like spectrograms. To transmit them, we need to convert the IQ data into a continuous analog waveform. This is precisely what the Digital-to-Analog Converter (DAC) and associated RF front-end hardware in a TX-capable SDR do.</p>\n<p>The process generally involves:</p>\n<ol>\n<li><strong>Loading the IQ data:</strong> The digital IQ samples are loaded into the SDR&#39;s transmit buffer.</li>\n<li><strong>DAC Conversion:</strong> The DAC converts the discrete digital samples into a continuous analog baseband signal (I and Q components).</li>\n<li><strong>Upconversion:</strong> This analog baseband signal is mixed with a local oscillator (LO) frequency to shift it up to the desired carrier frequency.</li>\n<li><strong>Amplification and Filtering:</strong> The signal is amplified to the desired transmission power and filtered to remove unwanted spectral components (like the LO image).</li>\n<li><strong>Transmission:</strong> The amplified and filtered signal is sent to the antenna and radiated.</li>\n</ol>\n<p><strong>Crucial Parameters for Transmission:</strong></p>\n<ul>\n<li><strong>Center Frequency:</strong> The carrier frequency at which the signal will be transmitted. Must match the frequency the target system is listening on.</li>\n<li><strong>Sample Rate:</strong> The rate at which the DAC processes the IQ samples. This determines the <em>bandwidth</em> of the transmitted signal (Nyquist theorem: max bandwidth is half the sample rate). The sample rate of the transmitted data <em>must</em> match the sample rate configured on the SDR.</li>\n<li><strong>Gain (TX Power):</strong> Controls the amplification stage. Needs careful setting based on the testbed setup and legal limits.</li>\n<li><strong>Antenna:</strong> The choice and placement of antenna significantly impact the radiated signal and its interaction with the environment.</li>\n</ul>\n<p><strong>Using SDR Libraries for Transmission:</strong></p>\n<p>Just as we used libraries like <code>pyrtlsdr</code> or <code>pysdr</code> for reception, libraries like <code>pyhackrf</code> (for HackRF), <code>pylimesdr</code> (for LimeSDR), <code>pyuhd</code> (for USRP), or <code>pyadi-iio</code> (for ADALM-PLUTO) provide Python interfaces to control the SDR&#39;s transmission capabilities.</p>\n<p>The core function is usually something like <code>tx_stream.send()</code> or <code>sdr.tx()</code>, which takes an array of complex IQ samples and sends them out the antenna.</p>\n<p><strong>Example: Conceptual SDR Transmission Script (using a hypothetical <code>pysdr_tx</code> interface):</strong></p>\n<pre><code class=\"language-python\">import numpy as np\nimport time\n# Assume pysdr_tx is installed and configured for your TX SDR\n# (In reality, you&#39;d use pyhackrf, pylimesdr, pyuhd, pyadi-iio, etc.)\n# from pysdr_tx import SDRTxDevice\n\n# --- Configuration ---\n# Replace with your actual SDR device initialization\n# sdr = SDRTxDevice(...)\n# For demonstration, let&#39;s mock the SDR object\nclass MockSDRTx:\n    def __init__(self, center_freq, sample_rate, tx_gain):\n        self.center_freq = center_freq\n        self.sample_rate = sample_rate\n        self.tx_gain = tx_gain\n        print(f&quot;MockSDRTx configured: Freq={center_freq/1e6} MHz, SR={sample_rate/1e6} MSps, Gain={tx_gain} dB&quot;)\n\n    def tx(self, iq_samples):\n        print(f&quot;MockSDRTx: Transmitting {len(iq_samples)} IQ samples...&quot;)\n        # In a real SDR library, this would send data to the hardware\n        # For mock, simulate transmission time\n        time.sleep(len(iq_samples) / self.sample_rate)\n        print(&quot;MockSDRTx: Transmission finished.&quot;)\n\n# --- SDR Setup ---\nTX_CENTER_FREQ = 433.92e6  # Example: ISM band frequency\nTX_SAMPLE_RATE = 1e6       # Example: 1 MSps\nTX_GAIN = 0                # Example: dB gain (adjust carefully!)\n\n# Initialize the SDR (replace with real code for your device)\nsdr = MockSDRTx(TX_CENTER_FREQ, TX_SAMPLE_RATE, TX_GAIN)\n\n# --- Prepare Data for Transmission ---\n# Load your adversarial IQ data generated in Module 6\n# This would be a numpy array of complex numbers\n# For this example, let&#39;s create a simple sine wave as placeholder data\nduration = 0.01 # seconds\nt = np.arange(0, duration, 1/TX_SAMPLE_RATE)\n# Create a tone at 100 kHz offset from center frequency\ntone_freq = 100e3\niq_data_to_transmit = 0.5 * np.exp(2j * np.pi * tone_freq * t)\nprint(f&quot;Prepared {len(iq_data_to_transmit)} IQ samples for transmission.&quot;)\n\n# --- Transmit ---\nprint(&quot;Starting transmission...&quot;)\ntry:\n    # In a real scenario, you might transmit in chunks or continuously\n    # For a simple example, transmit the whole array\n    sdr.tx(iq_data_to_transmit)\n    print(&quot;Transmission successful (mock).&quot;)\n\nexcept Exception as e:\n    print(f&quot;Error during transmission: {e}&quot;)\nfinally:\n    # In a real script, you&#39;d have sdr.close() or similar\n    print(&quot;Transmission script finished.&quot;)\n\n# --- Important Considerations for Real Transmission ---\n# 1. Power/Gain: START WITH VERY LOW GAIN and use attenuators.\n# 2. Frequency/Bandwidth: Ensure you are transmitting within legal/allowed bands.\n# 3. Data Type: SDR libraries often expect specific data types (e.g., complex64).\n# 4. Buffer Management: For continuous transmission, manage transmit buffers efficiently.\n# 5. Synchronization: If attacking a specific signal instance, timing is critical.\n</code></pre>\n<p>This example shows the basic structure: configure SDR, prepare IQ data, call the transmit function. The real complexity lies in preparing the <em>correct</em> adversarial IQ data and managing the transmission parameters safely and effectively.</p>\n<h3>7.3 Setting up the Controlled Over-the-Air Testbed</h3>\n<p>Executing adversarial RF attacks requires a controlled environment. You <em>must not</em> transmit adversarial signals that could interfere with legitimate communications. This is illegal and unethical. A controlled testbed allows repeatable experiments while minimizing risk.</p>\n<p><strong>Goal:</strong> Create a setup where you can transmit your adversarial signal and have your target AI classifier (running on the RX SDR from M4) receive it, isolated from external interference and without causing external interference.</p>\n<p><strong>Components:</strong></p>\n<ol>\n<li><strong>TX SDR:</strong> Your transmit-capable SDR.</li>\n<li><strong>RX SDR:</strong> Your receive-capable SDR connected to the system running your Module 4 AI classifier.</li>\n<li><strong>Antennas:</strong> Simple antennas for TX and RX.</li>\n<li><strong>Attenuators:</strong> <strong>CRITICAL.</strong> These reduce the signal power significantly. Needed to keep signals low power and, importantly, to control the <em>relative</em> strength of your adversarial signal at the receiver.</li>\n<li><strong>Cables:</strong> Connect SDRs to attenuators and antennas.</li>\n<li><strong>Optional but Recommended:</strong><ul>\n<li><strong>Shielded Box/Faraday Cage:</strong> A metal enclosure to isolate your testbed from the outside world. Even a simple metal box can help.</li>\n<li><strong>Directional Antennas:</strong> Can help focus signal energy between TX and RX, reducing spill-over.</li>\n<li><strong>Signal Generator:</strong> Useful for providing known, clean signals to test both your TX and RX/Classifier chain independently.</li>\n</ul>\n</li>\n</ol>\n<p><strong>Testbed Configurations (Increasing Control/Complexity):</strong></p>\n<ul>\n<li><p><strong>Direct Cable Connection (with Attenuation):</strong></p>\n<ul>\n<li>TX SDR -&gt; Cable -&gt; Attenuator -&gt; Cable -&gt; RX SDR</li>\n<li><em>Pros:</em> Complete control over the signal path. No channel effects, noise, or external interference. Repeatable. Easiest for initial testing of the digital-to-analog-to-digital chain.</li>\n<li><em>Cons:</em> Doesn&#39;t simulate real over-the-air effects (fading, multipath).</li>\n<li><em>Use Case:</em> Verify your adversarial IQ data is correctly transmitted and received <em>digitally</em> by the RX SDR <em>before</em> hitting the classifier, and that the classifier behaves as expected in a perfect scenario. Attenuation is still needed to protect SDR inputs/outputs.</li>\n</ul>\n</li>\n<li><p><strong>Attenuated Over-the-Air (Close Proximity with Attenuators):</strong></p>\n<ul>\n<li>TX SDR -&gt; Cable -&gt; Attenuator -&gt; Antenna</li>\n<li>RX SDR -&gt; Cable -&gt; Attenuator -&gt; Antenna</li>\n<li>Antennas placed very close (e.g., &lt; 1 meter), possibly pointed at each other.</li>\n<li><em>Pros:</em> Introduces some minimal over-the-air effects (line-of-sight path). Still relatively controlled.</li>\n<li><em>Cons:</em> Minimal simulation of real-world channel effects. Still requires significant attenuation. Risk of interference if not careful with power and frequency.</li>\n<li><em>Use Case:</em> Test the basic OTA transmission and reception chain with minimal channel impact.</li>\n</ul>\n</li>\n<li><p><strong>Over-the-Air in a Shielded Environment:</strong></p>\n<ul>\n<li>TX SDR -&gt; Cable -&gt; Attenuator -&gt; Antenna (inside box)</li>\n<li>RX SDR -&gt; Cable -&gt; Attenuator -&gt; Antenna (inside box)</li>\n<li>All inside a metal enclosure (Faraday cage).</li>\n<li><em>Pros:</em> Best simulation of OTA effects <em>without</em> causing external interference. Can introduce objects inside the box to simulate multipath.</li>\n<li><em>Cons:</em> Requires building or acquiring a shielded box. More complex setup.</li>\n<li><em>Use Case:</em> Realistic OTA testing in a safe, repeatable environment.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Safety and Legality Note:</strong></p>\n<ul>\n<li><strong>ALWAYS</strong> use appropriate attenuation, especially in direct cable connections or close proximity. SDRs have power limits on their ports.</li>\n<li><strong>NEVER</strong> transmit at high power or on frequencies you are not licensed for, especially outside a shielded environment. Know the legal regulations in your region (e.g., ISM bands, amateur radio licenses).</li>\n<li><strong>START WITH MINIMUM POWER/GAIN</strong> and gradually increase <em>only</em> as needed within your controlled setup.</li>\n<li>Consider using very low power test signals or only transmitting for very short durations.</li>\n</ul>\n<p><strong>Example Testbed Setup (Conceptual Diagram):</strong></p>\n<pre><code>+---------------------+       +------------+       +------------+       +---------------------+\n| TX SDR (Python TX  | ----&gt; | Cable      | ----&gt; | Attenuator | ----&gt; | TX Antenna (inside  |\n| Script w/ M6 data)  |       |            |       |            |       | test environment)   |\n+---------------------+       +------------+       +------------+       +---------------------+\n                                                                                   |\n                                                                                   |  Over-the-Air\n                                                                                   |  (Controlled, Attenuated Path)\n                                                                                   |\n+---------------------+       +------------+       +------------+       +---------------------+\n| RX SDR (Python RX  | &lt;---- | Cable      | &lt;---- | Attenuator | &lt;---- | RX Antenna (inside  |\n| Script feeding M4   |       |            |       |            |       | test environment)   |\n| Classifier)         |       +------------+       +------------+       +---------------------+\n+---------------------+\n</code></pre>\n<h3>7.4 Executing the Over-the-Air Attack</h3>\n<p>Now we combine our components: the target AI classifier system (M4), the adversarial generation code (M6), the SDR transmission capability (M7.2), and the controlled testbed (M7.3).</p>\n<p>The attack flow we will implement for the capstone project (and test in this module) is generally:</p>\n<ol>\n<li><strong>Set up the Testbed:</strong> Connect TX SDR, RX SDR, antennas, attenuators, and place them in the controlled environment.</li>\n<li><strong>Configure SDRs:</strong> Set center frequency, sample rate, and initial low gain for both TX and RX SDRs.</li>\n<li><strong>Start Target Classifier System:</strong> Launch the Python script from M4 that uses the RX SDR to capture data and feed it to the trained AI model for classification. Ensure it&#39;s running and ready to process incoming RF data.</li>\n<li><strong>Prepare Clean Signal:</strong> Have the IQ data for a <em>clean</em> example of the target signal class ready.</li>\n<li><strong>Transmit Clean Signal (Verification Step):</strong><ul>\n<li>Load the clean IQ data into the TX SDR transmit buffer.</li>\n<li>Transmit the clean signal.</li>\n<li>Observe the output of the target classifier system. It <em>should</em> correctly classify the signal. This verifies the basic OTA chain and the classifier&#39;s ability to recognize the clean signal over the air in your setup. Adjust TX/RX gain until this works reliably at the lowest possible power.</li>\n</ul>\n</li>\n<li><strong>Generate Adversarial Signal (Digital):</strong><ul>\n<li>Use the code from Module 6 to take the <em>clean</em> IQ data (or a representation like its spectrogram) and generate the <em>adversarial</em> perturbation.</li>\n<li>Add this perturbation to the clean IQ data to create the <em>adversarial IQ data</em> for transmission. Ensure the resulting IQ data respects any physical constraints you chose (e.g., power limits).</li>\n</ul>\n</li>\n<li><strong>Transmit Adversarial Signal:</strong><ul>\n<li>Load the adversarial IQ data into the TX SDR transmit buffer.</li>\n<li>Transmit the adversarial signal.</li>\n<li><em>Note:</em> If attacking a continuous stream or a specific instance of a signal, timing is critical. You might need to transmit the adversarial signal concurrently with a legitimate signal source (if allowed and controlled) or transmit a modified version of the legitimate signal itself. For this course&#39;s capstone, modifying and transmitting a known signal sample is the most direct approach.</li>\n</ul>\n</li>\n<li><strong>Observe and Record Classifier Output:</strong> Watch the target classifier system&#39;s output while it receives the adversarial signal.</li>\n<li><strong>Evaluate Success:</strong> Did the classifier misclassify the signal? Did it classify it as &quot;unknown&quot;? Compare this to the clean signal classification.</li>\n</ol>\n<p><strong>Conceptual Code Flow (Integrating M4, M6, M7):</strong></p>\n<p>This is the core logic you&#39;ll build towards the capstone, but you&#39;ll implement the TX part here in M7.</p>\n<pre><code class=\"language-python\"># --- Imports ---\nimport numpy as np\nimport time\n# Assume your SDR libraries are imported (e.g., pyhackrf, pyadi_iio)\n# Assume your M4 classifier code is importable\n# from my_classifier_system import RFClassifierSystem, iq_to_spectrogram\n\n# --- Configuration ---\nTX_CENTER_FREQ = 433.92e6\nTX_SAMPLE_RATE = 1e6\nTX_GAIN = -10 # Start low!\nRX_CENTER_FREQ = TX_CENTER_FREQ # RX and TX on the same frequency\nRX_SAMPLE_RATE = TX_SAMPLE_RATE\nRX_GAIN = 20 # Adjust based on signal strength after attenuation\n\n# --- Initialize SDRs ---\n# tx_sdr = init_tx_sdr(TX_CENTER_FREQ, TX_SAMPLE_RATE, TX_GAIN)\n# rx_sdr = init_rx_sdr(RX_CENTER_FREQ, RX_SAMPLE_RATE, RX_GAIN)\n\n# Mock SDRs for concept demonstration\nclass MockSDR:\n    def __init__(self, freq, sr, gain, mode):\n        self.freq = freq\n        self.sr = sr\n        self.gain = gain\n        self.mode = mode\n        print(f&quot;Mock SDR ({mode}) configured: Freq={freq/1e6} MHz, SR={sr/1e6} MSps, Gain={gain} dB&quot;)\n\n    def tx(self, iq_samples):\n        print(f&quot;Mock SDR (TX): Transmitting {len(iq_samples)} samples...&quot;)\n        # Simulate sending data - in reality, this pushes to hardware buffer\n        time.sleep(len(iq_samples) / self.sr)\n        print(&quot;Mock SDR (TX): Transmission complete.&quot;)\n        # In a real system, handle buffer management for continuous TX\n\n    def rx(self, num_samples):\n        print(f&quot;Mock SDR (RX): Capturing {num_samples} samples...&quot;)\n        # Simulate capturing data - in reality, this reads from hardware buffer\n        # For simulation, let&#39;s return some random noise\n        captured_iq = np.random.randn(num_samples) + 1j * np.random.randn(num_samples)\n        time.sleep(num_samples / self.sr)\n        print(&quot;Mock SDR (RX): Capture complete.&quot;)\n        return captured_iq\n\n# Initialize mock SDRs\ntx_sdr = MockSDR(TX_CENTER_FREQ, TX_SAMPLE_RATE, TX_GAIN, mode=&#39;TX&#39;)\nrx_sdr = MockSDR(RX_CENTER_FREQ, RX_SAMPLE_RATE, RX_GAIN, mode=&#39;RX&#39;)\n\n\n# --- Load Trained Classifier Model (from M4) ---\n# classifier_model = load_trained_model(&quot;path/to/your/model.h5&quot;)\n# classifier_system = RFClassifierSystem(classifier_model) # Assuming a class wrapper\n\n# Mock classifier system\nclass MockRFClassifierSystem:\n    def __init__(self, model=None):\n        self.model = model # Mock model\n        self.classes = [&quot;ClassA&quot;, &quot;ClassB&quot;, &quot;Noise&quot;] # Example classes\n        print(&quot;Mock RF Classifier System initialized.&quot;)\n\n    def classify(self, iq_data):\n        # In M4, this would process IQ to features (e.g., spectrogram) and run model.predict()\n        print(f&quot;Mock RF Classifier: Processing {len(iq_data)} IQ samples...&quot;)\n        # Simulate classification based on some simple rule or randomness\n        if np.sum(np.abs(iq_data)) &gt; 100: # Simple mock rule\n             predicted_class_idx = np.random.randint(0, len(self.classes)) # Random prediction for demo\n             prediction = self.classes[predicted_class_idx]\n             print(f&quot;Mock RF Classifier: Predicted class: {prediction}&quot;)\n             return prediction\n        else:\n             print(&quot;Mock RF Classifier: Predicted class: Noise&quot;)\n             return &quot;Noise&quot;\n\nclassifier_system = MockRFClassifierSystem()\n\n\n# --- Prepare Clean Signal Data ---\n# Load or generate your clean signal IQ data (e.g., a recorded WiFi burst)\n# clean_iq_data = load_iq_data(&quot;path/to/clean_wifi.iq&quot;)\n# For mock, generate a simple signal\nduration_clean = 0.005 # seconds\nt_clean = np.arange(0, duration_clean, 1/TX_SAMPLE_RATE)\nclean_iq_data = 0.8 * np.exp(2j * np.pi * 50e3 * t_clean) # Mock clean signal\n\n\n# --- STEP 5: Transmit Clean Signal (Verification) ---\nprint(&quot;\\n--- Transmitting CLEAN Signal ---&quot;)\ntx_sdr.tx(clean_iq_data)\n\n# In a real system, the RX SDR would capture and the classifier would run continuously\n# For this example, let&#39;s simulate capturing the transmitted signal\n# In the actual M4 script, the RX loop would be running in parallel\ncaptured_clean_iq = rx_sdr.rx(len(clean_iq_data)) # Simulate capturing what was TX&#39;d\n\n# Feed captured data to classifier\nprint(&quot;Classifying captured CLEAN signal...&quot;)\nclassifier_system.classify(captured_clean_iq) # Expected output: Correct class (e.g., &quot;ClassA&quot;)\n\n\n# --- STEP 6: Generate Adversarial Signal (using M6 logic) ---\nprint(&quot;\\n--- Generating ADVERSARIAL Signal (Digital) ---&quot;)\n# This is where you integrate your M6 adversarial generation code\n# Assume a function generate_adversarial_iq(clean_iq, target_class_idx, model)\n# The output is the IQ data that, when transmitted, should fool the classifier\n\n# Mock adversarial generation: Just add some structured noise\nperturbation = 0.1 * np.random.randn(len(clean_iq_data)) + 0.1j * np.random.randn(len(clean_iq_data))\n# A real M6 function would calculate this perturbation based on gradients\nadversarial_iq_data = clean_iq_data + perturbation\n\nprint(f&quot;Generated adversarial IQ data (shape: {adversarial_iq_data.shape})&quot;)\n\n# Optional: Digitally verify the adversarial example before transmission\n# This is what you did in M6\n# print(&quot;Digitally classifying adversarial data...&quot;)\n# classifier_system.classify(adversarial_iq_data) # Expected: Misclassification/Evasion digitally\n\n\n# --- STEP 7: Transmit Adversarial Signal ---\nprint(&quot;\\n--- Transmitting ADVERSARIAL Signal ---&quot;)\ntx_sdr.tx(adversarial_iq_data)\n\n# Simulate capturing the transmitted adversarial signal\ncaptured_adversarial_iq = rx_sdr.rx(len(adversarial_iq_data))\n\n# --- STEP 8 &amp; 9: Observe and Evaluate ---\nprint(&quot;Classifying captured ADVERSARIAL signal...&quot;)\nclassifier_system.classify(captured_adversarial_iq) # Expected output: Misclassification/Evasion OTA\n\nprint(&quot;\\n--- Over-the-Air Attack Execution Complete ---&quot;)\n\n# --- Cleanup\n</code></pre>\n\n                </div>\n             </div>\n         ",
    "module-8": "\n             <div class=\"card main-content-card\"> <!-- Added main-content-card class -->\n                <h1>Module 8: module_8</h1> <!-- Use module title here -->\n                <div class=\"markdown-content\">\n                    <p>Okay, buckle up! We&#39;ve journeyed from the fundamental waves of RF to the intricate world of AI classification and the theory of adversarial attacks. Module 8 is where it all converges. This isn&#39;t just another lesson; it&#39;s the culmination – the building of your functional adversarial RF lab and the demonstration of your newfound skills.</p>\n<p>We will meticulously integrate the components you&#39;ve built in Modules 4, 6, and 7, address the real-world challenges of over-the-air attacks, explore how to defend against them, and discuss the vital ethical implications of this powerful knowledge.</p>\n<hr>\n<h2><strong>Module 8: Capstone Project: Building the Adversarial RF Lab &amp; Future Directions</strong></h2>\n<p><strong>Module Objective:</strong> Integrate all previously built components into a cohesive system that demonstrates a functional adversarial RF attack against an AI classifier, and explore potential defenses and future research directions.</p>\n<p><strong>Learning Outcomes:</strong> By the end of this module, you will be able to:</p>\n<ol>\n<li>Design and implement an integrated workflow for an end-to-end adversarial RF attack using SDRs and an AI classifier.</li>\n<li>Set up and manage a controlled physical testbed for executing over-the-air adversarial RF experiments.</li>\n<li>Execute the full adversarial RF attack pipeline, from signal capture/generation to transmission and classification result observation.</li>\n<li>Evaluate the success of an over-the-air adversarial attack and document the methodology and results.</li>\n<li>Discuss and conceptually outline potential defense strategies against adversarial RF attacks.</li>\n<li>Identify key limitations of the implemented system and potential directions for future research in adversarial RF.</li>\n<li>Articulate the ethical responsibilities associated with researching and demonstrating adversarial RF techniques.</li>\n<li>Present your functional adversarial RF system and explain its components and operation.</li>\n</ol>\n<p><strong>Prerequisites:</strong> Successful completion of Modules 1 through 7, including having the functional code components from Module 4 (Target Classifier System), Module 6 (Digital Adversarial Generator), and Module 7 (SDR Transmission Setup/Code). Access to your chosen SDRs (at least one TX-capable, ideally two total for TX/RX).</p>\n<hr>\n<h3><strong>Section 8.1: Reviewing the Building Blocks (Recap)</strong></h3>\n<p>Before we integrate, let&#39;s quickly list the essential pieces we&#39;ve developed in previous modules:</p>\n<ul>\n<li><strong>From Module 4:</strong><ul>\n<li>A functional RF signal capture and processing pipeline using an SDR (the RX side of the target).</li>\n<li>A trained AI model (e.g., a CNN) for classifying RF signals (the brain of the target).</li>\n<li>Code to load the trained model and perform inference on processed SDR data.</li>\n<li>The <em>target</em> system – essentially, Python code that listens with an SDR, processes the data, and reports the classification according to your loaded AI model.</li>\n</ul>\n</li>\n<li><strong>From Module 6:</strong><ul>\n<li>Code to implement a digital adversarial attack algorithm (e.g., FGSM, PGD) adapted for your RF data representation (IQ or Spectrograms).</li>\n<li>The ability to take a <em>clean</em> digital representation of a signal, feed it into your <em>loaded target model</em>, calculate the required perturbation to cause misclassification, and generate the <em>adversarial digital signal</em> (clean signal + perturbation).</li>\n</ul>\n</li>\n<li><strong>From Module 7:</strong><ul>\n<li>Code to configure your TX-capable SDR for transmission.</li>\n<li>Code to load digital IQ data (representing the signal waveform) into the SDR&#39;s transmission buffer and transmit it over the air.</li>\n<li>Understanding of power control, sample rates, and frequency settings for transmission.</li>\n<li>Experience setting up a basic controlled test environment.</li>\n</ul>\n</li>\n</ul>\n<p>Our goal in Module 8 is to wire these pieces together into a single, executable workflow that demonstrates the attack end-to-end.</p>\n<h3><strong>Section 8.2: Designing the Integrated System Architecture</strong></h3>\n<p>How do these components interact in a full attack scenario? Let&#39;s visualize the data flow for our capstone system:</p>\n<pre><code>+-------------------+      +---------------------+      +-----------------------+\n|  Clean Signal     |-----&gt;|  Digital Data Rep |-----&gt;| Adversarial Generator |\n| (Source: File,    |      |  (IQ or Spectrogram)|      |    (M6 Code)          |\n|  Captured, Synth) |      |                     |      |                       |\n+-------------------+      +---------------------+      +-----------+-----------+\n                                   |                          |\n                                   |                          | (Needs Target Model)\n                           +-------+-------+                  |\n                           | Trained AI    |&lt;-----------------+\n                           | Classifier    |\n                           | (M4 Model)    |\n                           +---------------+\n                                   |\n                                   | (Perturbation)\n                                   |\n+-----------------------+      +--+------------------+\n|  SDR Transmission     |&lt;-----|  Adversarial Signal |\n|     (M7 Code)         |      |    (Digital)        |\n| (TX-Capable SDR)      |      +---------------------+\n+-----------------------+\n         |\n         | (Over-the-Air RF)\n         V\n+-----------------------+      +---------------------+      +-----------------------+\n|  SDR Reception        |-----&gt;|  Captured RF Data   |-----&gt;| Data Processing       |\n|     (M4 Code)         |      |    (IQ)             |      | (Spectrogram, etc.)   |\n| (RX-Capable SDR)      |      +---------------------+      +-----------+-----------+\n                                                                          |\n                                                                          |\n                                                                +-------+-------+\n                                                                | Trained AI    |\n                                                                | Classifier    |\n                                                                | (M4 Model)    |\n                                                                +---------------+\n                                                                          |\n                                                                          V\n                                                              +-----------------------+\n                                                              | Classification Result |\n                                                              | (Observe &amp; Evaluate)  |\n                                                              +-----------------------+\n</code></pre>\n<p><strong>Key Workflow Steps:</strong></p>\n<ol>\n<li><strong>Obtain a Clean Signal Sample:</strong> Get a digital representation (IQ, Spectrogram) of a signal you want to attack. This could be loaded from a file, captured live, or synthetically generated.</li>\n<li><strong>Generate Adversarial Perturbation:</strong> Use the digital adversarial generation code (M6) and the <em>target classifier model</em> (M4) to calculate a small perturbation for the clean signal sample.</li>\n<li><strong>Create Adversarial Signal:</strong> Add the calculated perturbation to the clean signal sample <em>in the digital domain</em>.</li>\n<li><strong>Prepare for Transmission:</strong> Convert the adversarial digital signal representation into a format suitable for SDR transmission (usually IQ data).</li>\n<li><strong>Transmit Adversarial Signal:</strong> Use the SDR transmission code (M7) to send the adversarial signal over the air using the TX SDR.</li>\n<li><strong>Receive and Classify:</strong> Use the target system&#39;s SDR reception and classification code (M4) to capture the transmitted signal and feed it to the AI model.</li>\n<li><strong>Observe Results:</strong> Check the output of the target classifier. Did it misclassify the signal? Did it fail to classify it (evasion)?</li>\n</ol>\n<h3><strong>Section 8.3: Implementing the Integrated Attack Workflow</strong></h3>\n<p>Now, let&#39;s translate the architecture into code. We&#39;ll likely use a main Python script (<code>capstone_attack.py</code>) to orchestrate this. We&#39;ll assume you have helper functions or classes built in previous modules, and this script will call them.</p>\n<p><strong>Assumed Helper Functions/Modules:</strong></p>\n<ul>\n<li><code>sdr_utils.py</code>: Contains functions for SDR capture (<code>capture_samples(freq, bw, duration, sample_rate)</code>) and transmission (<code>transmit_samples(iq_data, freq, sample_rate, power)</code>).</li>\n<li><code>data_processing.py</code>: Contains functions to convert raw IQ data to your chosen ML input format (e.g., <code>iq_to_spectrogram(iq_data, sample_rate)</code>).</li>\n<li><code>ml_utils.py</code>: Contains functions to load your trained model (<code>load_classifier_model(model_path)</code>) and perform inference (<code>predict(model, processed_data)</code>).</li>\n<li><code>adversarial_utils.py</code>: Contains functions to generate adversarial examples (<code>generate_adversarial_example(model, clean_data, target_label=None, epsilon=0.01, attack_type=&#39;fgsm&#39;)</code>).</li>\n</ul>\n<p><strong><code>capstone_attack.py</code> (Conceptual Structure):</strong></p>\n<pre><code class=\"language-python\">import numpy as np\n# Import your helper modules/functions\n# from sdr_utils import capture_samples, transmit_samples\n# from data_processing import iq_to_spectrogram\n# from ml_utils import load_classifier_model, predict\n# from adversarial_utils import generate_adversarial_example\n\n# --- Configuration ---\nMODEL_PATH = &#39;path/to/your/trained/model.h5&#39; # Or .pth, depending on framework\nTX_SDR_CONFIG = {\n    &#39;freq&#39;: 915e6,       # Example frequency (ISM band)\n    &#39;sample_rate&#39;: 2e6,  # Example sample rate\n    &#39;power&#39;: -10         # Example power level (dBm - keep LOW for testing!)\n}\nRX_SDR_CONFIG = {\n    &#39;freq&#39;: 915e6,       # Must match TX freq\n    &#39;sample_rate&#39;: 2e6,  # Must match TX sample rate\n    &#39;duration&#39;: 0.1      # Duration to capture for classification\n}\nATTACK_PARAMS = {\n    &#39;epsilon&#39;: 0.05,     # Perturbation magnitude (tune carefully!)\n    &#39;attack_type&#39;: &#39;fgsm&#39;, # Or &#39;pgd&#39;, etc.\n    &#39;target_label&#39;: None # Set to a specific class index for targeted attack, None for untargeted\n}\nCLEAN_SIGNAL_SOURCE = &#39;file&#39; # Or &#39;capture&#39;, &#39;synthetic&#39;\nCLEAN_SIGNAL_PATH = &#39;path/to/your/clean_signal.iq&#39; # Or &#39;path/to/synthetic_generator&#39;\nCLEAN_SIGNAL_TRUE_LABEL = 0 # The index of the true class of the clean signal\n\n# --- 1. Load the Target Classifier Model ---\nprint(&quot;Loading target classifier model...&quot;)\ntry:\n    model = load_classifier_model(MODEL_PATH)\n    print(&quot;Model loaded successfully.&quot;)\nexcept Exception as e:\n    print(f&quot;Error loading model: {e}&quot;)\n    exit() # Cannot proceed without the model\n\n# --- 2. Obtain Clean Signal Sample ---\nprint(f&quot;Obtaining clean signal sample from {CLEAN_SIGNAL_SOURCE}...&quot;)\nclean_iq_data = None\nif CLEAN_SIGNAL_SOURCE == &#39;file&#39;:\n    try:\n        # Load IQ data from file (assuming complex numpy array)\n        clean_iq_data = np.fromfile(CLEAN_SIGNAL_PATH, dtype=np.complex64)\n        print(f&quot;Loaded {len(clean_iq_data)} IQ samples.&quot;)\n    except Exception as e:\n        print(f&quot;Error loading clean signal file: {e}&quot;)\n        exit()\nelif CLEAN_SIGNAL_SOURCE == &#39;capture&#39;:\n    # This is more complex - you&#39;d need to capture a specific signal instance\n    # from the air, isolate it, and ensure it&#39;s clean.\n    # For a simple capstone, loading from a pre-captured file is easier.\n    print(&quot;Live capture for clean signal source is advanced. Using file for capstone.&quot;)\n    exit() # Or implement capture logic here\nelif CLEAN_SIGNAL_SOURCE == &#39;synthetic&#39;:\n     # Implement synthetic generation logic here\n     print(&quot;Synthetic generation for clean signal source is advanced. Using file for capstone.&quot;)\n     exit() # Or implement synth logic\n\nif clean_iq_data is None or len(clean_iq_data) == 0:\n    print(&quot;Failed to obtain clean signal data.&quot;)\n    exit()\n\n# Ensure the clean data length matches what your model expects after processing\n# (e.g., if Spectrograms need a fixed input size)\n# You might need padding, trimming, or capturing a specific length.\n# Let&#39;s assume for this example, clean_iq_data is already the correct length needed\n# for processing into one input sample for the model.\n# Example: If your spectrogram processing takes N samples:\n# clean_iq_data = clean_iq_data[:N] # Trim or handle size mismatch\n\n# --- 3. Preprocess Clean Signal for Model Input ---\n# Convert IQ data to the format the model expects (e.g., Spectrogram image)\nprint(&quot;Preprocessing clean signal for model input...&quot;)\ntry:\n    # This function needs to match the preprocessing done in Module 4/6\n    clean_processed_data = iq_to_spectrogram(clean_iq_data, RX_SDR_CONFIG[&#39;sample_rate&#39;])\n    # Models usually expect a batch dimension, even for a single sample\n    clean_processed_data = np.expand_dims(clean_processed_data, axis=0) # Add batch dimension\n    print(&quot;Preprocessing complete.&quot;)\nexcept Exception as e:\n    print(f&quot;Error during clean signal preprocessing: {e}&quot;)\n    exit()\n\n\n# --- 4. Classify the Clean Signal (Verify Target System) ---\nprint(&quot;Classifying the clean signal with the target model...&quot;)\ntry:\n    clean_prediction = predict(model, clean_processed_data)\n    predicted_label = np.argmax(clean_prediction)\n    print(f&quot;Clean signal classified as label: {predicted_label}&quot;)\n    if predicted_label != CLEAN_SIGNAL_TRUE_LABEL:\n        print(&quot;WARNING: Clean signal was NOT correctly classified by the target model!&quot;)\n        print(&quot;This might indicate an issue with the model, data processing, or true label.&quot;)\n        # Decide if you want to continue or exit\n        # exit()\n    else:\n         print(&quot;Clean signal correctly classified. Proceeding with attack generation.&quot;)\nexcept Exception as e:\n    print(f&quot;Error during clean signal classification: {e}&quot;)\n    exit()\n\n\n# --- 5. Generate Adversarial Perturbation ---\nprint(f&quot;Generating adversarial perturbation ({ATTACK_PARAMS[&#39;attack_type&#39;]} attack)...&quot;)\ntry:\n    # This function takes the model and the *processed* clean data\n    # It needs access to the model&#39;s gradients (for white-box attacks like FGSM/PGD)\n    # The output is the *perturbation* in the *processed data* domain\n    # Or, your generate function might output the *adversarial processed data* directly.\n    # Let&#39;s assume it outputs the perturbation in the *processed data* domain for clarity.\n    perturbation_processed = generate_adversarial_example(\n        model,\n        clean_processed_data, # Input to the model\n        target_label=ATTACK_PARAMS[&#39;target_label&#39;],\n        epsilon=ATTACK_PARAMS[&#39;epsilon&#39;],\n        attack_type=ATTACK_PARAMS[&#39;attack_type&#39;]\n    )\n    print(&quot;Perturbation generated in processed data domain.&quot;)\n\n    # --- IMPORTANT: Translate Perturbation back to IQ Domain ---\n    # This is a CRITICAL and often DIFFICULT step.\n    # If your attack was on Spectrograms, how do you create an IQ waveform\n    # that results in *that specific* adversarial spectrogram?\n    # For this capstone, a simpler approach is often taken:\n    # a) Attack directly in the IQ domain (requires an IQ-domain model or differentiable IQ-&gt;Processed conversion).\n    # b) Generate perturbation in processed domain, then use an approximation or optimization\n    #    to find an IQ perturbation that *approximates* the desired processed perturbation.\n    # c) Simplify: Generate perturbation *in the IQ domain* directly using an IQ-based attack\n    #    (e.g., FGSM on IQ data, requires model taking IQ as input or a differentiable pipeline).\n\n    # Let&#39;s assume for this example, your `generate_adversarial_example` function\n    # takes the *IQ data* and the *model* and returns the *adversarial IQ data* directly.\n    # This implies your model or a wrapper is differentiable w.r.t. the IQ input.\n    # This was a key challenge discussed in Module 6 - choose the method implemented there.\n    # If you attacked spectrograms, you need a function here to go from\n    # `adversarial_processed_data` back to `adversarial_iq_data`. This is non-trivial.\n    #\n    # REVISITING M6/M7: A common capstone approach is to generate the perturbation\n    # directly in the IQ domain if your model can handle it, or if you use a simpler\n    # attack that *adds* noise directly to IQ based on processed gradients.\n    # Let&#39;s adapt the flow slightly based on a simpler IQ-domain attack generation:\n\n    print(&quot;Generating adversarial perturbation directly in IQ domain...&quot;)\n    # We need the model accessible within the IQ-domain perturbation generation\n    # Or, the function needs access to the model&#39;s prediction/gradient function.\n    # Let&#39;s assume `generate_adversarial_iq` exists from M6 that takes clean IQ and model.\n    adversarial_iq_data = generate_adversarial_iq(\n        model, # The target model\n        clean_iq_data, # The clean IQ data\n        target_label=ATTACK_PARAMS[&#39;target_label&#39;],\n        epsilon=ATTACK_PARAMS[&#39;epsilon&#39;], # Epsilon applied in IQ domain (L-inf norm usually)\n        attack_type=ATTACK_PARAMS[&#39;attack_type&#39;]\n    )\n    print(&quot;Adversarial IQ data generated.&quot;)\n\nexcept Exception as e:\n    print(f&quot;Error during adversarial generation: {e}&quot;)\n    exit()\n\n# --- 6. Prepare Adversarial Signal for Transmission ---\n# (If generate_adversarial_iq already output the final IQ data, this step is minimal)\n# Ensure data type is correct (complex64), apply any final scaling if needed for SDR TX.\nadversarial_tx_iq = adversarial_iq_data\nprint(f&quot;Adversarial signal prepared for TX. Length: {len(adversarial_tx_iq)}&quot;)\n\n# --- 7. Transmit the Adversarial Signal ---\nprint(f&quot;Transmitting adversarial signal on {TX_SDR_CONFIG[&#39;freq&#39;]/1e6} MHz...&quot;)\ntry:\n    # Ensure your transmit function is non-blocking or run in a thread\n    # if you need to simultaneously capture with another SDR on the same machine.\n    transmit_samples(\n        adversarial_tx_iq,\n        TX_SDR_CONFIG[&#39;freq&#39;],\n        TX_SDR_CONFIG[&#39;sample_rate&#39;],\n        TX_SDR_CONFIG[&#39;power&#39;]\n    )\n    print(&quot;Adversarial signal transmission initiated.&quot;)\n    # Add a small delay if needed before stopping TX or starting RX capture\n    # time.sleep(len(adversarial_tx_iq) / TX_SDR_CONFIG[&#39;sample_rate&#39;] + 0.1)\n    # Ensure transmit function handles stopping TX after sending the data once.\n\nexcept Exception as e:\n    print(f&quot;Error during SDR transmission: {e}&quot;)\n    # You might want to stop any running TX process here\n    exit()\n\n# --- 8. Receive and Classify the Transmitted Signal ---\nprint(f&quot;Capturing transmitted signal on {RX_SDR_CONFIG[&#39;freq&#39;]/1e6} MHz for classification...&quot;)\ntry:\n    # Capture the signal as it&#39;s received over the air\n    # This capture needs to align with when the TX happens.\n    # If TX sends a single burst, RX needs to capture during that burst.\n    # If TX sends repeatedly, RX captures a segment.\n    # This synchronization is a key challenge!\n    # For a simple test, you might just capture *after* initiating TX, hoping to catch it.\n    # A more robust approach involves triggering TX and RX simultaneously or using a loop.\n    received_iq_data = capture_samples(\n        RX_SDR_CONFIG[&#39;freq&#39;],\n        RX_SDR_CONFIG[&#39;sample_rate&#39;],\n        RX_SDR_CONFIG[&#39;duration&#39;] # Capture duration\n    )\n    print(f&quot;Captured {len(received_iq_data)} samples.&quot;)\n\n    # Find the segment in the received data that corresponds to the transmitted signal\n    # This is another challenge! Simple correlation or energy detection might work\n    # if the signal is isolated. If it&#39;s added to existing noise, it&#39;s harder.\n    # For the capstone, you might simplify by assuming the *entire* capture is the signal,\n    # or that the signal is at the beginning of the capture.\n    # Let&#39;s assume the capture duration is exactly the signal length for simplicity.\n    if len(received_iq_data) &lt; len(adversarial_tx_iq):\n         print(&quot;Warning: Captured data is shorter than transmitted data. Classification might fail.&quot;)\n         # Pad or handle size mismatch\n    # Trim or pad received_iq_data to the expected size for processing\n    processed_received_iq = received_iq_data[:len(adversarial_tx_iq)] # Example trimming\n\n    # Preprocess the received data for the model\n    print(&quot;Preprocessing received signal for model input...&quot;)\n    processed_received_data = iq_to_spectrogram(processed_received_iq, RX_SDR_CONFIG[&#39;sample_rate&#39;])\n    processed_received_data = np.expand_dims(processed_received_data, axis=0) # Add batch dimension\n    print(&quot;Preprocessing complete.&quot;)\n\n    # Classify the received adversarial signal\n    print(&quot;Classifying the received adversarial signal...&quot;)\n    adversarial_prediction = predict(model, processed_received_data)\n    adversarial_predicted_label = np.argmax(adversarial_prediction)\n    print(f&quot;Received adversarial signal classified as label: {adversarial_predicted_label}&quot;)\n\n    # --- 9. Evaluate Attack Success ---\n    print(&quot;\\n--- Attack Evaluation ---&quot;)\n    print(f&quot;True Clean Label: {CLEAN_SIGNAL_TRUE_LABEL}&quot;)\n    print(f&quot;Clean Signal Classified As: {predicted_label}&quot;)\n    print(f&quot;Adversarial Signal Classified As: {adversarial_predicted_label}&quot;)\n\n    if adversarial_predicted_label != CLEAN_SIGNAL_TRUE_LABEL:\n        print(&quot;Attack Successful (Untargeted Misclassification)!&quot;)\n        if ATTACK_PARAMS[&#39;target_label&#39;] is not None and adversarial_predicted_label == ATTACK_PARAMS[&#39;target_label&#39;]:\n             print(&quot;Attack Successful (Targeted Misclassification)!&quot;)\n        elif ATTACK_PARAMS[&#39;target_label&#39;] is not None:\n             print(&quot;Attack resulted in misclassification, but not the targeted label.&quot;)\n    else:\n        print(&quot;Attack Failed: Adversarial signal still classified as the true label.&quot;)\n\n    # Optional: Quantify perturbation magnitude in IQ domain\n    # perturbation_iq = adversarial_iq_data - clean_iq_data\n    # l_inf_norm = np.max(np.abs(perturbation_iq))\n    # l_2_norm = np.linalg.norm(perturbation_iq)\n    # print(f&quot;Perturbation L-inf norm (IQ): {l_inf_norm}&quot;)\n    # print(f&quot;Perturbation L-2 norm (IQ): {l_2_norm}&quot;)\n\n\nexcept Exception as e:\n    print(f&quot;Error during reception or classification: {e}&quot;)\n    exit()\n\nprint(&quot;\\nCapstone attack simulation finished.&quot;)\n</code></pre>\n<p><strong>Explanation of Code Structure and Challenges:</strong></p>\n<ul>\n<li><strong>Modularity:</strong> The code relies heavily on the helper functions developed in previous modules. This makes the main script cleaner and easier to understand.</li>\n<li><strong>Configuration:</strong> All parameters (frequencies, sample rates, file paths, attack epsilon) are at the top for easy modification.</li>\n<li><strong>Workflow Steps:</strong> The code follows the logical steps outlined in Section 8.2.</li>\n<li><strong>Error Handling:</strong> Basic <code>try...except</code> blocks are included, but real-world robustness requires more.</li>\n<li><strong>Key Challenge - IQ to Processed Data Differentiability:</strong> The most complex part of generating adversarial RF examples for <em>over-the-air</em> transmission is often translating the required perturbation in the <em>processed data domain</em> (like Spectrograms) back into the <em>IQ domain</em>. The simplified code above assumes your <code>generate_adversarial_iq</code> function somehow handles this, or that your attack operates <em>directly</em> on IQ data, which is a significant simplification. A more realistic approach might involve iterative optimization in the IQ domain guided by the processed data gradient. Be prepared to discuss this limitation!</li>\n<li><strong>Key Challenge - Synchronization:</strong> Capturing the signal <em>exactly</em> when it&#39;s transmitted is hard. The code assumes the RX capture duration aligns perfectly or that the signal is easy to find in the capture. In reality, you might need energy detection, correlation, or precise timing/triggering between SDRs.</li>\n<li><strong>Key Challenge - Channel Effects:</strong> The digital adversarial example assumes a perfect channel. The over-the-air transmission introduces noise, fading, multipath, and hardware non-linearities. The perturbation needs to be robust enough to survive these effects and <em>still</em> cause misclassification upon reception and processing. Your chosen <code>epsilon</code> (perturbation magnitude) is crucial here – too small, and the channel destroys it; too large, and it might be detectable or exceed legal power limits.</li>\n</ul>\n<h3><strong>Section 8.4: Setting up the Physical Testbed</strong></h3>\n<p>Executing the attack requires a controlled physical environment.</p>\n<p><strong>Recommended Setup:</strong></p>\n<ol>\n<li><strong>Two SDRs:</strong> One configured for Transmission (TX SDR), one for Reception (RX SDR). They should be connected to the same computer running your attack script, or on separate computers communicating over a network (more complex).</li>\n<li><strong>Antennas:</strong> Use small, low-gain antennas appropriate for your chosen frequency band.</li>\n<li><strong>Attenuators:</strong> <strong>CRITICAL!</strong> Use inline RF attenuators between the TX SDR and its antenna, and potentially between the RX SDR and its antenna. This allows you to control the signal strength precisely and keep transmission power extremely low to stay within legal limits and avoid interfering with others. Start with high attenuation (~30-40 dB or more) and reduce it gradually.</li>\n<li><strong>Cables:</strong> Using RF cables to directly connect the TX and RX SDRs (via attenuators!) is the <em>most controlled</em> environment. This eliminates multipath and external interference, making your results repeatable. This is highly recommended for the capstone demonstration.</li>\n<li><strong>Shielding (Optional but Recommended):</strong> Conduct the experiment in a shielded box (like a Faraday cage) if possible, especially if transmitting any power over the air, to contain your signals.</li>\n<li><strong>Environment:</strong> Choose a location away from sensitive RF systems. Check local regulations <em>rigorously</em> before transmitting anything. Using ISM bands (like 915 MHz, 2.4 GHz, 5.8 GHz) at very low power is generally safer, but <em>always</em> verify local laws. <strong>When in doubt, use cables and attenuators instead of antennas for purely lab-based testing.</strong></li>\n</ol>\n<p><strong>Diagram (Cabled Testbed):</strong></p>\n<pre><code>+-----------------+       +--------------+       +--------------+       +-----------------+\n|  TX SDR         |-------|  RF Cable    |-------|  Attenuator  |-------|  RF Cable    |-------+\n| (Computer A/Same)|       |              |       |              |       |              |       |\n+-----------------+       +--------------+       +--------------+       +--------------+       |\n                                                                                                |\n                                                                                                |\n+-----------------+       +--------------+       +--------------+       +-----------------+   |\n|  RX SDR         |-------|  RF Cable    |-------|  Attenuator  |-------|  RF Cable    |&lt;------+\n| (Computer B/Same)|       |              |       |              |       |              |\n+-----------------+       +--------------+       +--------------+       +--------------+\n</code></pre>\n<p><strong>Setup Steps:</strong></p>\n<ol>\n<li>Connect the TX SDR to its antenna (or cable/attenuator chain).</li>\n<li>Connect the RX SDR to its antenna (or cable/attenuator chain).</li>\n<li>If using cables,</li>\n</ol>\n\n                </div>\n             </div>\n         "
  },
  "sidebarOverview": "\n         <div class=\"card course-progress-card\">\n             <h3>Course Progress</h3>\n             <!-- Progress bar placeholder -->\n             <div class=\"progress-bar-container\">\n                 <div class=\"progress-bar\" style=\"width: 0%;\"></div>\n             </div>\n             <p>0% Complete</p>\n             <p>0/8 modules completed</p>\n             <button>Continue Learning</button>\n         </div>\n         <div class=\"card\">\n             <h3>What You'll Learn</h3>\n             <div class=\"markdown-content text-center\"> <p>Coming Soon</p>\n </div> <!-- Placeholder Coming Soon -->\n         </div>\n         <div class=\"card\">\n             <h3>Requirements</h3>\n              <div class=\"markdown-content text-center\"> <p>Coming Soon</p>\n </div> <!-- Placeholder Coming Soon -->\n         </div>\n     ",
  "rawModules": [
    {
      "title": "module_1",
      "description": "module_1 Overview",
      "order": 1,
      "content": "Alright team, strap in! We're about to embark on a seriously cool journey into the heart of the RF spectrum, armed with software and a healthy dose of curiosity. This first module is all about building that rock-solid foundation. Think of it as learning the alphabet before you write a novel. We'll cover the basics of what makes radio signals tick, how our awesome Software-Defined Radios (SDRs) let us play with them, and why putting AI in charge of analyzing signals opens up fascinating new attack surfaces.\r\n\r\nLet's dive into Module 1!\r\n\r\n---\r\n\r\n## **Module 1: Foundations: RF, SDRs, and the AI Spectrum Landscape**\r\n\r\n**Welcome!**\r\n\r\nHey everyone, and welcome to \"Adversarial RF Attacks on AI Spectrum Systems using SDRs\"! I'm genuinely excited to guide you through this cutting-edge topic. We're going to bridge the worlds of radio frequencies, hacking, and artificial intelligence, and you'll gain the skills to understand and even replicate the core concepts behind adversarial attacks in the physical RF domain.\r\n\r\nThis isn't just theoretical; we're building towards a functional \"clone\" of an adversarial system by the end of the course. And it all starts right here, with the fundamentals.\r\n\r\n**Module Objective:**\r\n\r\nBy the end of this module, you will:\r\n\r\n*   Understand the fundamental concepts of Radio Frequency communications and the Electromagnetic Spectrum.\r\n*   Grasp the core capabilities and architecture of Software-Defined Radios (SDRs).\r\n*   Recognize the increasing role of Artificial Intelligence in analyzing the RF spectrum.\r\n*   Identify the critical intersection where AI's reliance on data meets the physical reality of RF, creating potential vulnerabilities.\r\n*   Be introduced to the concept of Adversarial Machine Learning (AML) in the RF context.\r\n*   Understand the crucial ethical and legal considerations when working with RF transmission.\r\n*   Have your development environment set up and ready to capture and visualize real-world RF data using an SDR.\r\n\r\n**Why This Module Matters:**\r\n\r\nYou can't attack what you don't understand. This module provides the essential vocabulary and foundational knowledge in RF and SDRs. It also introduces *why* AI in this domain is interesting from an offensive security perspective, setting the stage for everything that follows. Think of it as learning the landscape and identifying the target building before planning your approach.\r\n\r\n---\r\n\r\n### **Essential Subtopics Deep Dive:**\r\n\r\n#### **1. Introduction to the Electromagnetic Spectrum and RF Signals**\r\n\r\n*   **What is RF?** Radio Frequency is a part of the electromagnetic (EM) spectrum. Just like visible light, X-rays, or microwaves, RF energy travels in waves. The key difference is their frequency (how many wave cycles pass a point per second) and wavelength (the physical distance of one wave cycle).\r\n*   **The EM Spectrum:** Imagine a giant rainbow of energy. At one end, you have very low frequencies (like power lines). As you move up, you get AM radio, FM radio, TV broadcasts, Wi-Fi, Bluetooth, cellular signals, satellite communications, radar, microwaves (for heating food!), infrared, visible light, ultraviolet, X-rays, and finally, gamma rays. RF generally sits below the infrared part of the spectrum.\r\n    *   **Key Relationship:** The speed of light (`c`, approximately 300,000,000 meters/second) is constant for all EM waves in a vacuum. The relationship is `c = frequency (f) * wavelength (λ)`. This means higher frequencies have shorter wavelengths, and lower frequencies have longer wavelengths. This is important for antenna design and how signals propagate.\r\n*   **RF Signals:** These are time-varying electromagnetic waves used to carry information wirelessly. They start as electrical signals, are converted to EM waves by a transmitting antenna, travel through the air, and are converted back to electrical signals by a receiving antenna.\r\n\r\n#### **2. Basic Signal Properties: Frequency, Amplitude, Phase, Modulation Types**\r\n\r\nThink of a pure, simple radio wave like a single musical note.\r\n\r\n*   **Frequency (Hz):** This is the *pitch* of our note. It's the number of cycles the wave completes per second. Measured in Hertz (Hz), Kilohertz (kHz), Megahertz (MHz), Gigahertz (GHz). A signal centered at 99.5 MHz is an FM radio station. A signal at 2.4 GHz is likely Wi-Fi or Bluetooth.\r\n*   **Amplitude (V or Power):** This is the *loudness* of our note. It's the height or intensity of the wave. Higher amplitude usually means a stronger signal, easier to receive. Measured in Volts (V) or often discussed in terms of Power (Watts, milliwatts, dBm).\r\n*   **Phase (degrees or radians):** This describes the position of the wave within a single cycle at a specific point in time relative to a reference. Think of it as the *timing* or *starting point* of the wave's oscillation. If you have two waves of the same frequency, their phase difference tells you how \"offset\" they are from each other.\r\n*   **Modulation:** This is how we take raw information (like voice, music, or digital data) and *encode* it onto our carrier wave (the basic frequency). We change one or more of the carrier wave's properties (Amplitude, Frequency, or Phase) in a controlled way according to the information we want to send.\r\n    *   **Amplitude Modulation (AM):** The amplitude of the carrier wave is varied according to the information signal. Simpler to implement, but more susceptible to noise. (Think classic AM radio).\r\n    *   **Frequency Modulation (FM):** The frequency of the carrier wave is varied according to the information signal. More complex, but more resistant to noise. (Think FM radio).\r\n    *   **Phase Modulation (PM):** The phase of the carrier wave is varied according to the information signal. Often used in digital schemes.\r\n    *   **Digital Modulation:** Combinations and variations of AM, FM, and PM are used to encode digital bits (0s and 1s). Examples include Frequency Shift Keying (FSK), Phase Shift Keying (PSK), Quadrature Amplitude Modulation (QAM - combines AM and PSK), Orthogonal Frequency-Division Multiplexing (OFDM - used in Wi-Fi, LTE, 5G). We'll encounter these as the signals our AI tries to classify.\r\n\r\n#### **3. What is a Software-Defined Radio (SDR)? Architecture and Capabilities**\r\n\r\nThis is our primary tool!\r\n\r\n*   **Traditional Radio vs. SDR:** A traditional radio (like the one in your car) uses dedicated, specialized electronic circuits (hardware) for each step: tuning to a frequency, filtering out others, demodulating the signal. To change what kind of signal it can receive (e.g., from AM to FM), you need different hardware circuits.\r\n*   **SDR Philosophy:** An SDR moves as much of the signal processing as possible from dedicated hardware into *software* running on a general-purpose processor (like your computer's CPU).\r\n*   **Basic Architecture:**\r\n    1.  **Antenna:** Captures RF energy from the air.\r\n    2.  **RF Front End:** Basic amplification, filtering, and frequency shifting (downconversion for RX, upconversion for TX) to bring the signal into a range that can be digitized.\r\n    3.  **Analog-to-Digital Converter (ADC) / Digital-to-Analog Converter (DAC):** This is the crucial step. The ADC converts the analog RF signal (or a downconverted version) into a stream of digital numbers (for RX). The DAC converts digital numbers back into an analog signal (for TX).\r\n    4.  **Digital Processing:** This is where the \"software-defined\" magic happens. A powerful chip (often an FPGA or a dedicated DSP) or your computer's CPU processes these digital samples. This is where tuning, filtering, demodulation, modulation, etc., are performed *in software*.\r\n*   **Key Capabilities:**\r\n    *   **Receive (RX):** Capture a specific range of frequencies.\r\n    *   **Transmit (TX):** Generate and broadcast signals (requires a TX-capable SDR).\r\n    *   **Bandwidth:** The *range* of frequencies the SDR can capture or transmit *simultaneously*. A larger bandwidth means you can see or interact with more of the spectrum at once. Limited by the ADC/DAC speed.\r\n    *   **Sample Rate:** The number of digital samples taken per second from the analog signal. According to the Nyquist theorem, the sample rate must be at least twice the bandwidth you want to capture. Often, sample rate and bandwidth are closely related or used interchangeably in SDR specs, but technically Bandwidth is the usable frequency range, Sample Rate is how fast you're digitizing.\r\n    *   **Frequency Range:** The range of frequencies the hardware's front end can tune to.\r\n*   **Why SDRs are Powerful for this Course:** They give us the flexibility to programmatically capture *any* signal within their range, analyze its raw digital representation (IQ data), generate *arbitrary* digital waveforms, and then transmit them. This is *exactly* what we need to build and test adversarial RF attacks.\r\n\r\n#### **4. Overview of AI Applications in RF: Spectrum Sensing, Signal Classification, Anomaly Detection, Jamming Detection**\r\n\r\nThe RF spectrum is becoming incredibly crowded and dynamic. Manually monitoring and understanding everything happening is impossible. This is where AI shines.\r\n\r\n*   **Spectrum Sensing:** AI can quickly scan wide swaths of spectrum to detect the presence of signals, identify occupied frequencies, and find available \"white space.\" Used in cognitive radio to dynamically hop frequencies.\r\n*   **Signal Classification (Modulation Recognition):** Given a slice of captured RF data, AI can analyze its characteristics (modulation type, bandwidth, symbol rate) to identify *what kind* of signal it is. Is it Wi-Fi? Bluetooth? LoRa? FM broadcast? A military signal? *This is the primary target for our adversarial attacks.*\r\n*   **Anomaly Detection:** AI can learn what \"normal\" spectrum activity looks like and flag anything unusual – a signal appearing where it shouldn't, a signal with strange characteristics, or unexpected interference.\r\n*   **Jamming Detection:** A specific type of anomaly detection focused on identifying intentional interference designed to disrupt communication. AI can potentially differentiate jamming from natural noise or interference.\r\n\r\n**Case Study Idea:** Imagine a battlefield scenario. AI is used to monitor enemy radio communications. It classifies signals to identify different units or types of activity. Accurate classification is critical for situational awareness.\r\n\r\n#### **5. Why AI in RF is a Target: Reliance on Data Patterns, Potential for Deception, Impact of Failure in Critical Applications**\r\n\r\nThis is the core motivation for the course! Why would someone *want* to attack an AI system analyzing RF?\r\n\r\n*   **Reliance on Data Patterns:** AI models, especially deep learning models, learn complex patterns from the data they are trained on. They become very good at identifying signals that look like their training data. However, they can be surprisingly brittle when presented with inputs that are slightly, but specifically, different.\r\n*   **Potential for Deception (Adversarial Examples):** Just as you can craft an image that looks like a cat to a human but is classified as a dog by an AI, you can potentially craft an RF signal that sounds/looks like one thing but is classified as another by an AI.\r\n    *   **Example Scenario:** Make a malicious signal look like benign noise. Make an enemy signal look like a friendly signal. Make a critical signal look like something unimportant.\r\n*   **Impact of Failure in Critical Applications:** As AI is deployed in more critical RF systems (military comms, air traffic control, critical infrastructure monitoring, autonomous systems), fooling the AI has increasingly severe consequences. Misclassifying a signal could lead to missed threats, incorrect decisions, or system failures. This raises the stakes significantly compared to fooling an image classifier on social media.\r\n\r\n#### **6. Introduction to Adversarial Machine Learning (AML) Concept: The Idea of Crafted Inputs to Fool AI**\r\n\r\nWe touched on this, but let's formalize it slightly.\r\n\r\n*   **The Core Idea:** AML is the study of vulnerabilities in machine learning models to malicious inputs designed to trick them. An \"adversarial example\" is a specially crafted input (like an image, text, or in our case, an RF signal representation) that is very similar to a legitimate input but causes the model to make a wrong prediction.\r\n*   **Not Random Noise:** This is key. Adversarial perturbations are *calculated*. They often exploit the linearity or high dimensionality of the model's decision boundaries. Adding random noise might *reduce* accuracy, but an adversarial perturbation is designed to achieve a *specific* misclassification or evasion with minimal changes to the original input.\r\n*   **Our Goal:** In this course, we want to generate adversarial *RF signals* that, when processed by an AI classifier, cause it to misidentify the signal type.\r\n\r\n#### **7. Ethical and Legal Considerations: Responsible Disclosure, Legal Boundaries of RF Transmission**\r\n\r\n**This is non-negotiable.** We are working with the physical airwaves, which are a regulated public resource.\r\n\r\n*   **Legality:** Transmitting radio signals is heavily regulated by government bodies (like the FCC in the US, Ofcom in the UK, etc.).\r\n    *   You *must* operate within legal limits. This means:\r\n        *   Using authorized frequencies (e.g., ISM bands like 2.4 GHz or 915 MHz, *if* your SDR supports them and you follow power restrictions).\r\n        *   Operating at legal power levels (often very low for unlicensed bands).\r\n        *   *Never* interfering with licensed or critical services (emergency services, air traffic control, commercial broadcasts, etc.).\r\n    *   **Jamming is Illegal:** Intentionally interfering with communications is against the law in most places. Our goal is *not* to jam, but to *deceive* an AI classifier. While generating adversarial signals involves transmission, it must be done in a way that *only* affects your controlled test environment and does not cause harmful interference.\r\n*   **Ethical Responsibility:**\r\n    *   **Responsible Research:** Your work should aim to understand vulnerabilities to improve security, not to cause harm.\r\n    *   **Controlled Environment:** Whenever possible, use cables, attenuators, or shielded enclosures (like a Faraday cage) for transmission testing to prevent signals from radiating and potentially causing interference.\r\n    *   **Low Power:** When over-the-air testing is necessary and legal, use the lowest possible power.\r\n    *   **Responsible Disclosure:** If you find significant vulnerabilities in real-world systems, follow responsible disclosure practices (contact the vendor/owner privately) rather than making them public immediately.\r\n*   **In this course:** We will emphasize using *controlled test environments* (cables, attenuators) or operating strictly within legal, low-power, unlicensed bands for transmission exercises. We are building a *functional clone* of the attack *concept*, not building a tool for malicious activity.\r\n\r\n#### **8. Setting up the Development Environment: SDR Drivers, Python, Essential Libraries**\r\n\r\nAlright, let's get our hands dirty and set up our workspace.\r\n\r\n*   **Hardware:** You need an SDR.\r\n    *   **RTL-SDR:** Excellent and cheap for RX only. Great for starting with capture and analysis.\r\n    *   **HackRF One, LimeSDR, ADALM-PLUTO:** More expensive, but capable of both RX and TX. You'll need one of these (or similar) for the later TX modules. *For Module 1, an RTL-SDR is sufficient.*\r\n*   **Operating System:** Linux (Ubuntu, Kali, etc.) is generally the most common and best-supported environment for SDR work, especially with open-source tools. macOS also works well. Windows is possible but can sometimes require more effort for driver setup.\r\n*   **Python:** We'll use Python for scripting our SDR control, data processing, and AI model interaction. Install Python 3 if you don't have it. Use a virtual environment (`venv` or `conda`) to keep your project dependencies clean.\r\n*   **Essential Libraries:**\r\n    *   `numpy`: For numerical operations, especially handling IQ data arrays and performing FFTs.\r\n    *   `scipy`: Contains various scientific tools, including signal processing functions that might be useful.\r\n    *   `matplotlib`: For plotting and visualizing spectrum data, spectrograms, etc.\r\n    *   `pyrtlsdr` (or `pyhackrf`, `pysdr`): Python libraries to interface with your specific SDR hardware. `pyrtlsdr` is the go-to for RTL-SDR. `pysdr` offers a potentially cleaner abstraction layer over different SDRs.\r\n\r\n**Step-by-Step Setup Guide:**\r\n\r\n1.  **Install Python 3 and pip:** Most modern OS have Python 3 pre-installed. Check with `python3 --version` and `pip3 --version`. If not installed, follow guides for your specific OS.\r\n2.  **Create a Virtual Environment (Recommended):**\r\n    ```bash\r\n    python3 -m venv sdr_env\r\n    source sdr_env/bin/activate # On Linux/macOS\r\n    # On Windows command prompt: sdr_env\\Scripts\\activate.bat\r\n    # On Windows PowerShell: sdr_env\\Scripts\\Activate.ps1\r\n    ```\r\n    You should see `(sdr_env)` at the start of your command prompt. This means you're inside the virtual environment.\r\n3.  **Install SDR Drivers:** This step is OS-dependent.\r\n    *   **Linux (Ubuntu/Debian):** `sudo apt update && sudo apt install librtlsdr-dev` (for RTL-SDR). You might need to blacklist the default DVB-T drivers so `librtlsdr` can access the device. Create a file like `/etc/modprobe.d/blacklist-rtl.conf` and add `blacklist dvb_usb_rtl28xxu` and `blacklist rtl2832`. Run `sudo rmmod dvb_usb_rtl28xxu rtl2832` and unplug/replug the SDR.\r\n    *   **macOS:** Use Homebrew: `brew install librtlsdr`.\r\n    *   **Windows:** This is trickier. You usually need `zadig` to replace the default Windows driver with a WinUSB driver. Search for guides specific to your SDR and Zadig.\r\n    *   *Verification:* Once drivers are installed, plug in your SDR and try a command-line tool like `rtl_test` (if using RTL-SDR). If it runs and shows stats without errors, your drivers are likely working.\r\n4.  **Install Python Libraries:** While *inside* your virtual environment:\r\n    ```bash\r\n    pip install numpy matplotlib scipy pyrtlsdr # Add pyhackrf or pysdr if needed for your hardware\r\n    ```\r\n5.  **Verify Python Libraries:** Open a Python interpreter or script *within your virtual environment* and try importing the libraries:\r\n    ```python\r\n    import numpy\r\n    import matplotlib.pyplot as plt\r\n    import scipy\r\n    from rtlsdr import RtlSdr # Or from hackrf import Hackrf, etc.\r\n\r\n    print(\"Libraries imported successfully!\")\r\n    ```\r\n    If no errors appear, you're good to go!\r\n\r\n---\r\n\r\n### **Suggested Resources/Prerequisites:**\r\n\r\n*   **Basic Python:** You should be comfortable writing simple scripts, using variables, loops, lists, and functions. Online tutorials like Codecademy, Coursera, or the official Python documentation are great.\r\n*   **Command Line:** Basic navigation (`cd`, `ls`/`dir`), running scripts, installing packages (`pip`, `apt`, `brew`).\r\n*   **Access to an SDR:** As mentioned, RTL-SDR (RX only) is the minimum for this module. A TX-capable SDR is needed for later modules.\r\n*   **Recommended Reading:**\r\n    *   \"RF & Wireless Technologies\" basics: Search for introductory articles or videos on frequency, wavelength, amplitude, phase, and modulation. The ARRL Handbook has great foundational chapters, though it's extensive.\r\n    *   Introductory SDR guides: The documentation for your specific SDR (e.g., the `pyrtlsdr` documentation, HackRF wiki) often has setup and basic usage examples. Michael Ossmann's SDR tutorials are excellent.\r\n\r\n---\r\n\r\n### **Module Project/Exercise: Capture, Visualize, and Identify**\r\n\r\nThis exercise is your first hands-on interaction with the RF spectrum using your SDR and Python. It confirms your setup works and gives you a taste of seeing real signals.\r\n\r\n**Objective:** Use your SDR and Python to capture a segment of the RF spectrum, perform a basic frequency analysis, and visualize the results.\r\n\r\n**Steps:**\r\n\r\n1.  **Ensure your SDR is connected** and drivers are working (e.g., run `rtl_test` if using RTL-SDR).\r\n2.  **Activate your Python virtual environment** (`source sdr_env/bin/activate`).\r\n3.  **Write a Python script** using `pyrtlsdr` (or your SDR library) to:\r\n    *   Import necessary libraries (`RtlSdr`, `numpy`, `matplotlib.pyplot`).\r\n    *   Create an SDR object (`sdr = RtlSdr()`).\r\n    *   Configure the SDR:\r\n        *   Set the center frequency (`sdr.center_freq = ...`). Choose a frequency range likely to have signals, like the FM broadcast band (around 88-108 MHz), or a known ISM band (like 915 MHz or 2.4 GHz, though 2.4 GHz needs a different SDR than RTL-SDR). For RTL-SDR, FM broadcast or airband (108-137 MHz) are good starting points. Let's target FM broadcast around 98 MHz.\r\n        *   Set the sample rate (`sdr.sample_rate = ...`). A common value for RTL-SDR is 2.048 MS/s (Mega Samples per second). This gives you a bandwidth of about 2 MHz.\r\n        *   Set the gain (`sdr.gain = 'auto'` or a specific value). Start with auto or a moderate value.\r\n    *   Capture a block of samples (`samples = sdr.read_samples(...)`). Start with a reasonable number, maybe 256k or 512k samples.\r\n    *   Close the SDR (`sdr.close()`).\r\n    *   Perform a Fast Fourier Transform (FFT) on the samples to move from the time domain to the frequency domain. This shows the strength of different frequencies within the captured bandwidth.\r\n        *   Use `numpy.fft.fft(samples)`.\r\n        *   Use `numpy.fft.fftshift` to center the zero frequency component.\r\n    *   Calculate the power spectral density (strength) from the FFT output (e.g., `power_spectrum = 20*np.log10(np.abs(fft_result))`). The `20*log10` converts to dB, a common unit for signal strength.\r\n    *   Create a frequency axis corresponding to the FFT results. This maps the FFT bin index to an actual frequency. `freqs = np.fft.fftshift(np.fft.fftfreq(len(samples), 1/sdr.sample_rate)) + sdr.center_freq`.\r\n    *   Plot the frequency spectrum using `matplotlib`. Plot `freqs` on the x-axis and `power_spectrum` on the y-axis. Label your axes!\r\n    *   Add a title to your plot indicating the center frequency and sample rate.\r\n    *   Display the plot (`plt.show()`).\r\n4.  **Run your script.**\r\n5.  **Observe the plot:**\r\n    *   Can you see a peak around your chosen center frequency?\r\n    *   Do you see other peaks? If you tuned to the FM band, you should see several peaks corresponding to radio stations.\r\n    *   What does the noise floor look like? (The relatively flat, lower part of the graph).\r\n    *   Try changing the center frequency and sample rate (staying within your SDR's limits) and see how the plot changes.\r\n\r\nHere's a basic Python code template to get you started:\r\n\r\n```python\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom rtlsdr import RtlSdr # Make sure this matches your installed library\r\n\r\n# --- SDR Configuration ---\r\n# Adjust these values based on your SDR and desired capture\r\ncenter_freq = 98e6  # Example: 98 MHz (FM broadcast band)\r\nsample_rate = 2.048e6 # Example: 2.048 MS/s\r\nnum_samples = 256 * 1024 # Number of samples to capture (e.g., 256k)\r\n\r\n# --- SDR Setup and Capture ---\r\nsdr = None\r\ntry:\r\n    sdr = RtlSdr() # Or your SDR class (e.g., Hackrf())\r\n\r\n    sdr.center_freq = center_freq\r\n    sdr.sample_rate = sample_rate\r\n    # sdr.gain = 'auto' # You can try 'auto' or a fixed value like 20, 30, etc.\r\n    # If using HackRF, gain settings are different (vga_gain, lna_gain)\r\n    # sdr.vga_gain = 30\r\n    # sdr.lna_gain = 8\r\n\r\n    print(f\"Capturing {num_samples} samples at {center_freq/1e6:.2f} MHz with {sample_rate/1e6:.2f} MS/s...\")\r\n    samples = sdr.read_samples(num_samples)\r\n    print(\"Capture complete.\")\r\n\r\nexcept Exception as e:\r\n    print(f\"Error during SDR setup or capture: {e}\")\r\n    print(\"Ensure your SDR is connected and drivers are correctly installed.\")\r\n    samples = None # Ensure samples is None if capture failed\r\n\r\nfinally:\r\n    if sdr:\r\n        sdr.close()\r\n        print(\"SDR closed.\")\r\n\r\n# --- Analysis and Visualization (if capture was successful) ---\r\nif samples is not None:\r\n    # Perform FFT\r\n    fft_result = np.fft.fft(samples)\r\n    fft_result_shifted = np.fft.fftshift(fft_result)\r\n\r\n    # Calculate power spectrum (in dB)\r\n    power_spectrum = 20 * np.log10(np.abs(fft_result_shifted))\r\n\r\n    # Create frequency axis\r\n    freqs = np.fft.fftshift(np.fft.fftfreq(len(samples), 1/sample_rate)) + center_freq\r\n\r\n    # Plot the spectrum\r\n    plt.figure(figsize=(12, 6))\r\n    plt.plot(freqs / 1e6, power_spectrum) # Plot frequency in MHz\r\n    plt.xlabel(\"Frequency (MHz)\")\r\n    plt.ylabel(\"Power (dB)\")\r\n    plt.title(f\"RF Spectrum around {center_freq/1e6:.2f} MHz (Sample Rate: {sample_rate/1e6:.2f} MS/s)\")\r\n    plt.grid(True)\r\n    plt.show()\r\nelse:\r\n    print(\"Skipping analysis and visualization due to capture error.\")\r\n\r\n```\r\n\r\n**Contribution to Capstone:** Successfully completing this exercise confirms that your SDR is working with your Python environment and libraries. You've demonstrated the ability to capture raw RF data and perform a basic frequency analysis, which is a fundamental building block for"
    },
    {
      "title": "module_2",
      "description": "module_2 Overview",
      "order": 2,
      "content": "Okay, class! Welcome back. If you've just finished Module 1, you should have your SDR environment set up and have dipped your toes into capturing some raw RF data. You understand the *why* – why AI is a target in the spectrum, and why SDRs are our tool.\r\n\r\nNow, in Module 2, we shift gears squarely into the *how*. This is where we get our hands dirty with the fundamental skills of using Software-Defined Radios programmatically. Think of this as boot camp for your SDR. By the end, you'll be comfortable capturing signals, looking at them in different ways, creating your own signals, and even transmitting them (responsibly, of course!). These are the absolute core mechanics we'll build upon for every subsequent module, especially when we start generating and transmitting those sneaky adversarial signals.\r\n\r\nLet's dive in!\r\n\r\n---\r\n\r\n## Module 2: SDR Mastery: Capture, Analysis, and Generation\r\n\r\n**Module Objective:** Become proficient in using SDRs to capture, analyze, and generate arbitrary RF signals, laying the practical groundwork for handling RF data and transmitting crafted signals.\r\n\r\n**Contribution to Capstone:** Develops the core SDR data handling and transmission skills required for the attack system.\r\n\r\n### 2.1 The SDR Software Ecosystem: Our Tools of the Trade\r\n\r\nWhile Gnu Radio Companion (GRC) is a fantastic graphical tool for building complex signal processing flows, our focus in this course is on programmatic control using Python. This allows us to integrate SDR operations directly with data analysis, machine learning libraries, and eventually, our adversarial attack generation code.\r\n\r\nWe'll primarily leverage powerful Python libraries built on top of SDR drivers:\r\n\r\n*   **`pyrtlsdr`:** A Python wrapper for `librtlsdr`, specifically for the ubiquitous and inexpensive RTL-SDR dongles. These are receive-only but excellent for spectrum sensing and signal capture. A great starting point!\r\n*   **`pyhackrf`:** A Python wrapper for `libhackrf`, for the HackRF One. This is a more capable, transmit-and-receive (half-duplex) SDR, essential for our signal generation and adversarial transmission later.\r\n*   **`pysdr`:** While less a driver wrapper and more a collection of DSP and SDR utilities *in* Python, `pysdr` provides helpful functions for signal analysis, modulation, and simulation that complement the driver libraries. It's great for understanding the math behind the operations.\r\n\r\n**Setup Check (from Module 1):** Ensure you have the necessary drivers (`librtlsdr`, `libhackrf`) installed and the corresponding Python libraries (`pip install pyrtlsdr pyhackrf pysdr`). Make sure your SDR is plugged in and recognized by your system (e.g., check `lsusb` on Linux/macOS, or device manager on Windows).\r\n\r\n### 2.2 Working with IQ Data: The Language of SDRs\r\n\r\nThis is *the* most fundamental concept when working with SDRs. SDRs don't give you a simple \"voltage vs. time\" like an oscilloscope on a low-frequency signal. They give you **IQ data**.\r\n\r\nWhy? Because RF signals are oscillating at very high frequencies. Digitizing the raw RF signal directly is incredibly difficult and requires impossibly fast Analog-to-Digital Converters (ADCs) for most bands. Instead, SDRs use a technique called **quadrature downconversion**.\r\n\r\nImagine your signal is at 100 MHz. The SDR mixes this signal with a local oscillator (LO) signal at or near 100 MHz. This mixing process shifts the signal's frequency down to a much lower frequency, centered around 0 Hz (baseband). Crucially, to preserve *all* the information (amplitude, frequency, *and phase*), the mixing is done twice, once with the LO signal directly (producing the **In-phase, I** component) and once with the LO signal phase-shifted by 90 degrees (producing the **Quadrature, Q** component).\r\n\r\n*   **I component:** Represents the signal's component in phase with the LO.\r\n*   **Q component:** Represents the signal's component 90 degrees out of phase with the LO.\r\n\r\nTogether, I and Q form a **complex number**: `Signal(t) = I(t) + jQ(t)`, where `j` is the imaginary unit (`sqrt(-1)`).\r\n\r\nThis complex number at any given time sample represents the signal's **magnitude** and **phase** relative to the center frequency the SDR is tuned to.\r\n\r\n*   **Magnitude:** `Amplitude = sqrt(I^2 + Q^2)`\r\n*   **Phase:** `Phase = atan2(Q, I)` (using the `atan2` function which handles quadrants correctly)\r\n\r\n**Why is this important?**\r\n\r\n1.  **Full Information:** IQ data preserves the complete state of the signal at baseband, allowing you to reconstruct or analyze *any* aspect of the original RF signal digitally.\r\n2.  **Digital Processing:** Once you have the IQ data, all further signal processing (filtering, demodulation, analysis, *and adversarial perturbation*) can be done efficiently in software.\r\n3.  **Adversarial Attacks:** Our adversarial perturbations will directly manipulate this IQ data to try and fool the AI classifier. Understanding its structure is paramount.\r\n\r\nWhen you capture data with an SDR, you get a sequence of these complex IQ samples over time. In Python, this is typically represented as a NumPy array of complex numbers (`dtype='complex64'` or `complex128`).\r\n\r\nLet's visualize some simple IQ data:\r\n\r\n```python\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\n# Generate some simple IQ data representing a tone slightly off center frequency\r\n# A complex exponential e^(j*theta) represents a signal rotating in the IQ plane\r\n# If theta = 2 * pi * f * t, it's a tone at frequency f relative to the center frequency\r\nsample_rate = 2.048e6 # Samples per second\r\nduration = 1e-3       # 1 millisecond\r\nfrequency_offset = 10e3 # 10 kHz tone relative to center frequency\r\n\r\nt = np.arange(int(duration * sample_rate)) / sample_rate\r\n# Create the complex tone: exp(j * 2 * pi * f * t)\r\niq_data = np.exp(1j * 2 * np.pi * frequency_offset * t)\r\n\r\n# Separate I and Q components\r\ni_data = iq_data.real\r\nq_data = iq_data.imag\r\n\r\n# Plot the I and Q components over time\r\nplt.figure(figsize=(12, 6))\r\nplt.plot(t, i_data, label='I (In-phase)')\r\nplt.plot(t, q_data, label='Q (Quadrature)')\r\nplt.xlabel('Time (s)')\r\nplt.ylabel('Amplitude')\r\nplt.title('IQ Data Components over Time (10 kHz Tone)')\r\nplt.legend()\r\nplt.grid(True)\r\nplt.show()\r\n\r\n# Plot the IQ data on the complex plane (constellation plot for a single tone)\r\nplt.figure(figsize=(6, 6))\r\nplt.scatter(i_data, q_data, s=1, alpha=0.5)\r\nplt.xlabel('I Component')\r\nplt.ylabel('Q Component')\r\nplt.title('IQ Constellation Plot (10 kHz Tone)')\r\nplt.axis('equal') # Important for constellation plots\r\nplt.grid(True)\r\nplt.show()\r\n```\r\n\r\n**Explanation:**\r\n\r\n*   The first plot shows the sinusoidal nature of the I and Q components over time. They are 90 degrees out of phase, as expected.\r\n*   The second plot, the \"constellation plot\" or IQ plot, shows the points `(I, Q)` in the complex plane. For a single, clean tone, this forms a perfect circle centered at the origin. The radius is the amplitude, and the speed of rotation around the circle corresponds to the frequency offset from the center.\r\n\r\nUnderstanding this representation is key because this is the raw material we'll capture, analyze, and manipulate.\r\n\r\n### 2.3 Capturing RF Signals: Bringing Data from the Air\r\n\r\nNow let's use our SDR library to capture real data. We'll start with `pyrtlsdr` as it's widely accessible.\r\n\r\n**Prerequisites:**\r\n\r\n*   An RTL-SDR dongle plugged in.\r\n*   `pyrtlsdr` installed (`pip install pyrtlsdr`).\r\n\r\n```python\r\nfrom rtlsdr import RtlSdr\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\n# --- Configuration ---\r\ncenter_freq = 100e6  # Tune to 100 MHz (example: near FM broadcast band)\r\nsample_rate = 2.048e6 # 2.048 MSps is a common rate for RTL-SDR\r\ngain = 'auto'        # Or a specific gain value, e.g., 40\r\nnum_samples = 2**20  # Number of samples to capture (a power of 2 is good for FFT)\r\n\r\n# --- SDR Setup and Capture ---\r\nsdr = None\r\ntry:\r\n    # List available devices and pick the first one\r\n    sdr = RtlSdr(0)\r\n\r\n    print(f\"Configuring SDR:\")\r\n    print(f\"  Center Freq: {center_freq/1e6} MHz\")\r\n    print(f\"  Sample Rate: {sample_rate/1e6} MSps\")\r\n    print(f\"  Gain: {gain}\")\r\n    print(f\"  Number of Samples: {num_samples}\")\r\n\r\n    sdr.sample_rate = sample_rate\r\n    sdr.center_freq = center_freq\r\n    sdr.gain = gain\r\n\r\n    print(\"Capturing data...\")\r\n    # Capture samples. This returns a NumPy array of complex numbers (IQ data).\r\n    samples = sdr.read_samples(num_samples)\r\n    print(f\"Captured {len(samples)} samples.\")\r\n\r\nexcept Exception as e:\r\n    print(f\"Error during SDR capture: {e}\")\r\n    print(\"Ensure your SDR is plugged in and drivers are installed correctly.\")\r\n    print(\"If using Linux, check udev rules for permissions.\")\r\n    print(\"You might need to run this script with sudo if permissions are an issue (use with caution!).\")\r\n    sdr = None # Ensure sdr variable is None if setup fails\r\n\r\nfinally:\r\n    # Always close the SDR connection when done\r\n    if sdr:\r\n        sdr.close()\r\n        print(\"SDR closed.\")\r\n\r\n# --- Optional: Save the captured data ---\r\nif sdr and len(samples) > 0:\r\n    file_name = f\"capture_{int(center_freq/1e6)}MHz_{int(sample_rate/1e6)}MSps_{num_samples}_samples.iq\"\r\n    samples.tofile(file_name)\r\n    print(f\"Data saved to {file_name}\")\r\n\r\n# --- Proceed to analysis in the next section using the 'samples' variable ---\r\n# If capture failed, 'samples' will not be defined or will be empty.\r\n# For analysis examples, you can load a pre-saved file if capture fails.\r\n```\r\n\r\n**Explanation:**\r\n\r\n1.  We import the necessary library (`RtlSdr`).\r\n2.  We define our capture parameters: `center_freq` (where we tune), `sample_rate` (how wide a band we capture around the center frequency - the bandwidth is `sample_rate`), `gain` (amplification level), and `num_samples` (how much data to grab).\r\n3.  We create an `RtlSdr` object, typically using the device index `0` if you only have one.\r\n4.  We configure the SDR with our chosen parameters.\r\n5.  `sdr.read_samples(num_samples)` performs the capture. This is a blocking call that waits until `num_samples` are received.\r\n6.  The captured data `samples` is a NumPy array of complex numbers (our IQ data!).\r\n7.  We include error handling and ensure the SDR is closed using a `try...finally` block.\r\n8.  Saving to a binary file (`.iq`) is shown, which is useful for later analysis or creating datasets.\r\n\r\n**Important Notes on Capture:**\r\n\r\n*   **Center Frequency vs. Signal Frequency:** The `center_freq` is the middle of the captured bandwidth. A signal at, say, `center_freq + 500e3` Hz will appear at `+500e3` Hz relative to the *baseband* (0 Hz) in your captured IQ data. A signal at `center_freq - 200e3` Hz will appear at `-200e3` Hz.\r\n*   **Sample Rate:** This determines the *bandwidth* you capture. An SDR capturing at `sample_rate` can capture signals within `center_freq ± sample_rate/2`. Higher sample rates capture wider swaths of spectrum but generate more data.\r\n*   **Gain:** Too low, and faint signals are lost in the noise. Too high, and strong signals can be distorted (saturation). Often requires experimentation or using 'auto' if available.\r\n*   **Antenna:** The antenna matters! Ensure it's appropriate for the frequency band you're tuning to.\r\n*   **Permissions (Linux):** You might need to add a udev rule or run with `sudo` to access the SDR device, although this is generally discouraged for security reasons if not necessary.\r\n\r\n### 2.4 Signal Analysis with SDRs: Making Sense of IQ Data\r\n\r\nRaw IQ data isn't very intuitive. We need ways to visualize and extract meaningful information. The most common tools are spectrum plots and spectrograms.\r\n\r\n**Spectrum Analysis (FFT):**\r\n\r\nThe Fast Fourier Transform (FFT) is our key tool here. It transforms the signal from the time domain (IQ samples over time) to the frequency domain (amplitude/power at different frequencies).\r\n\r\n```python\r\n# Assuming 'samples' variable contains the captured IQ data from the previous step\r\n\r\nif 'samples' in locals() and len(samples) > 0:\r\n    # --- Spectrum Analysis (FFT) ---\r\n    # Calculate the power spectral density (PSD)\r\n    # NFFT is the number of points used in each FFT block.\r\n    # A power of 2 is efficient. num_samples is often a power of 2.\r\n    NFFT = len(samples)\r\n    # Ensure NFFT is a power of 2 for optimal performance, though not strictly required by np.fft\r\n    # NFFT = 2**int(np.ceil(np.log2(len(samples)))) # Alternative if you need a specific NFFT\r\n\r\n    # Use Matplotlib's specgram for a quick PSD plot\r\n    # It handles windowing, overlapping, and FFT calculation\r\n    plt.figure(figsize=(12, 6))\r\n    # The first argument is the signal (IQ samples)\r\n    # NFFT is the number of data points in each block for the FFT\r\n    # Fs is the sampling frequency\r\n    # Fc is the center frequency (used for the x-axis scale)\r\n    Pxx, freqs, bins, im = plt.specgram(samples, NFFT=NFFT, Fs=sample_rate, Fc=center_freq, noverlap=NFFT//2, cmap='viridis')\r\n\r\n    # Calculate the frequency axis correctly\r\n    # The FFT output covers frequencies from -sample_rate/2 to +sample_rate/2 relative to center_freq\r\n    freqs_fft = np.fft.fftshift(np.fft.fftfreq(NFFT, 1/sample_rate)) + center_freq\r\n\r\n    # Calculate the power spectrum manually (optional, specgram does it)\r\n    # power_spectrum = np.abs(np.fft.fftshift(np.fft.fft(samples)))**2\r\n    # plt.figure(figsize=(12, 6))\r\n    # plt.plot(freqs_fft/1e6, 10 * np.log10(power_spectrum)) # Plot in dB\r\n    # plt.xlabel(\"Frequency (MHz)\")\r\n    # plt.ylabel(\"Power (dB)\")\r\n    # plt.title(\"Power Spectrum\")\r\n    # plt.grid(True)\r\n    # plt.show()\r\n\r\n\r\n    # Plotting the result from specgram (it's primarily for spectrogram, but returns PSD data too)\r\n    # Let's just use a simple plot of the average spectrum if specgram output isn't clear for single spectrum\r\n    # A more direct way:\r\n    spectrum = np.fft.fftshift(np.fft.fft(samples))\r\n    power_spectrum_db = 10 * np.log10(np.abs(spectrum)**2)\r\n\r\n    plt.figure(figsize=(12, 6))\r\n    plt.plot(freqs_fft/1e6, power_spectrum_db)\r\n    plt.xlabel(\"Frequency (MHz)\")\r\n    plt.ylabel(\"Power (dB)\")\r\n    plt.title(f\"Power Spectrum around {center_freq/1e6} MHz\")\r\n    plt.grid(True)\r\n    plt.show()\r\n\r\nelse:\r\n    print(\"No samples available for analysis. Please run the capture step first or load a file.\")\r\n\r\n```\r\n\r\n**Explanation:**\r\n\r\n1.  We use `np.fft.fft()` to perform the Fourier Transform on our `samples` array.\r\n2.  `np.fft.fftshift()` rearranges the output so that the 0 Hz component is in the center, matching the `center_freq` of our SDR tune. Frequencies range from `-sample_rate/2` to `+sample_rate/2` relative to the center.\r\n3.  `np.fft.fftfreq()` calculates the corresponding frequency bins for the FFT output.\r\n4.  We calculate the magnitude (absolute value) and often the power (`magnitude**2`) or power in dB (`10 * log10(power)`) for plotting.\r\n5.  The plot shows frequency on the x-axis and power/amplitude on the y-axis. Peaks indicate where signals are present.\r\n\r\n**Spectrogram (Waterfall Plot):**\r\n\r\nA spectrogram shows how the spectrum changes over time. It's a sequence of FFTs calculated on small, overlapping chunks of the data. This is invaluable for analyzing signals that change frequency or have intermittent activity.\r\n\r\n```python\r\n# Assuming 'samples' variable is available\r\n\r\nif 'samples' in locals() and len(samples) > 0:\r\n    # --- Spectrogram Analysis ---\r\n    # NFFT: size of the FFT window (determines frequency resolution)\r\n    # noverlap: number of samples overlapping between windows\r\n    # Fs: sampling frequency\r\n    # Fc: center frequency\r\n\r\n    NFFT_spectrogram = 1024 # Common FFT size for spectrograms\r\n    noverlap_spectrogram = NFFT_spectrogram // 2 # 50% overlap is typical\r\n\r\n    plt.figure(figsize=(12, 8))\r\n    # The specgram function is ideal for this\r\n    Pxx, freqs, bins, im = plt.specgram(samples,\r\n                                        NFFT=NFFT_spectrogram,\r\n                                        Fs=sample_rate,\r\n                                        Fc=center_freq,\r\n                                        noverlap=noverlap_spectrogram,\r\n                                        cmap='viridis', # Color map (e.g., 'plasma', 'inferno', 'cividis')\r\n                                        xextent=(0, len(samples)/sample_rate)) # Show time on x-axis\r\n\r\n    plt.xlabel(\"Time (s)\")\r\n    plt.ylabel(\"Frequency (MHz)\")\r\n    plt.title(f\"Spectrogram around {center_freq/1e6} MHz\")\r\n    plt.colorbar(label=\"Power (dB)\") # Add a color bar to show power scale\r\n    plt.ylim(center_freq/1e6 - sample_rate/2e6, center_freq/1e6 + sample_rate/2e6) # Set frequency limits\r\n    plt.show()\r\n\r\nelse:\r\n     print(\"No samples available for spectrogram analysis.\")\r\n\r\n```\r\n\r\n**Explanation:**\r\n\r\n1.  We use `matplotlib.pyplot.specgram()`. This function is designed for spectrograms.\r\n2.  `NFFT` now determines the frequency resolution (number of points in each mini-FFT). A larger `NFFT` gives better frequency resolution but poorer time resolution (wider windows).\r\n3.  `noverlap` controls how much the windows overlap. Overlapping helps capture transient signals and makes the spectrogram smoother.\r\n4.  The x-axis typically represents time, the y-axis represents frequency (relative to the center frequency), and the color intensity represents the power at that time/frequency point.\r\n5.  This plot is crucial for seeing how signals appear and disappear, change frequency (like FM), or have complex time-frequency characteristics.\r\n\r\n**Other Analysis Techniques:**\r\n\r\n*   **Time-domain plots:** Plotting I and Q over time (as shown in 2.2) or calculating instantaneous amplitude/phase can reveal modulation patterns.\r\n*   **Constellation Plots:** Plotting `Q` vs `I` for modulated signals shows clusters of points corresponding to different symbols (e.g., PSK, QAM). This is less useful for broadband signals but vital for digital modulation analysis.\r\n\r\n**Exercise:**\r\n\r\n1.  Capture data from a known signal source near you (e.g., a strong local FM radio station, or if you have a simple FSK transmitter like a wireless doorbell or weather station sensor, capture that).\r\n2.  Use the spectrum plot to find the exact center frequency and bandwidth of the signal.\r\n3.  Use the spectrogram to observe how the signal changes over time. Can you see the characteristics of the modulation? (e.g., FM will show frequency deviations).\r\n\r\n### 2.5 Basic Signal Generation: Putting Bits to Air\r\n\r\nNow for the exciting part: creating our *own* RF signals using code and transmitting them. This requires a TX-capable SDR like the HackRF One, LimeSDR, or ADALM-PLUTO.\r\n\r\n**Prerequisites:**\r\n\r\n*   A TX-capable SDR plugged in.\r\n*   The corresponding Python library installed (e.g., `pyhackrf`).\r\n*   **A controlled environment (e.g., using attenuators, connecting TX directly to RX with a cable and attenuator, or operating in a shielded room) and confirmation you are operating within legal power limits and frequency bands.** **Seriously, don't cause interference!**\r\n\r\nLet's generate a simple, continuous tone slightly offset from our center frequency.\r\n\r\n```python\r\nfrom hackrf import HackRF\r\nimport numpy as np\r\nimport time\r\n\r\n# --- Configuration ---\r\ncenter_freq = 433.92e6 # A common ISM band frequency (check local regulations!)\r\nsample_rate = 2.0e6    # Sample rate for TX\r\ntx_gain = 0            # Transmit gain (0-47 dB for HackRF)\r\namplitude = 0.5        # Amplitude of the generated signal (affects output power)\r\nduration = 5           # Seconds to transmit\r\n\r\n# --- Signal Generation (Creating IQ data) ---\r\n# Generate a tone at 50 kHz above the center frequency\r\ntone_freq_offset = 50e3 # 50 kHz offset\r\n\r\n# Number of samples needed\r\nnum_samples = int(duration * sample_rate)\r\n\r\n# Generate the time vector\r\nt = np.arange(num_samples) / sample_rate\r\n\r\n# Create the complex tone waveform: amplitude * exp(j * 2 * pi * f * t)\r\n# Ensure the maximum value is within the SDR's digital range (-1 to 1 for IQ components)\r\n# The HackRF expects float32 IQ pairs\r\niq_data_float32 = (amplitude * np.exp(1j * 2 * np.pi * tone_freq_offset * t)).astype(np.complex64)\r\n\r\nprint(f\"Generated {len(iq_data_float32)} samples for transmission.\")\r\n\r\n# --- SDR Setup and Transmission ---\r\nsdr = None\r\ntry:\r\n    sdr = HackRF()\r\n\r\n    print(f\"Configuring SDR for TX:\")\r\n    print(f\"  Center Freq: {center_freq/1e6} MHz\")\r\n    print(f\"  Sample Rate: {sample_rate/1e6} MSps\")\r\n    print(f\"  TX Gain: {tx_gain} dB\")\r\n    print(f\"  Amplitude: {amplitude}\")\r\n    print(f\"  Duration: {duration} seconds\")\r\n\r\n    sdr.sample_rate = sample_rate\r\n    sdr.center_freq = center_freq\r\n    sdr.tx_gain = tx_gain # Use tx_gain for transmit power\r\n\r\n    # --- Transmission ---\r\n    print(\"Starting transmission...\")\r\n    # transmit() takes the IQ data (NumPy array)\r\n    # It's often blocking for the duration of the data provided\r\n    sdr.transmit(iq_data_float32, repeat=False) # Set repeat=True for continuous loop\r\n\r\n    print(\"Transmission finished.\")\r\n\r\nexcept Exception as e:\r\n    print(f\"Error during SDR transmission: {e}\")\r\n    print(\"Ensure your TX-capable SDR is plugged in and drivers are installed correctly.\")\r\n    print(\"Check permissions if necessary.\")\r\n    print(\"Are you sure your SDR supports TX (e.g., HackRF, not RTL-SDR)?\")\r\n\r\nfinally:\r\n    # Always stop and close the SDR connection\r\n    if sdr:\r\n        sdr.stop_tx() # Explicitly stop transmission\r\n        sdr.close()\r\n        print(\"SDR stopped and closed.\")\r\n\r\n```\r\n\r\n**Explanation:**\r\n\r\n1.  We import the necessary library (`HackRF`).\r\n2."
    },
    {
      "title": "module_3",
      "description": "module_3 Overview",
      "order": 3,
      "content": "Alright team! Welcome back. We've set the stage, got our SDRs humming, and captured some raw RF goodness. Now, it's time to bridge the gap between that raw data and the intelligent systems that try to make sense of it. This is where Artificial Intelligence steps into our RF world, and frankly, it's where some fascinating vulnerabilities start to emerge.\r\n\r\nIn **Module 3: AI/ML for Spectrum: Classification & Feature Engineering**, we're going to understand *how* those messy RF signals get transformed into something an AI can chew on, and we'll build our very first simple AI model designed specifically to classify these signals. This isn't just theory; the model we build here is the very *target* system we'll be attacking later. So, let's make it real!\r\n\r\n---\r\n\r\n## **Module 3: AI/ML for Spectrum: Classification & Feature Engineering**\r\n\r\n*   **Module Objective:** Understand how RF signals are processed and represented for Artificial Intelligence models, and learn the fundamentals of training a basic AI model for signal classification. By the end, you'll have built the core AI component that our adversarial attacks will target.\r\n\r\n### **Introduction: Why AI Needs Data in a Specific Format**\r\n\r\nThink about how *you* recognize things. You don't process raw light waves hitting your retina as complex electromagnetic field equations. Your brain processes them into shapes, colors, patterns. Similarly, AI models, especially neural networks, need data presented in a structured, understandable format.\r\n\r\nRF signals, in their raw form (like the IQ data we captured in Module 2), are sequences of complex numbers changing over time. While powerful, this format isn't always the most intuitive for standard AI architectures designed for things like images or text. This module is all about turning that RF complexity into AI-friendly inputs.\r\n\r\n### **Essential Subtopics Deep Dive:**\r\n\r\n#### **1. Review of Relevant ML Concepts**\r\n\r\nBefore we apply ML to RF, let's quickly recap the core ideas we'll rely on:\r\n\r\n*   **Supervised Learning:** This is our primary mode. We have input data (our RF signals) and corresponding correct outputs (the signal type, e.g., \"WiFi\", \"LTE\", \"FM\"). The AI learns a mapping from input to output based on this labeled data.\r\n*   **Classification:** Our specific task. We want the AI to assign an input RF signal to one of several predefined categories or classes (the signal types).\r\n*   **Training Data:** The set of input-output pairs used to *teach* the model. The model adjusts its internal parameters to minimize errors on this data.\r\n*   **Labels:** The correct output category associated with each piece of training data.\r\n*   **Features:** The specific, quantifiable pieces of information extracted from the raw data that the AI model uses to make its decision. This is a critical step in RF.\r\n*   **Models:** The mathematical structure (e.g., a neural network) that learns the relationship between features and labels.\r\n\r\n#### **2. Representing RF Data for ML**\r\n\r\nThis is arguably the most crucial step. How do we take our IQ data and make it digestible for an AI? Several common approaches exist:\r\n\r\n*   **a) Raw IQ Data:**\r\n    *   **Concept:** Feed the sequence of complex IQ samples directly into the model.\r\n    *   **Pros:** Preserves all information, no manual feature engineering needed initially.\r\n    *   **Cons:** High dimensionality (each sample is complex, often need many samples for a window), sensitive to timing shifts, requires models good with sequences (like LSTMs). Can be computationally intensive.\r\n    *   **Example Data Shape:** A sequence of `N` complex samples might be represented as a tensor of shape `(N, 2)` for real and imaginary parts, or `(N,)` for complex numbers if the framework supports it.\r\n\r\n*   **b) Time-Series Features:**\r\n    *   **Concept:** Calculate statistical features over a window of IQ data.\r\n    *   **Pros:** Reduces dimensionality significantly, relatively easy to compute.\r\n    *   **Cons:** Loses fine-grained temporal structure, potentially discarding discriminative information.\r\n    *   **Examples:** Mean amplitude, variance, skewness, kurtosis, peak-to-average power ratio (PAPR), zero-crossing rate, etc. Calculated over segments of the IQ stream.\r\n\r\n*   **c) Frequency-Domain Features:**\r\n    *   **Concept:** Apply a Fast Fourier Transform (FFT) to convert the time-domain IQ data into the frequency domain. Calculate features from the resulting spectrum.\r\n    *   **Pros:** Robust to time shifts, highlights spectral characteristics (bandwidth, center frequency, presence of carriers/harmonics).\r\n    *   **Cons:** Loses temporal information within the window.\r\n    *   **Examples:** Peak frequency, occupied bandwidth, spectral flatness, spectral entropy. Often calculated from the Power Spectral Density (PSD), which is the magnitude-squared of the FFT.\r\n\r\n*   **d) Time-Frequency Representations (Spectrograms):**\r\n    *   **Concept:** Apply the Short-Time Fourier Transform (STFT) repeatedly over overlapping windows of IQ data. This generates a 2D representation showing how the frequency content of the signal changes over time. This is a spectrogram!\r\n    *   **Pros:** Captures both time and frequency characteristics, visually intuitive, can leverage powerful image-based AI models (like CNNs). Very popular in RF signal classification.\r\n    *   **Cons:** Requires careful tuning of STFT parameters (window size, overlap), loses some phase information (though magnitude is usually sufficient).\r\n    *   **Example Data Shape:** A spectrogram is essentially an image. If you generate a spectrogram with `T` time bins and `F` frequency bins, the shape is `(T, F)`. If you treat it as a single channel image, it's `(T, F, 1)`.\r\n\r\n    *   **Let's focus on Spectrograms for our project.** They are a widely used representation that works well with common neural network architectures (CNNs), making the ML part more straightforward for this course's scope.\r\n\r\n    ```python\r\n    import numpy as np\r\n    import matplotlib.pyplot as plt\r\n    from scipy.signal import spectrogram\r\n    from scipy.io import loadmat # Often used for RF datasets\r\n\r\n    # --- Example: Generating a Spectrogram ---\r\n\r\n    # Assume 'iq_data' is a numpy array of complex numbers (from Module 2 capture)\r\n    # Example: Create some dummy IQ data (a simple tone plus noise)\r\n    sample_rate = 1e6 # 1 MHz\r\n    duration = 0.01   # 10 ms\r\n    t = np.linspace(0, duration, int(sample_rate * duration), endpoint=False)\r\n    # Tone at 100 kHz\r\n    tone_freq = 100e3\r\n    iq_data = 0.5 * np.exp(2j * np.pi * tone_freq * t)\r\n    # Add some noise\r\n    iq_data += np.random.randn(len(iq_data)) + 1j * np.random.randn(len(iq_data))\r\n\r\n    # Spectrogram parameters (need tuning based on signal type and desired resolution)\r\n    nperseg = 256   # Samples per segment (FFT size)\r\n    noverlap = nperseg // 2 # Overlap between segments\r\n\r\n    # Generate the spectrogram\r\n    # fs = sample rate, noverlap = overlap, nperseg = segment length\r\n    f, t_spec, Sxx = spectrogram(iq_data, fs=sample_rate, noverlap=noverlap, nperseg=nperseg)\r\n\r\n    # Sxx is the power spectral density (magnitude squared).\r\n    # Often, we use log scale for better visualization and sometimes for ML input\r\n    Sxx_log = 10 * np.log10(Sxx + 1e-10) # Add small value to avoid log(0)\r\n\r\n    # Visualize the spectrogram\r\n    plt.figure(figsize=(10, 6))\r\n    plt.pcolormesh(t_spec, f, Sxx_log, shading='gouraud')\r\n    plt.ylabel('Frequency [Hz]')\r\n    plt.xlabel('Time [sec]')\r\n    plt.title('Spectrogram of IQ Data')\r\n    plt.colorbar(label='Power/Frequency [dB/Hz]')\r\n    plt.show()\r\n\r\n    print(f\"Raw IQ data shape: {iq_data.shape}\")\r\n    print(f\"Spectrogram shape (freq bins, time bins): {Sxx.shape}\")\r\n    ```\r\n    *   **Note:** The output `Sxx` has shape `(n_frequencies, n_times)`. For a CNN input (like an image), we might reshape it to `(n_times, n_frequencies, 1)` to represent it as a single-channel image, or just use the `(n_times, n_frequencies)` shape if the framework supports it.\r\n\r\n#### **3. Selecting and Engineering Features for RF Signals**\r\n\r\nIf we weren't using Spectrograms, we'd spend time here deciding which statistical or frequency-domain features are most discriminative for the signal types we care about. This often involves domain knowledge about the signals (e.g., FM has constant amplitude, PSK has constant envelope but phase changes).\r\n\r\nSince we're focusing on Spectrograms for the project, our \"feature engineering\" is primarily the *process* of generating the spectrograms themselves and deciding on parameters like `nperseg` and `noverlap`. These parameters affect the time-frequency resolution trade-off.\r\n\r\n#### **4. Introduction to Neural Networks suitable for RF**\r\n\r\n*   **Convolutional Neural Networks (CNNs):** These are powerhouses for image recognition and are excellent for processing grid-like data like Spectrograms.\r\n    *   **Why they work:** They use convolutional layers to automatically learn hierarchical features (edges, patterns, textures) from the input data, regardless of where they appear spatially. In a spectrogram, this means learning to recognize specific frequency patterns or time-varying structures characteristic of a modulation type.\r\n    *   **Architecture:** Typically involve convolutional layers, pooling layers (to reduce dimensionality and provide translation invariance), and dense (fully connected) layers at the end for classification.\r\n\r\n*   **Recurrent Neural Networks (RNNs), particularly LSTMs (Long Short-Term Memory):** These are designed for sequential data.\r\n    *   **Why they work:** They have internal memory that allows them to process sequences piece by piece, making them suitable for raw IQ data streams where temporal order is important.\r\n    *   **Architecture:** Often involve LSTM layers followed by dense layers.\r\n\r\n    *   **For our project, we will use a simple CNN, as Spectrograms are our chosen representation.**\r\n\r\n#### **5. Using ML Frameworks: TensorFlow or PyTorch basics**\r\n\r\nThese frameworks provide the tools to build, train, and evaluate neural networks efficiently, often leveraging GPU acceleration. We'll use TensorFlow with its Keras API for simplicity in our examples.\r\n\r\nBasic steps in Keras:\r\n\r\n1.  **Load and Preprocess Data:** Get your Spectrograms (or other features) and labels into NumPy arrays or a framework-specific data format. Ensure shapes are correct. Split into training and testing sets.\r\n2.  **Define the Model:** Stack layers (e.g., `Conv2D`, `MaxPooling2D`, `Flatten`, `Dense`).\r\n3.  **Compile the Model:** Specify the optimizer (how the model learns), the loss function (how errors are measured), and metrics (what to track during training, like accuracy).\r\n4.  **Train the Model:** Feed the training data to the model (`model.fit()`).\r\n5.  **Evaluate the Model:** Check performance on unseen test data (`model.evaluate()`).\r\n6.  **Make Predictions:** Use the trained model to classify new data (`model.predict()`).\r\n\r\n```python\r\n# --- Example: Basic CNN using TensorFlow/Keras ---\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.preprocessing import LabelBinarizer # To one-hot encode labels\r\n\r\n# Assume you have:\r\n# spectrograms_data: numpy array of spectrograms, shape (num_samples, height, width)\r\n# labels_data: numpy array of strings or integers, shape (num_samples,)\r\n# num_classes: integer, number of distinct signal types\r\n\r\n# --- Dummy Data Generation (Replace with your actual data loading) ---\r\n# Let's simulate having 100 samples of 3 classes, spectrogram size 128x128\r\nnum_samples = 300\r\nheight, width = 128, 128\r\nnum_classes = 3\r\nclass_names = ['FM', 'AM', 'Noise'] # Example class names\r\n\r\nspectrograms_data = np.random.rand(num_samples, height, width).astype(np.float32) * 100 # Simulate some data\r\nlabels_data = np.random.choice(class_names, num_samples) # Simulate labels\r\n\r\n# Add a channel dimension (required for Conv2D)\r\n# Input shape for CNN is (height, width, channels)\r\nspectrograms_data = np.expand_dims(spectrograms_data, axis=-1) # Shape becomes (num_samples, height, width, 1)\r\n\r\n# Convert labels to one-hot encoding (e.g., 'FM' -> [1, 0, 0])\r\nlabel_binarizer = LabelBinarizer()\r\nlabels_one_hot = label_binarizer.fit_transform(labels_data)\r\n\r\n# Split data into training and testing sets\r\nX_train, X_test, y_train, y_test = train_test_split(\r\n    spectrograms_data, labels_one_hot, test_size=0.2, random_state=42, stratify=labels_one_hot\r\n)\r\n\r\nprint(f\"Training data shape: {X_train.shape}\")\r\nprint(f\"Testing data shape: {X_test.shape}\")\r\nprint(f\"Training labels shape: {y_train.shape}\")\r\nprint(f\"Testing labels shape: {y_test.shape}\")\r\n\r\n# --- Define the CNN Model ---\r\nmodel = Sequential([\r\n    Conv2D(32, (3, 3), activation='relu', input_shape=(height, width, 1)),\r\n    MaxPooling2D((2, 2)),\r\n    Conv2D(64, (3, 3), activation='relu'),\r\n    MaxPooling2D((2, 2)),\r\n    Conv2D(128, (3, 3), activation='relu'),\r\n    MaxPooling2D((2, 2)),\r\n    Flatten(), # Flatten the 3D output to 1D\r\n    Dense(128, activation='relu'),\r\n    Dropout(0.5), # Helps prevent overfitting\r\n    Dense(num_classes, activation='softmax') # Output layer with softmax for classification\r\n])\r\n\r\n# --- Compile the Model ---\r\nmodel.compile(optimizer='adam',\r\n              loss='categorical_crossentropy', # Suitable for multi-class classification with one-hot labels\r\n              metrics=['accuracy'])\r\n\r\nmodel.summary()\r\n\r\n# --- Train the Model ---\r\n# Use a small number of epochs for demonstration\r\nepochs = 10\r\nbatch_size = 32\r\n\r\nprint(\"\\nStarting model training...\")\r\nhistory = model.fit(X_train, y_train,\r\n                    epochs=epochs,\r\n                    batch_size=batch_size,\r\n                    validation_split=0.2, # Use a portion of training data for validation during training\r\n                    verbose=1) # Show training progress\r\n\r\nprint(\"Training finished.\")\r\n\r\n# --- Evaluate the Model ---\r\nprint(\"\\nEvaluating model on test data...\")\r\nloss, accuracy = model.evaluate(X_test, y_test, verbose=0)\r\n\r\nprint(f\"Test Loss: {loss:.4f}\")\r\nprint(f\"Test Accuracy: {accuracy:.4f}\")\r\n\r\n# --- Save the Model (for use in Module 4) ---\r\nmodel_save_path = \"rf_classifier_model.h5\" # Keras HDF5 format\r\nmodel.save(model_save_path)\r\nprint(f\"Model saved to {model_save_path}\")\r\n\r\n# To load the model later:\r\n# loaded_model = tf.keras.models.load_model(model_save_path)\r\n```\r\n\r\n#### **6. Evaluating Classifier Performance**\r\n\r\nAccuracy is the most common metric, but it can be misleading, especially with imbalanced datasets. A classifier that always predicts the majority class might have high accuracy but be useless.\r\n\r\n*   **Accuracy:** (Number of correct predictions) / (Total number of predictions). Simple, but potentially misleading.\r\n*   **Confusion Matrix:** A table showing the counts of True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN) for each class.\r\n    *   **TP:** Predicted class X, actual class X.\r\n    *   **FP:** Predicted class X, actual class Y (Type I error).\r\n    *   **FN:** Predicted class Y, actual class X (Type II error).\r\n    *   **TN:** Predicted not class X, actual not class X.\r\n    *   From the confusion matrix, you can derive other metrics like Precision, Recall (Sensitivity), F1-score, and Specificity, which give a more nuanced view of performance, especially per class.\r\n\r\n    ```python\r\n    # --- Example: Confusion Matrix using scikit-learn ---\r\n    from sklearn.metrics import confusion_matrix, classification_report\r\n    import seaborn as sns # For pretty plots\r\n\r\n    # Assume you have the trained 'model' and test data X_test, y_test from the previous example\r\n\r\n    # Get predictions for the test set\r\n    y_pred_one_hot = model.predict(X_test)\r\n    y_pred_labels = label_binarizer.inverse_transform(y_pred_one_hot) # Convert one-hot back to original labels\r\n    y_true_labels = label_binarizer.inverse_transform(y_test)\r\n\r\n    # Generate the confusion matrix\r\n    cm = confusion_matrix(y_true_labels, y_pred_labels, labels=class_names) # Specify labels to ensure order\r\n\r\n    print(\"\\nConfusion Matrix:\")\r\n    print(cm)\r\n\r\n    # Visualize the confusion matrix (more intuitive)\r\n    plt.figure(figsize=(8, 6))\r\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\r\n    plt.xlabel('Predicted Label')\r\n    plt.ylabel('True Label')\r\n    plt.title('Confusion Matrix')\r\n    plt.show()\r\n\r\n    # Print a classification report (includes precision, recall, f1-score)\r\n    print(\"\\nClassification Report:\")\r\n    print(classification_report(y_true_labels, y_pred_labels, target_names=class_names))\r\n    ```\r\n\r\n#### **7. Case Study: How AI is used in military/intelligence signal identification (SIGINT)**\r\n\r\nThis is a huge and fascinating area where AI is rapidly being deployed.\r\n\r\n*   **Task:** Automatically monitor vast swaths of the RF spectrum.\r\n*   **Goal:**\r\n    *   Identify known communication signals (voice, data, radar, etc.).\r\n    *   Classify their modulation type, protocol, or even specific device.\r\n    *   Detect unknown or anomalous signals.\r\n    *   Geolocate signal sources.\r\n*   **Why AI?** The sheer volume and complexity of modern spectrum make manual analysis impossible. AI can process data at machine speed, identify subtle patterns, and adapt (if trained properly) to new signal types.\r\n*   **Vulnerability:** If an adversary knows (or can figure out) the AI models being used for classification, they can potentially craft signals that mimic friendly communications, evade detection, or appear as something benign, disrupting intelligence gathering or enabling deception. This is *exactly* the type of scenario our adversarial attacks explore.\r\n\r\n### **Module Project/Exercise:**\r\n\r\nAlright, let's build the heart of our target system!\r\n\r\n1.  **Obtain RF Signal Data:**\r\n    *   **Option A (Recommended for repeatability):** Use a publicly available, pre-labeled dataset like the [RML2016.10a dataset](https://www.deepsig.io/datasets). This dataset contains IQ samples for various modulation types (8PSK, AM, FM, GFSK, etc.) at different Signal-to-Noise Ratios (SNRs). It's structured specifically for ML tasks. You'll need to download it.\r\n    *   **Option B (More challenging, but uses your own data):** Use the data you captured in Module 2. *Challenge:* This data is likely unlabeled noise or unknown signals. You would need to generate a few known signals (e.g., using GNU Radio or another SDR) and capture them, carefully labeling each capture. This is harder to make repeatable and get enough diverse data for training.\r\n    *   *Let's proceed assuming you'll use a structured dataset like RML2016.10a for the code examples.*\r\n\r\n2.  **Process Data into Spectrograms:**\r\n    *   Load the IQ data for a selected subset of modulation types (start with 2-3 distinct ones, e.g., FM, AM, BPSK) at a reasonable SNR (e.g., 0dB or higher).\r\n    *   For each IQ sample (which is a time series), compute its spectrogram using `scipy.signal.spectrogram`. Experiment with `nperseg` and `noverlap` parameters. A common approach is to make the spectrogram image size reasonable (e.g., 64x64, 128x128). Remember to use the magnitude or log-magnitude.\r\n    *   Store the resulting spectrograms and their corresponding labels.\r\n\r\n3.  **Create a Labeled Dataset for ML:**\r\n    *   Organize your processed spectrograms and their labels into NumPy arrays.\r\n    *   Ensure your labels are consistently represented (e.g., strings like 'FM', 'AM', 'BPSK').\r\n    *   Split your dataset into training and testing sets using `train_test_split` from `sklearn.model_selection`.\r\n\r\n4.  **Train a Simple Neural Network Classifier:**\r\n    *   Define a simple CNN model using TensorFlow/Keras, similar to the example code provided above. Adjust the input shape to match your spectrogram dimensions.\r\n    *   Compile the model using `adam` optimizer and `categorical_crossentropy` loss.\r\n    *   Train the model on your training data for a sufficient number of epochs (start with 10-20, monitor validation accuracy).\r\n    *   *Computational Note:* Training can take time depending on dataset size and your hardware. Start with a small dataset subset if needed.\r\n\r\n5.  **Evaluate the Trained Model:**\r\n    *   Evaluate the model's performance on the *unseen* test set using `model.evaluate`. Note the test accuracy.\r\n    *   Generate and display the confusion matrix using `sklearn.metrics.confusion_matrix` and `seaborn.heatmap`. Analyze which signal types are being confused.\r\n\r\n**Code Sketch for Project Steps 1-3 (using a simplified RML-like structure):**\r\n\r\n```python\r\nimport numpy as np\r\nfrom scipy.signal import spectrogram\r\nfrom scipy.io import loadmat\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.preprocessing import LabelBinarizer\r\n\r\n# --- Project Step 1 & 2: Load Data and Generate Spectrograms ---\r\n# Assuming you downloaded RML2016.10a.tar.gz and extracted it.\r\n# The .mat file structure is usually a dictionary.\r\n# Data is typically indexed by SNR and Modulation Type.\r\n\r\n# Example: Load a small subset of data (e.g., 3 modulation types at 0dB SNR)\r\n# Note: The actual RML dataset structure might require adaptation of loading code.\r\n# This is a conceptual example. A helper function is often used for RML loading.\r\n\r\n# FAKE DATA STRUCTURE FOR DEMO - REPLACE WITH REAL LOADING\r\n# In a real scenario, you'd load the .mat file and iterate through its structure\r\n# to get IQ samples and labels.\r\n# Example structure: data['SNR']['ModType'] = np.array([...IQ samples...])\r\nfake_data = {}\r\nfake_data[0] = {} # SNR 0dB\r\nfake_data[0]['8PSK'] = np.random.randn(100, 128) + 1j*np.random.randn(100, 128) # 100 samples, 128 points each\r\nfake_data[0]['FM'] = np.random.randn(100, 128) + 1j*np.random.randn(100, 128)\r\nfake_data[0]['WFM'] = np.random.randn(100, 128) + 1j*np.random.randn(100, 128)\r\n\r\n# --- Real RML data loading is more involved ---\r\n# You'd typically load the .mat file:\r\n# data = loadmat('RML2016.10a.mat')['RML2016_10a'] # Adjust key based on file structure\r\n# Then iterate through SNRs and modulations:\r\n# classes = ['8PSK', 'AM', 'CPFSK', ...]\r\n# snrs = [-20, -18, ..., 18]\r\n# X = []\r\n# Y = []\r\n# for snr in snrs:\r\n#     for mod in classes:\r\n#         iq_samples_list = data[mod][snr] # Get list/array of IQ sequences for this mod/snr\r\n#         for iq_seq in iq_samples_list:\r\n#             X.append(iq_seq)\r\n#             Y.append(mod)\r\n# X = np.array(X) # Shape (num_samples, iq_points)\r\n# Y = np.array(Y) # Shape (num_samples,)\r\n# --- End Real RML loading sketch ---\r\n\r\n\r\n# --- Let's use the fake data for the spectrogram part ---\r\nall_spectrograms = []\r\nall_labels = []\r\nspectrogram_height = 64 # Desired height of spectrogram image\r\nspectrogram_width = 64  # Desired width of spectrogram image\r\n\r\n# Spectrogram parameters - adjust these!\r\nnperseg = 32 # Affects frequency resolution\r\nnoverlap = nperseg // 2 # Affects time resolution\r\n\r\nprint(\"Generating spectrograms...\")\r\nfor snr, mods in fake_data.items(): # Iterate through SNR levels\r\n    for mod_type, iq_samples_array in mods.items(): # Iterate through modulation types\r\n        print(f\"  Processing {mod_type} at {snr} dB SNR...\")\r\n        for iq_sample in iq_samples_array: # Iterate through each IQ sequence\r\n            # Calculate spectrogram\r\n            # fs=sample_rate (need to know dataset's sample rate, let's assume 1.0 for simplicity here)\r\n            # Sxx will have shape (n_freq_bins, n_time_bins)\r\n            f, t_spec, Sxx = spectrogram(iq_sample, fs=1.0, noverlap=noverlap, nperseg=nperseg)\r\n\r\n            # Convert to log scale and resize/crop if needed to get consistent dimensions\r\n            Sxx_log = 10 * np.log10(Sxx + 1e-10)\r\n\r\n            # --- Resizing/Cropping (Important for consistent input size) ---\r\n            # This is a common challenge. Spectrogram output dimensions depend on input length\r\n            # and STFT parameters. We need a fixed size for CNNs.\r\n            # A simple approach is cropping or padding. Let's crop for this example.\r\n            # Ensure spectrogram_height <= Sxx_log.shape[0] and spectrogram_width <= Sxx_log.shape[1]\r\n            # In a real scenario, you'd handle varying input IQ lengths.\r\n            # For standard datasets like RML, IQ lengths are fixed, so Sxx dimensions will be consistent.\r\n            # Let's assume Sxx_log is already large enough or we resize properly.\r\n            # For this demo, let's just take a slice if it's too big,"
    },
    {
      "title": "module_4",
      "description": "module_4 Overview",
      "order": 4,
      "content": "Okay, let's dive deep into Module 4: Building the Target: An AI-Driven RF Classifier System. This module is where we bring together the SDR skills from Module 2 and the AI/ML knowledge from Module 3 to create the core system that we will later attack. Think of this as building the 'lock' before we learn to pick it in subsequent modules.\r\n\r\n---\r\n\r\n## **Module 4: Building the Target: An AI-Driven RF Classifier System**\r\n\r\n**Module Objective:** Integrate SDR data capture with the trained AI model to build a functional, albeit basic, real-time or near-real-time RF signal classification system. This system will serve as the primary target for adversarial attacks.\r\n\r\n**Contribution to Capstone:** Completes the *target* system component of the functional clone. By the end of this module, you will have a running system that listens to RF, processes it, and attempts to identify signals using your previously trained AI model.\r\n\r\n---\r\n\r\n### **Introduction: From Digital Dream to RF Reality**\r\n\r\nIn Module 3, you trained an AI model to classify RF signals based on processed data (like spectrograms or IQ features). That model was trained on static datasets. Now, we face the challenge of making that model useful in a dynamic RF environment. How do we feed live data from an SDR into our trained model? How do we handle the continuous stream of RF energy? This module is about building the bridge between the digital realm of your trained model and the physical world of radio waves captured by your SDR.\r\n\r\nOur goal isn't necessarily a production-ready, perfectly optimized real-time system. The goal is a *functional clone* – a system that works well enough in a controlled environment that we can reliably test adversarial attacks against it. This system will be our primary target in Modules 6 and 7.\r\n\r\n### **4.1 Connecting SDR Capture Pipeline to ML Inference Pipeline**\r\n\r\nThis is the core architectural challenge. We have two distinct processes:\r\n1.  **SDR Capture:** Reading raw IQ data from the SDR hardware.\r\n2.  **ML Inference:** Taking processed data and feeding it through the trained AI model to get a classification.\r\n\r\nWe need a way to connect these two. The simplest approach is a sequential pipeline:\r\n\r\n```\r\nSDR -> Read Samples -> Process Samples -> Prepare ML Input -> Load Model -> Run Inference -> Get Prediction -> Output Result\r\n```\r\n\r\nThis process needs to happen repeatedly to classify a continuous signal or monitor a frequency band.\r\n\r\n**Data Flow:**\r\n\r\n*   The SDR captures a chunk of IQ samples over a specific duration.\r\n*   This raw IQ data needs to be transformed into the format your Module 3 model expects. If your model used spectrograms, you'll need to compute a spectrogram from the IQ chunk. If it used raw IQ or time-series features, you'll process accordingly.\r\n*   This processed data (e.g., a NumPy array representing a spectrogram image or IQ features) becomes the input to your loaded ML model.\r\n*   The model outputs probabilities for each class.\r\n*   You interpret these probabilities (usually by taking the class with the highest probability) and display the result.\r\n\r\n**Libraries Involved:**\r\n\r\n*   **SDR Interaction:** `pyrtlsdr`, `pyhackrf`, `pysdr`, or similar libraries depending on your SDR.\r\n*   **Numerical Processing:** `NumPy` for array manipulation, `SciPy` for signal processing functions (like FFT, windowing).\r\n*   **Spectrogram Generation (if needed):** `Matplotlib` (specifically its backend) or custom `NumPy`/`SciPy` code.\r\n*   **ML Framework:** `TensorFlow` or `PyTorch` for loading and running the model.\r\n\r\n### **4.2 Data Buffering and Processing for Continuous Classification**\r\n\r\nRF signals don't arrive in neat little packages perfectly sized for your model's input. They are continuous. Your SDR reads data in chunks (buffers).\r\n\r\n**Handling Chunks:**\r\n\r\n*   Your SDR library will have a function to read a specified number of samples (e.g., `sdr.read_samples(num_samples)`).\r\n*   The `num_samples` you read will depend on:\r\n    *   The sample rate of your SDR.\r\n    *   The duration of the signal segment you want to classify at a time.\r\n    *   The input size required by your ML model *after* processing.\r\n\r\n**Example: Spectrogram Input**\r\n\r\nIf your model takes a spectrogram of a certain size (e.g., 128x128 pixels) representing a 10ms signal segment, you need to calculate how many IQ samples correspond to 10ms at your SDR's sample rate.\r\n\r\n`Number of Samples = Duration (seconds) * Sample Rate (samples/second)`\r\n\r\nFor a 10ms duration and 2.048 Msps sample rate:\r\n`Number of Samples = 0.010 * 2,048,000 = 20,480 samples`\r\n\r\nYou would read 20,480 IQ samples. Then, you'd process these 20,480 samples to generate a spectrogram. This might involve:\r\n\r\n1.  Splitting the chunk into smaller overlapping segments (e.g., 256 samples with 50% overlap).\r\n2.  Applying a window function (e.g., Hann window) to each segment.\r\n3.  Performing an FFT on each windowed segment.\r\n4.  Taking the magnitude (power spectrum) of the FFT results.\r\n5.  Stacking these power spectra side-by-side to form the spectrogram image data.\r\n6.  Resizing or interpolating this data to match the 128x128 input size expected by your model.\r\n\r\n**Code Snippet (Illustrative: IQ Capture & Basic Spectrogram Generation Logic)**\r\n\r\n```python\r\nimport numpy as np\r\nfrom scipy.signal import spectrogram\r\nimport matplotlib.pyplot as plt\r\n# Assuming sdr object is initialized and configured (center_freq, sample_rate)\r\n# from Module 2 setup\r\n\r\n# --- Configuration ---\r\nsample_rate = 2.048e6 # Example sample rate (2.048 Msps)\r\ncenter_freq = 433e6   # Example center frequency (433 MHz ISM band)\r\nclassification_duration = 0.01 # Classify every 10 ms of signal\r\nchunk_size = int(sample_rate * classification_duration) # Number of samples per classification chunk\r\n\r\n# Spectrogram parameters (adjust based on desired resolution and model input size)\r\nnperseg = 256 # Samples per segment for FFT\r\nnoverlap = nperseg // 2 # 50% overlap\r\nnfft = nperseg # Number of FFT points\r\n\r\n# --- Capture and Processing Loop (Conceptual) ---\r\n# In a real system, this would run continuously or triggered\r\nprint(f\"Capturing {chunk_size} samples ({classification_duration*1000} ms) at {center_freq/1e6} MHz...\")\r\n\r\ntry:\r\n    # Capture a chunk of complex IQ data\r\n    # NOTE: The actual read function depends on your SDR library (pyrtlsdr, pyhackrf, etc.)\r\n    # Example using a hypothetical sdr.read_samples function:\r\n    # iq_samples = sdr.read_samples(chunk_size)\r\n\r\n    # For demonstration, let's create some dummy IQ data\r\n    t = np.arange(chunk_size) / sample_rate\r\n    # Simple tone + noise\r\n    iq_samples = 0.5 * np.exp(1j * 2 * np.pi * 100000 * t) + \\\r\n                 (np.random.randn(chunk_size) + 1j * np.random.randn(chunk_size)) * 0.1\r\n\r\n\r\n    # --- Processing Step: Generate Spectrogram ---\r\n    # f: Array of sample frequencies\r\n    # t_spec: Array of segment times\r\n    # Sxx: Spectrogram (power spectral density)\r\n    f, t_spec, Sxx = spectrogram(iq_samples, fs=sample_rate, nperseg=nperseg,\r\n                                 noverlap=noverlap, nfft=nfft, mode='psd')\r\n\r\n    # Convert power spectral density to dB for better visualization/ML input\r\n    # Add a small epsilon to avoid log(0)\r\n    Sxx_db = 10 * np.log10(Sxx + 1e-9)\r\n\r\n    # Sxx_db now represents your spectrogram data.\r\n    # Its shape will be (number_of_frequency_bins, number_of_time_segments)\r\n    # Example: (nfft/2 + 1, (chunk_size - noverlap) // (nperseg - noverlap))\r\n\r\n    print(f\"Generated spectrogram data with shape: {Sxx_db.shape}\")\r\n\r\n    # --- Further Processing for ML Input ---\r\n    # Your model expects a specific input shape (e.g., 128x128 image-like).\r\n    # You might need to resize, crop, or pad Sxx_db.\r\n    # You also need to normalize/scale the values (e.g., between 0 and 1 or -1 and 1)\r\n    # Example: Simple resizing (this is often lossy, training data processing should match)\r\n    from skimage.transform import resize # requires scikit-image installed\r\n    target_height, target_width = 128, 128 # Assuming your model expects 128x128\r\n    Sxx_resized = resize(Sxx_db, (target_height, target_width), anti_aliasing=True)\r\n\r\n    # Normalize (e.g., min-max scaling)\r\n    Sxx_normalized = (Sxx_resized - Sxx_resized.min()) / (Sxx_resized.max() - Sxx_resized.min())\r\n\r\n    # Your model might expect a channel dimension (e.g., for CNNs)\r\n    # If grayscale spectrogram, add a channel dimension: (height, width, 1)\r\n    ml_input = np.expand_dims(Sxx_normalized, axis=-1)\r\n\r\n    # Add batch dimension (model expects input shape like (batch_size, height, width, channels))\r\n    ml_input = np.expand_dims(ml_input, axis=0) # Batch size 1\r\n\r\n    print(f\"Prepared ML input data with shape: {ml_input.shape}\")\r\n\r\n    # --- Visualization (Optional, for debugging/display) ---\r\n    plt.figure(figsize=(10, 4))\r\n    plt.imshow(Sxx_db, aspect='auto', origin='lower',\r\n               extent=[t_spec[0], t_spec[-1], f[0]/1e6, f[-1]/1e6],\r\n               cmap='viridis')\r\n    plt.colorbar(label='Power/Frequency (dB/Hz)')\r\n    plt.xlabel('Time (s)')\r\n    plt.ylabel('Frequency (MHz)')\r\n    plt.title('Spectrogram of Captured RF Data Chunk')\r\n    plt.show()\r\n\r\nexcept Exception as e:\r\n    print(f\"An error occurred during capture or processing: {e}\")\r\n    # Handle SDR disconnection, permission issues, etc.\r\n\r\n# NOTE: In a real system, this capture and processing would be inside a loop.\r\n```\r\n\r\n**Teaching Tip:** Emphasize that the exact processing steps (windowing, FFT size, overlap, normalization, resizing) *must* match the processing steps used when creating the training data for the Module 3 model. Inconsistent processing will lead to poor performance.\r\n\r\n### **4.3 Handling Real-World RF Data Variability**\r\n\r\nThe clean, synthetic signals or carefully curated datasets used for training in M3 don't perfectly represent the chaos of the real RF environment.\r\n\r\n*   **Noise:** The noise floor varies. Your model was likely trained on data with a relatively consistent noise level. Real noise fluctuates.\r\n*   **Interference:** Other signals might bleed into your target frequency band.\r\n*   **Signal Strength:** The power of the signal you're trying to classify will change depending on distance, obstacles, antenna orientation, etc.\r\n*   **Timing:** Signals might start or stop mid-chunk.\r\n\r\n**Impact on Classification:** These variations can significantly degrade the performance of a model trained on clean data. A signal that was perfectly classifiable during training might look very different to the model when captured live with real-world noise and varying power levels.\r\n\r\n**Mitigation (for building a robust target, though not strictly required for the *basic* target):**\r\n\r\n*   **Normalization:** Ensure your processing pipeline includes robust normalization (e.g., min-max scaling, z-score normalization) that can handle variations in signal strength and noise floor. Apply the *same* normalization used during training.\r\n*   **Data Augmentation:** (This would ideally be done in M3 training) Train your model on data augmented with realistic noise, fading, and interference.\r\n*   **Thresholding:** You might implement logic to only attempt classification if the signal power in the chunk exceeds a certain threshold, avoiding classifying pure noise.\r\n\r\nFor the initial *target* system, focus on getting the pipeline working with *some* variability. We will exploit this variability and the model's potential lack of robustness in later modules.\r\n\r\n### **4.4 Loading Pre-trained Models for Inference**\r\n\r\nThis is straightforward using your chosen ML framework. You saved your trained model in Module 3. Now you load it.\r\n\r\n**Code Snippet (TensorFlow/Keras)**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n# --- Configuration ---\r\nmodel_path = 'path/to/your/saved_model' # Path where you saved your model in M3\r\n\r\n# --- Load Model ---\r\nprint(f\"Loading model from: {model_path}\")\r\ntry:\r\n    model = tf.keras.models.load_model(model_path)\r\n    print(\"Model loaded successfully.\")\r\n    model.summary() # Print model architecture to verify\r\nexcept Exception as e:\r\n    print(f\"Error loading model: {e}\")\r\n    print(\"Make sure the model path is correct and the model was saved correctly.\")\r\n    model = None # Ensure model is None if loading fails\r\n\r\n# You can now use this 'model' object to make predictions\r\n# model.predict(ml_input) # Assuming ml_input is prepared as in 4.2\r\n```\r\n\r\n**Code Snippet (PyTorch)**\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn # Needed if your model architecture is defined in a class\r\n\r\n# Assuming your model architecture class (e.g., SimpleCNN) is defined\r\n# from your Module 3 training code\r\n# from your_module3_code import SimpleCNN # Example import\r\n\r\n# --- Configuration ---\r\nmodel_path = 'path/to/your/saved_model.pth' # Path where you saved your model in M3\r\n\r\n# --- Load Model ---\r\nprint(f\"Loading model from: {model_path}\")\r\ntry:\r\n    # If you saved the state_dict\r\n    # model = SimpleCNN(...) # Instantiate your model architecture first\r\n    # model.load_state_dict(torch.load(model_path))\r\n\r\n    # If you saved the entire model object (less recommended but simpler for demos)\r\n    model = torch.load(model_path)\r\n\r\n    model.eval() # Set model to evaluation mode (important for dropout, batchnorm)\r\n    print(\"Model loaded successfully.\")\r\n    # print(model) # Print model architecture (less detailed than Keras summary)\r\nexcept Exception as e:\r\n    print(f\"Error loading model: {e}\")\r\n    print(\"Make sure the model path is correct and the model was saved correctly.\")\r\n    model = None # Ensure model is None if loading fails\r\n\r\n# You can now use this 'model' object to make predictions\r\n# with torch.no_grad(): # Inference should be done without tracking gradients\r\n#     output = model(ml_input) # Assuming ml_input is a PyTorch tensor\r\n```\r\n\r\n**Teaching Tip:** Remind learners that the model architecture *must* be compatible with the saved weights/state. If they saved only the state dictionary in PyTorch, they need to instantiate the model class first. Saving the whole model object is simpler for this course's purpose if using PyTorch.\r\n\r\n### **4.5 Implementing the Classification Logic based on SDR Input**\r\n\r\nOnce you have the `ml_input` data ready (processed chunk of SDR data in the correct format) and the `model` loaded, the next step is running inference and interpreting the output.\r\n\r\n**Inference:**\r\n\r\nThe model will output a tensor/array of probabilities (if using softmax in the output layer) or raw scores (logits) for each class.\r\n\r\n*   **TensorFlow/Keras:** `predictions = model.predict(ml_input)`\r\n*   **PyTorch:** `with torch.no_grad(): output = model(ml_input)`\r\n\r\n**Interpretation:**\r\n\r\nAssuming the model outputs probabilities (summing to 1 across classes), the predicted class is the one with the highest probability.\r\n\r\n*   Use `np.argmax()` to find the index of the maximum probability.\r\n*   Map this index back to the corresponding class label (e.g., using a list or dictionary created during M3 training).\r\n\r\n**Code Snippet (Classification Logic)**\r\n\r\n```python\r\nimport numpy as np\r\n\r\n# Assuming 'model' is loaded (TensorFlow or PyTorch)\r\n# Assuming 'ml_input' is prepared (NumPy array for TF, PyTorch tensor for PyTorch)\r\n# Assuming you have a list mapping index to label from M3\r\nlabel_map = ['ClassA', 'ClassB', 'ClassC'] # Example: replace with your actual labels\r\n\r\n# --- Run Inference ---\r\nif model is not None:\r\n    try:\r\n        # --- TensorFlow ---\r\n        if isinstance(model, tf.keras.Model):\r\n             # model.predict expects a batch, ml_input should have batch dimension\r\n             predictions = model.predict(ml_input)\r\n             # predictions shape will be (batch_size, num_classes)\r\n             # Assuming batch_size = 1\r\n             probabilities = predictions[0] # Get probabilities for the single input\r\n\r\n             predicted_index = np.argmax(probabilities)\r\n             predicted_label = label_map[predicted_index]\r\n             confidence = probabilities[predicted_index]\r\n\r\n             print(f\"Prediction: {predicted_label} (Confidence: {confidence:.4f})\")\r\n\r\n        # --- PyTorch ---\r\n        elif isinstance(model, torch.nn.Module):\r\n            # PyTorch model expects a tensor\r\n            if not isinstance(ml_input, torch.Tensor):\r\n                 # Convert NumPy array to PyTorch tensor if needed\r\n                 ml_input = torch.from_numpy(ml_input).float() # Adjust dtype if necessary\r\n\r\n            with torch.no_grad():\r\n                 output = model(ml_input) # output shape will be (batch_size, num_classes)\r\n\r\n            # Assuming batch_size = 1\r\n            output = output.squeeze(0) # Remove batch dimension\r\n\r\n            # If model output is logits, apply softmax to get probabilities\r\n            if not torch.isclose(output.sum(), torch.tensor(1.0)): # Simple check if it's likely not probabilities\r\n                 probabilities = torch.softmax(output, dim=0)\r\n            else:\r\n                 probabilities = output # Assume it's already probabilities\r\n\r\n            predicted_index = torch.argmax(probabilities).item()\r\n            predicted_label = label_map[predicted_index]\r\n            confidence = probabilities[predicted_index].item()\r\n\r\n            print(f\"Prediction: {predicted_label} (Confidence: {confidence:.4f})\")\r\n\r\n        else:\r\n            print(\"Model is of an unknown type (not TF Keras or PyTorch Module).\")\r\n\r\n    except Exception as e:\r\n        print(f\"Error during inference: {e}\")\r\n        print(\"Check input data shape and model compatibility.\")\r\n\r\nelse:\r\n    print(\"Model not loaded, cannot perform inference.\")\r\n\r\n```\r\n\r\n**Teaching Tip:** Stress the importance of the `label_map` matching the order of output classes from the model trained in M3.\r\n\r\n### **4.6 Visualization of Classification Results**\r\n\r\nSeeing is believing! Simply printing the prediction is useful, but visualizing it alongside the spectrum data provides valuable context and helps debug.\r\n\r\n**Visualization Options:**\r\n\r\n*   **Text Overlay on Plot:** If you are displaying a real-time spectrum or waterfall plot (using `matplotlib.animation`), you can update a text annotation on the plot with the current prediction.\r\n*   **Dedicated Display Area:** A simple GUI (using `Tkinter`, `PyQt`, or `Streamlit` for a web interface) could show the current spectrum chunk, the classification result, and maybe a confidence score.\r\n*   **Command Line Output:** For simplicity, just printing the prediction and confidence is sufficient for the core functionality of the target system.\r\n\r\n**Code Snippet (Basic Matplotlib Plot Update - Illustrative)**\r\n\r\nThis requires setting up an animated plot, which is more complex. Here's a simplified idea of updating a title or text:\r\n\r\n```python\r\n# This is conceptual and requires a Matplotlib animation loop setup\r\n# See Matplotlib documentation for real-time plotting/animation examples\r\n\r\n# Assuming 'ax' is a Matplotlib axes object displaying the spectrum/spectrogram\r\n# Assuming 'predicted_label' and 'confidence' are available from 4.5\r\n\r\n# Option 1: Update plot title\r\nax.set_title(f\"Classified as: {predicted_label} ({confidence:.2f})\")\r\n\r\n# Option 2: Update a text annotation\r\n# You would need to create a text object initially and update it\r\n# text_annotation = ax.text(x_pos, y_pos, \"\", color='white')\r\n# text_annotation.set_text(f\"Prediction: {predicted_label}\\nConfidence: {confidence:.2f}\")\r\n\r\n# Remember to redraw the canvas in the animation loop\r\n# fig.canvas.draw()\r\n# fig.canvas.flush_events()\r\n```\r\n\r\n**Teaching Tip:** For the Module Project, simple command-line output is perfectly acceptable. Real-time plotting adds complexity that isn't core to the adversarial attack concept itself. Focus on getting the capture -> process -> classify loop working reliably first.\r\n\r\n### **4.7 Setting up a Controlled Test Environment**\r\n\r\nThis is absolutely critical for developing and testing adversarial attacks. The unpredictable nature of the real RF environment makes repeatable experiments very difficult.\r\n\r\n**Why Controlled?**\r\n\r\n*   **Repeatability:** You need to be able to feed the \"same\" signal to the target system multiple times and observe consistent results (or consistent misclassifications from your attack).\r\n*   **Isolation:** Minimize interference from external sources (WiFi, Bluetooth, local broadcasts) that could affect the signal or confuse your target classifier.\r\n*   **Known Signal Source:** You need a reliable way to generate the clean signals that your target system is supposed to classify correctly before you add adversarial perturbations.\r\n*   **Controlled Signal Strength:** Adversarial attacks are often sensitive to the signal-to-noise ratio (SNR). Controlling the signal strength reaching the target RX antenna is important.\r\n\r\n**How to Build a Controlled Test Environment:**\r\n\r\n1.  **Two SDRs:** You ideally need at least two SDRs.\r\n    *   One **Transmitter (TX) SDR:** To generate the \"clean\" signal you want to classify (or later, the adversarial signal). Requires a TX-capable SDR like HackRF One, LimeSDR, ADALM-PLUTO.\r\n    *   One **Receiver (RX) SDR:** Connected to your target classification system. Can be an RTL-SDR if your TX SDR covers the same frequency range, or another TX/RX SDR.\r\n2.  **Cables and Attenuators:** The *most* controlled environment uses cables to directly connect the TX SDR's output to the RX SDR's input. **WARNING:** SDRs are sensitive! You *must* use appropriate in-line attenuators to reduce the TX power significantly before it reaches the RX input. Transmitting directly into an RX port without attenuation can damage the receiver. Attenuators (e.g., 30dB, 40dB, 50dB pads) are essential.\r\n3.  **Antennas (Less Controlled):** If direct cabling isn't feasible or you want a slightly more realistic \"over-the-air\" feel, use antennas. Place the TX and RX antennas very close together (e.g., inches or a few feet apart).\r\n4.  **Shielding (Ideal but Hard):** A Faraday cage or shielded box can significantly reduce external interference. This is often impractical for a home lab setup but is the gold standard for controlled RF testing.\r\n5.  **Fixed Parameters:** Keep SDR parameters (center frequency, sample rate, gain) constant during experiments.\r\n\r\n**Setup Diagram (Conceptual - Cabling Method):**\r\n\r\n```\r\n[TX SDR] --> [Attenuator Bank (e.g., 40dB + 10dB)] --> [RX SDR] --> [Computer Running Target System]\r\n```\r\n\r\n**Setup Diagram (Conceptual - Close Antennas):**\r\n\r\n```\r\n[TX SDR] -- Antenna ]))) ((([ Antenna -- [RX SDR] --> [Computer Running Target System]\r\n```\r\n\r\n**Teaching Tip:** Strongly advise learners to start with the cabled setup with *significant* attenuation. It removes the variability of the air interface and makes debugging the core capture/process/classify pipeline much easier. Emphasize starting with very low transmit power on the TX SDR as well.\r\n\r\n### **Module Project/Exercise: Building the Target System**\r\n\r\nYour goal is to combine the pieces discussed into a functional script or program.\r\n\r\n**Steps:**\r\n\r\n1.  **Review and Prepare:**\r\n    *   Ensure you have a trained ML model file saved from Module 3.\r\n    *   Have your SDR setup working (tested in Module 2).\r\n    *   Make sure all necessary libraries are installed (`pyrtlsdr`/`pyhackrf`/`pysdr`, `numpy`, `scipy`, `matplotlib`, `tensorflow`/`pytorch`, `scikit-image` if using `resize`).\r\n\r\n2.  **Load the Model:** Write the code to load your trained model using the path where you saved it. Include error handling.\r\n\r\n3.  **Initialize the RX SDR:**\r\n    *   Write code to initialize your SDR in receive mode.\r\n    *   Set the center frequency and sample rate. Choose a frequency you can control (e.g., an ISM band frequency like 433 MHz or 915 MHz if allowed in your region, or a frequency you'll generate with your TX SDR).\r\n    *   Set the gain (experiment to find a good level that isn't saturated but captures signals).\r\n\r\n4.  **Implement the Main Loop:** Create a loop that runs continuously (or for a set duration).\r\n\r\n5.  **Inside the Loop:**\r\n    *   **Capture Data:** Read a chunk of IQ samples from the SDR. Calculate the required chunk size based on your desired classification duration and sample rate (as discussed in 4.2).\r\n    *   **Process Data:** Apply the *exact same* processing steps used to prepare your training data in Module 3. If using spectrograms:\r\n        *   Generate the spectrogram data (`scipy.signal.spectrogram`).\r\n        *   Convert to dB (optional but common).\r\n        *   Resize/reshape to the model's input dimensions (`skimage.transform.resize` is an option, or custom slicing/padding).\r\n        *   Normalize/scale the data.\r\n        *   Add batch and channel dimensions if required by your model (`numpy.expand_dims`).\r\n    *   **Run Inference:** Feed the processed data (`ml_input`) into the loaded model (`model.predict()` or `model("
    },
    {
      "title": "module_5",
      "description": "module_5 Overview",
      "order": 5,
      "content": "Okay class, welcome to Module 5! We've built our target system – a foundational AI classifier for RF signals. Now, it's time to switch gears and think like an adversary. This module is where we delve into the fascinating, sometimes counter-intuitive, world of Adversarial Machine Learning (AML). We'll learn *why* AI models, despite their impressive capabilities, can be surprisingly fragile and susceptible to carefully crafted inputs designed to deceive them.\r\n\r\nThink of it like this: You've built a highly skilled guard dog (your AI classifier) trained to recognize specific scents (RF signal patterns). Now, we're going to learn how to create a synthetic scent that smells *just slightly* different from a recognized friendly scent, but is different *enough* to fool the dog into thinking it's smelling something else entirely, or perhaps nothing at all.\r\n\r\nThis module lays the theoretical and practical groundwork for generating the adversarial signals we'll eventually transmit over the air. While our primary goal is RF security, the concepts of AML are universal. For simplicity and to leverage readily available tools and datasets, our hands-on coding exercises in this module will focus on generating adversarial examples for image classifiers. This is a standard practice in learning AML, as the principles transfer directly to other data types like RF signals, and visualizing image perturbations is often more intuitive initially.\r\n\r\nLet's dive in!\r\n\r\n---\r\n\r\n## **Module 5: The Adversarial Mindset: Introduction to AI Attacks**\r\n\r\n*   **Module Objective:** Dive into the theory and common techniques of Adversarial Machine Learning, understanding *why* AI models are vulnerable and the different goals of an attacker.\r\n\r\n### **5.1 What are Adversarial Examples? Small Perturbations, Large Impacts**\r\n\r\n*   **Concept:** Adversarial examples are inputs to an AI model that have been intentionally modified to cause the model to make a specific error (usually a misclassification). The key characteristic is that these modifications are often *imperceptible* or *insignious* to humans, but drastically alter the model's output.\r\n\r\n*   **Why is this surprising?** We often think of AI models, especially deep learning ones, as learning robust, high-level features similar to how humans perceive things. The existence of adversarial examples shows that they sometimes rely on subtle, brittle patterns that don't align with human intuition.\r\n\r\n*   **Classic Example (Image Domain):** The most famous example is modifying an image of a panda by adding a small amount of carefully calculated noise. To a human, it still looks clearly like a panda. But to a trained image classifier, it might confidently classify the image as a gibbon.\r\n\r\n    *   *Original Image:* Panda (classified correctly)\r\n    *   *Perturbation:* Small, almost invisible noise pattern.\r\n    *   *Adversarial Image:* Original Image + Perturbation = Still looks like a panda to you and me.\r\n    *   *Model Output:* Gibbon (misclassified with high confidence).\r\n\r\n*   **Relevance to RF:** Imagine our RF classifier is trained to distinguish between different modulation types (FM, FSK, QPSK, etc.). An adversarial RF attack would involve taking a legitimate signal (say, a standard FM signal) and adding a carefully crafted, low-power noise or perturbation. To a standard RF receiver or even a human looking at a spectrogram, it might still look like FM, but the AI classifier might misinterpret it as FSK, or perhaps just noise.\r\n\r\n### **5.2 The Geometry of Adversarial Attacks: Decision Boundaries**\r\n\r\n*   **Concept:** Machine learning classifiers work by learning complex *decision boundaries* in a high-dimensional feature space. These boundaries separate inputs belonging to different classes.\r\n\r\n*   **How it Works:** During training, the model adjusts its parameters (weights and biases) to define these boundaries based on the training data. For a new input, the model determines which side of the boundary it falls on to make a prediction.\r\n\r\n*   **Vulnerability:** Deep neural networks often learn decision boundaries that are complex and, crucially, can be relatively close to the training data points. Adversarial attacks exploit this. By adding a small perturbation, the attacker pushes the input data point across a decision boundary into a region that the model associates with a different class.\r\n\r\n*   **Linearity:** Another factor is the (piecewise) linear nature of many neural network operations. While the overall function is non-linear, locally, the gradients are well-defined. Gradient-based attacks (like FGSM or PGD) work by calculating the direction of the steepest increase in the loss function (which corresponds to moving the data point towards a misclassification) *with respect to the input data*. A small step in this direction can quickly cross a decision boundary, especially in high-dimensional spaces.\r\n\r\n### **5.3 Attack Goals: What Does the Adversary Want?**\r\n\r\nThe goal of an adversarial attack isn't just to break the model; it's usually a specific kind of failure.\r\n\r\n*   **Untargeted Misclassification:** The simplest goal. The attacker wants the model to misclassify the input into *any* wrong class.\r\n    *   *RF Example:* Make the AI classifier think a legitimate WiFi signal is *anything other than WiFi*.\r\n\r\n*   **Targeted Misclassification:** A more sophisticated goal. The attacker wants the model to misclassify the input into a *specific* target class chosen by the attacker.\r\n    *   *RF Example:* Make the AI classifier think a legitimate WiFi signal is specifically an LTE signal. This could be used to trick a cognitive radio system into allocating spectrum incorrectly.\r\n\r\n*   **Evasion:** The attacker wants the input to be classified as an \"unknown\" or \"noise\" class, or simply to avoid detection entirely by appearing benign.\r\n    *   *RF Example:* Make a malicious or unauthorized signal appear as background noise or a known, harmless signal type to avoid triggering alarms in a spectrum monitoring system.\r\n\r\nIn the context of our course, we'll primarily focus on demonstrating targeted or untargeted misclassification of known signal types as a proof of concept. Evasion attacks are also highly relevant but can sometimes require more complex techniques.\r\n\r\n### **5.4 Threat Models: What Does the Attacker Know?**\r\n\r\nThe attacker's knowledge about the target model significantly impacts the types of attacks they can perform.\r\n\r\n*   **White-box Attack:** The attacker has full knowledge of the target model. This includes:\r\n    *   Model architecture (layers, activation functions, etc.).\r\n    *   Model parameters (weights and biases).\r\n    *   Training data (or at least the distribution it came from).\r\n    *   The specific machine learning framework used.\r\n    *   *Advantage:* Attackers can calculate exact gradients and craft highly effective, minimal perturbations.\r\n    *   *Realism:* Less common in real-world black-box systems, but crucial for *research* to understand vulnerabilities and potential defenses. Our initial attacks will be white-box.\r\n\r\n*   **Black-box Attack:** The attacker has limited or no knowledge of the internal workings of the target model. They can only interact with the model by sending inputs and observing the outputs (e.g., predicted class, confidence scores, or raw logits).\r\n    *   *Challenge:* Cannot directly calculate gradients with respect to the input data using the target model's parameters.\r\n    *   *Techniques:*\r\n        *   **Transferability:** Exploit the fact that adversarial examples often transfer between different models. An attacker might train a *surrogate* model (that they control) to mimic the target model's behavior and generate adversarial examples using a white-box attack on the surrogate. These examples might then successfully fool the real black-box target.\r\n        *   **Query-based Attacks:** Iteratively probe the target model with many inputs and observe the outputs to estimate the gradient or build a local approximation of the decision boundary. This requires many queries and can be computationally expensive.\r\n    *   *Realism:* More representative of real-world scenarios where attackers don't have access to model internals (e.g., attacking a deployed system via its API or RF interface).\r\n\r\nFor this course, we will focus on white-box attacks initially because they are the easiest to understand and implement. Module 8 might briefly touch upon the concept of transferability as a potential extension.\r\n\r\n### **5.5 Common White-box Attack Techniques**\r\n\r\nThese techniques leverage the attacker's knowledge of the model's internal structure and parameters, particularly using gradients.\r\n\r\n*   **Fast Gradient Sign Method (FGSM):**\r\n    *   **Idea:** A simple, one-step attack. Calculate the gradient of the loss function with respect to the input data. The sign of this gradient indicates the direction in the input space that maximally increases the loss (i.e., makes the correct class less likely). Take a step in this direction, scaled by a small value `epsilon`.\r\n    *   **Formula:**\r\n        `adversarial_example = original_input + epsilon * sign(gradient(Loss(original_input, true_label), original_input))`\r\n    *   **Explanation:**\r\n        *   `Loss(...)`: The model's loss function (e.g., cross-entropy).\r\n        *   `gradient(Loss, original_input)`: The gradient of the loss with respect to the input data. This tells us how much the loss changes if we slightly change each element of the input.\r\n        *   `sign(...)`: Takes the sign (+1 or -1) of each element in the gradient. This gives us the *direction* to push the input data to increase the loss.\r\n        *   `epsilon`: A small scalar value that controls the magnitude of the perturbation. A larger `epsilon` usually results in a more effective attack but a more noticeable perturbation.\r\n    *   **Pros:** Simple, fast to compute.\r\n    *   **Cons:** Often results in noticeable perturbations, less effective than iterative methods, less likely to achieve targeted misclassification reliably.\r\n\r\n*   **Projected Gradient Descent (PGD):**\r\n    *   **Idea:** An iterative attack. Instead of one large step, PGD takes multiple small steps. It starts with a small random perturbation and then repeatedly applies a small gradient step (like FGM, the non-sign version) and *projects* the result back into an allowed perturbation region (e.g., an L-infinity ball around the original input). This projection ensures the total perturbation remains bounded.\r\n    *   **Process:**\r\n        1.  Start with `adversarial_example = original_input + random_noise` (where noise is within a small bound).\r\n        2.  Repeat for `N` iterations:\r\n            *   Calculate `gradient = gradient(Loss(adversarial_example, true_label), adversarial_example)`.\r\n            *   Update `adversarial_example = adversarial_example + step_size * sign(gradient)`. (Sometimes the sign is omitted, using just the gradient).\r\n            *   Project `adversarial_example` back onto the allowed perturbation set (e.g., ensure `abs(adversarial_example - original_input) <= epsilon`).\r\n    *   **Pros:** Generally more effective and produces less noticeable perturbations than FGSM, considered a strong first-order adversary.\r\n    *   **Cons:** More computationally expensive than FGSM due to iterations.\r\n\r\n*   **DeepFool:**\r\n    *   **Idea:** Finds the *minimum* perturbation needed to cross a decision boundary. It iteratively linearizes the classifier around the current point and moves the point orthogonally to the nearest hyperplane until it crosses a boundary.\r\n    *   **Pros:** Aims for minimal perturbation, often more efficient than PGD in finding *any* misclassification boundary.\r\n    *   **Cons:** Can be more complex to implement than FGSM/PGD, primarily designed for untargeted attacks.\r\n\r\nFor our practical exercise, we will focus on implementing FGSM due to its simplicity, making it an excellent starting point to grasp the core concept of gradient-based adversarial generation.\r\n\r\n### **5.6 Common Black-box Attack Techniques (Briefly)**\r\n\r\nAs mentioned under Threat Models, black-box attacks are harder as they can't directly use gradients of the target model.\r\n\r\n*   **Transferability:** Generate adversarial examples using a white-box attack on a *different* model (a surrogate) trained on similar data or performing the same task. Then, use these examples against the black-box target. This works surprisingly well because different models often learn similar decision boundaries or rely on similar features.\r\n*   **Query-based (e.g., Boundary Attack, HopSkipJumpAttack):** These methods interact with the model by sending queries and observing the response. They use this information to estimate gradients or find the shortest path from a misclassified point back to the boundary of the original class, then step just across it. These require many interactions with the target model.\r\n\r\nWe won't implement these in detail, but it's important to know they exist and are relevant for real-world scenarios where the attacker doesn't have white-box access.\r\n\r\n### **5.7 Metrics for Adversarial Attacks**\r\n\r\nHow do we measure the success and quality of an adversarial attack?\r\n\r\n*   **Success Rate:** The percentage of adversarial examples that successfully fool the target model into making the desired misclassification (targeted) or any misclassification (untargeted).\r\n*   **Perturbation Size:** How \"big\" is the added noise? We want the perturbation to be minimal (ideally imperceptible) to be stealthy. Common ways to measure the size of the perturbation vector (`delta = adversarial_example - original_input`) are using Lp norms:\r\n    *   **L-infinity norm (`||delta||_inf`):** The maximum absolute value of any element in the perturbation vector. This measures the largest change applied to any single feature (e.g., pixel value, IQ sample). It's often used because it's easy to constrain (e.g., ensure no pixel changes by more than `epsilon`).\r\n    *   **L-2 norm (`||delta||_2`):** The Euclidean distance of the perturbation vector (`sqrt(sum(delta^2))`). This measures the overall \"energy\" or magnitude of the perturbation across all features.\r\n    *   **L-0 norm (`||delta||_0`):** The number of non-zero elements in the perturbation vector. This measures how sparse the perturbation is (how many features were changed).\r\n\r\nFor RF signals, these norms will eventually need to be related to physical constraints like power limits or bandwidth limits, which we'll discuss in Module 6. But for now, thinking about L-infinity (maximum change per sample) and L-2 (total energy of the perturbation) is useful.\r\n\r\n### **5.8 Case Study: Adversarial Attacks in the Image Domain**\r\n\r\nWe touched on the panda/gibbon example. Let's consider others and their implications:\r\n\r\n*   **Traffic Signs:** Researchers have shown that small stickers or graffiti on stop signs can cause autonomous vehicle vision systems to misclassify them as speed limit signs. The implications for safety are obvious and severe.\r\n*   **Facial Recognition:** Perturbations added to glasses or makeup can cause facial recognition systems to fail to identify a person or misidentify them as someone else entirely.\r\n*   **Object Detection:** Adding adversarial patterns to clothing or objects can make them \"invisible\" to object detection systems.\r\n\r\nThese examples highlight that adversarial vulnerabilities are not just theoretical curiosities; they pose real security and safety risks in deployed AI systems across various domains. Our focus is RF, but the underlying principles of manipulating input data to exploit model weaknesses are the same.\r\n\r\n### **5.9 Ethical and Legal Considerations (Revisited)**\r\n\r\nAs we move into generating attacks, it's crucial to reiterate the ethical responsibilities. Learning these techniques is for understanding vulnerabilities and developing defenses. Any practical application *must* be confined to controlled environments, with explicit permission, and within legal boundaries. Unauthorized RF transmission is illegal and can interfere with critical services. Always check local regulations (e.g., FCC rules in the US). The goal is responsible research and disclosure, not malicious activity.\r\n\r\n### **5.10 Setting up the Environment (AML Specific)**\r\n\r\nWe'll continue using our Python environment from Module 1, but we'll add libraries specifically useful for Machine Learning and potentially Adversarial Machine Learning research.\r\n\r\n*   **ML Frameworks:** TensorFlow or PyTorch. We'll use TensorFlow for our examples as it integrates well with Keras and has built-in functions for gradient calculation.\r\n*   **Standard Libraries:** NumPy, Matplotlib (already covered).\r\n\r\nMake sure you have TensorFlow installed:\r\n```bash\r\npip install tensorflow matplotlib numpy\r\n```\r\n*(Note: If you have a CUDA-enabled GPU, install `tensorflow-gpu` for faster training, though for our simple MNIST example, CPU is fine).*\r\n\r\n### **5.11 Practical Implementation: Implementing FGSM on an Image Classifier**\r\n\r\nNow for the hands-on part! We will implement the FGSM attack on a simple Convolutional Neural Network (CNN) trained on the MNIST dataset. MNIST consists of small (28x28 pixel) grayscale images of handwritten digits (0-9). This is a common \"hello world\" for image classification and adversarial attacks due to its simplicity and speed.\r\n\r\n**Goal:** Take a correctly classified MNIST image, apply the FGSM perturbation, and show that the model misclassifies the resulting adversarial image.\r\n\r\n**Steps:**\r\n\r\n1.  Load the MNIST dataset.\r\n2.  Build and train a simple CNN classifier (or load a pre-trained one).\r\n3.  Select a test image that the model classifies correctly.\r\n4.  Implement the FGSM logic using TensorFlow's gradient capabilities.\r\n5.  Generate the adversarial example.\r\n6.  Visualize the original, perturbation, and adversarial image.\r\n7.  Show the model's prediction on the original and adversarial images.\r\n8.  (Optional) Evaluate the attack success rate on a small batch of test images.\r\n\r\nLet's walk through the code.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\n# Ensure reproducibility\r\ntf.random.set_seed(42)\r\nnp.random.seed(42)\r\n\r\nprint(f\"TensorFlow version: {tf.__version__}\")\r\n\r\n# --- Step 1: Load and Prepare Data ---\r\n# Load MNIST dataset\r\nmnist = tf.keras.datasets.mnist\r\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\r\n\r\n# Normalize pixel values to [0, 1]\r\ntrain_images = train_images.astype('float32') / 255.0\r\ntest_images = test_images.astype('float32') / 255.0\r\n\r\n# Add a channel dimension for CNN input (MNIST is grayscale, so 1 channel)\r\ntrain_images = train_images[..., tf.newaxis]\r\ntest_images = test_images[..., tf.newaxis]\r\n\r\n# Select a subset for faster testing if needed\r\n# test_images = test_images[:1000]\r\n# test_labels = test_labels[:1000]\r\n\r\n# --- Step 2: Build and Train a Simple CNN Classifier ---\r\n# We'll build a small, simple CNN\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\r\n    tf.keras.layers.MaxPooling2D((2, 2)),\r\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\r\n    tf.keras.layers.MaxPooling2D((2, 2)),\r\n    tf.keras.layers.Flatten(),\r\n    tf.keras.layers.Dense(128, activation='relu'),\r\n    tf.keras.layers.Dense(10, activation='softmax') # 10 classes for digits 0-9\r\n])\r\n\r\n# Compile the model\r\nmodel.compile(optimizer='adam',\r\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), # Use from_logits=False with softmax output\r\n              metrics=['accuracy'])\r\n\r\n# Train the model (for simplicity, we'll train for a short time)\r\nprint(\"Training the model...\")\r\nmodel.fit(train_images, train_labels, epochs=3, validation_data=(test_images, test_labels))\r\nprint(\"Model training complete.\")\r\n\r\n# Evaluate the model on the test set\r\nloss, acc = model.evaluate(test_images, test_labels, verbose=0)\r\nprint(f\"Model accuracy on test set: {acc:.4f}\")\r\n\r\n# --- Step 3: Select a Test Image ---\r\n# Find a test image that the model classifies correctly\r\n# We need the image, its true label, and the model's correct prediction\r\ncorrectly_classified_index = -1\r\nfor i in range(len(test_images)):\r\n    img = test_images[i:i+1] # Get image as a batch of 1\r\n    true_label = test_labels[i]\r\n    prediction = model.predict(img)\r\n    predicted_label = np.argmax(prediction[0])\r\n    if predicted_label == true_label:\r\n        correctly_classified_index = i\r\n        break\r\n\r\nif correctly_classified_index == -1:\r\n    print(\"Could not find a correctly classified image in the first batch. Check model accuracy.\")\r\n    exit()\r\n\r\noriginal_image = test_images[correctly_classified_index:correctly_classified_index+1]\r\ntrue_label = test_labels[correctly_classified_index]\r\n\r\nprint(f\"\\nSelected image index: {correctly_classified_index}\")\r\nprint(f\"True label: {true_label}\")\r\ninitial_prediction = model.predict(original_image)\r\ninitial_predicted_label = np.argmax(initial_prediction[0])\r\nprint(f\"Initial model prediction: {initial_predicted_label}\")\r\n\r\n\r\n# --- Step 4 & 5: Implement FGSM and Generate Adversarial Example ---\r\n\r\n# Function to generate adversarial example using FGSM\r\ndef create_adversarial_example_fgsm(model, original_image, true_label, epsilon):\r\n    # Convert image and label to TensorFlow tensors with correct data types\r\n    image_tensor = tf.cast(original_image, tf.float32)\r\n    label_tensor = tf.cast([true_label], tf.int64) # Ensure label is a batch of 1\r\n\r\n    # Use tf.GradientTape to record operations for automatic differentiation\r\n    with tf.GradientTape() as tape:\r\n        tape.watch(image_tensor) # Tell the tape to watch the input image tensor\r\n        prediction = model(image_tensor) # Get the model's prediction\r\n\r\n        # Calculate the loss. We want to maximize the loss for the *true* class\r\n        # to push the prediction away from the true class.\r\n        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(label_tensor, prediction)\r\n\r\n    # Get the gradients of the loss with respect to the input image\r\n    gradient = tape.gradient(loss, image_tensor)\r\n\r\n    # Get the sign of the gradients\r\n    signed_grad = tf.sign(gradient)\r\n\r\n    # Create the adversarial example by adding the signed gradient scaled by epsilon\r\n    adversarial_image = image_tensor + epsilon * signed_grad\r\n\r\n    # Clip the adversarial image to ensure pixel values are still in [0, 1]\r\n    # This is important for image data, less so for raw RF IQ, but good practice\r\n    adversarial_image = tf.clip_by_value(adversarial_image, 0, 1)\r\n\r\n    return adversarial_image, signed_grad # Also return the perturbation for visualization\r\n\r\n# Choose an epsilon value. This controls the strength of the attack.\r\n# Start small and increase if needed. For MNIST, 0.1 to 0.3 often works.\r\nepsilon = 0.15 # Experiment with this value!\r\n\r\nprint(f\"\\nGenerating adversarial example with epsilon = {epsilon}\")\r\nadversarial_image, perturbation = create_adversarial_example_fgsm(model, original_image, true_label, epsilon)\r\nprint(\"Adversarial example generated.\")\r\n\r\n# --- Step 6 & 7: Visualize and Evaluate ---\r\n\r\n# Function to display images and predictions\r\ndef display_images_and_predictions(original_img, adv_img, perturbation_img, model, true_label):\r\n    original_pred = model.predict(original_img)\r\n    original_pred_label = np.argmax(original_pred[0])\r\n    original_pred_confidence = np.max(original_pred[0])\r\n\r\n    adv_pred = model.predict(adv_img)\r\n    adv_pred_label = np.argmax(adv_pred[0])\r\n    adv_pred_confidence = np.max(adv_pred[0])\r\n\r\n    # Remove the channel dimension for plotting grayscale images\r\n    original_img_plot = original_img[0, ..., 0]\r\n    adv_img_plot = adv_img[0, ..., 0]\r\n    # The perturbation can be bipolar, so we might need to scale it for visualization\r\n    perturbation_img_plot = perturbation_img[0, ..., 0]\r\n\r\n\r\n    plt.figure(figsize=(12, 4))\r\n\r\n    # Original Image\r\n    plt.subplot(1, 3, 1)\r\n    plt.imshow(original_img_plot, cmap='gray')\r\n    plt.title(f\"Original\\nTrue: {true_label}, Pred: {original_pred_label}\\nConfidence: {original_pred_confidence:.2f}\")\r\n    plt.axis('off')\r\n\r\n    # Perturbation (scaled for visibility)\r\n    plt.subplot(1, 3, 2)\r\n    # Use a diverging colormap to show positive/negative changes\r\n    # vmin/vmax symmetric around 0 makes 0 white/gray\r\n    abs_max_pert = np.max(np.abs(perturbation_img_plot))\r\n    plt.imshow(perturbation_img_plot, cmap='coolwarm', vmin=-abs_max_pert, vmax=abs_max_pert)\r\n    plt.title(f\"Perturbation (epsilon={epsilon:.2f})\\nL-inf norm: {np.max(np.abs(original_img - adv_img)):.4f}\")\r\n    plt.axis('off')\r\n    plt.colorbar() # Add a colorbar to show the scale of perturbation\r\n\r\n    # Adversarial Image\r\n    plt.subplot(1, 3, 3)\r\n    plt.imshow(adv_img_plot, cmap='gray')\r\n    plt.title(f\"Adversarial\\nTrue: {true_label}, Pred: {adv_pred_label}\\nConfidence: {adv_pred_confidence:.2f}\")\r\n    plt.axis('off')\r\n\r\n    plt.tight_layout()\r\n    plt.show()\r\n\r\n# Display the results for our single example\r\ndisplay_images_and_predictions(original_image, adversarial_image, perturbation, model, true_label)\r\n\r\n# --- Step 8 (Optional): Evaluate on a Batch ---\r\n# This is more robust than just one example\r\ndef evaluate_fgsm_attack(model, test_images, test_labels, epsilon):\r\n    successful_attacks = 0\r\n    total_attempts = 0\r\n\r\n    # We will only attack images that the model initially classifies correctly\r\n    for i in range(len(test_images)):\r\n        img = test_images[i:i+1]\r\n        true_label = test_labels[i]\r\n\r\n        initial_prediction = model.predict(img, verbose=0)\r\n        initial_predicted_label = np.argmax(initial_prediction[0])\r\n\r\n        if initial_predicted_label == true_label: # Only attack if initially correct\r\n            total"
    },
    {
      "title": "module_6",
      "description": "module_6 Overview",
      "order": 6,
      "content": "Alright team, welcome back! We've built our foundation in RF and SDRs (Modules 1 & 2), we've understood how AI plays in the spectrum space and even built our own basic classifier (Modules 3 & 4). In Module 5, we peered into the fascinating, and frankly, a little unsettling, world of Adversarial Machine Learning – understanding *why* AI can be fooled and the core concepts like gradient ascent.\r\n\r\nNow, in Module 6, we take that adversarial theory and smack it right onto our RF classification problem. This is where we bridge the gap between the abstract math of AML and the concrete data of RF signals. Our goal isn't to transmit anything *yet* – that's Module 7. Here, we focus on generating the *digital* adversarial examples, the specially crafted data inputs that, when fed into our trained AI model (from M4), cause it to misclassify.\r\n\r\nThis is a crucial step. We need to confirm our adversarial generation *works* in the pristine digital domain before we introduce the messiness of over-the-air transmission. Think of it as crafting the perfect blueprint before heading to the construction site.\r\n\r\nLet's dive in!\r\n\r\n---\r\n\r\n## Module 6: Crafting RF Adversarial Examples (Digital)\r\n\r\n**Module Objective:** Adapt general adversarial machine learning techniques to the RF domain, focusing on generating adversarial *data* (IQ or spectrograms) that can deceive the target RF classifier *before* over-the-air transmission.\r\n\r\n**Contribution to Capstone:** Creates the core adversarial signal *generation* component (digital stage).\r\n\r\n---\r\n\r\n### 6.1 Recapping Adversarial Attacks & The Gradient Idea\r\n\r\nRemember from Module 5, the core idea behind many white-box adversarial attacks (like FGSM and PGD) is to find a small perturbation to the input data that maximizes the model's loss for the correct class (untargeted attack) or minimizes the loss for the correct class while maximizing it for a target class (targeted attack).\r\n\r\nHow do we find this perturbation? Using the model's gradients! If we know how the model's loss changes with respect to small changes in the input data, we can take a \"step\" in the direction that increases the loss the fastest. This is essentially gradient ascent on the loss function, but applied to the *input data* rather than the model weights.\r\n\r\nThe formula we saw was something like:\r\n\r\n`adversarial_example = original_example + epsilon * sign(gradient_of_loss_wrt_input)`\r\n\r\nWhere:\r\n*   `original_example`: Your clean RF data (e.g., a spectrogram image, or an IQ data block).\r\n*   `epsilon`: A small scalar value controlling the magnitude of the perturbation. This is *critical* for RF, as it relates to power constraints later.\r\n*   `gradient_of_loss_wrt_input`: How much the loss changes as each element of the input data changes.\r\n*   `sign()`: Takes the sign of each element in the gradient (either +1 or -1). This ensures we only care about the *direction* of the maximum increase, not the magnitude of the gradient itself (for FGSM).\r\n\r\nOur task now is to apply this concept when the \"input\" is RF data.\r\n\r\n### 6.2 Challenges of RF Perturbations (Digital Stage)\r\n\r\nWhile the mathematical concept is the same, applying AML to RF data presents unique challenges, even before we go over the air:\r\n\r\n1.  **Data Representation:** RF data can be raw IQ samples (complex numbers over time) or processed features like spectrograms (images). The attack needs to be crafted for the specific input format your M4 classifier uses.\r\n    *   **Attacking Spectrograms:** This is often easier as spectrograms are 2D arrays, similar to images. Many existing image AML techniques and libraries can be adapted. The perturbation is applied to the pixel values of the spectrogram.\r\n    *   **Attacking Raw IQ:** This is closer to the physical reality but harder. IQ data is a sequence of complex numbers. Perturbing IQ directly means modifying the amplitude and phase at specific time points. This requires models that handle sequential complex data (e.g., 1D CNNs, LSTMs) and the perturbation constraints need careful consideration (e.g., perturbing phase might be easier/less noticeable than perturbing amplitude, which directly impacts power).\r\n    *   **Decision for this Module:** We'll primarily focus on attacking the **Spectrogram** representation, as it aligns well with CNN classifiers often used for spectrograms (like the one likely built in M4) and allows us to leverage standard AML library functions more directly. We'll discuss the implications for IQ data.\r\n\r\n2.  **Physical Constraints (Mentally applied in digital):** Even in the digital stage, we must be mindful of the physical world.\r\n    *   **Power:** The total power of the transmitted signal is crucial. An adversarial perturbation shouldn't require infinite power. The `epsilon` value directly relates to the magnitude of the perturbation, which in turn affects the power of the resulting adversarial signal. We need ways to constrain this.\r\n    *   **Bandwidth:** The perturbation shouldn't drastically change the signal's bandwidth unless that's the specific attack goal (e.g., making a narrow signal look wide). FGSM/PGD applied naively can spread the signal in the frequency domain.\r\n    *   **Signal Structure:** Ideally, the perturbed signal should *still look somewhat like* the original signal in ways the AI *doesn't* use, but different in ways it *does* use. This is very hard to guarantee with simple gradient attacks.\r\n\r\n3.  **Maintaining Signal Validity (Digital Stage):** For some signal types, the relationship between IQ samples isn't arbitrary (e.g., phase changes in PSK, frequency jumps in FSK). A random perturbation might break the fundamental structure of the signal, making it look like generic noise rather than a crafted adversarial example of a specific class. Simple attacks like FGSM don't inherently respect these structural constraints.\r\n\r\n### 6.3 Applying Adversarial Generation to RF Data (Spectrograms)\r\n\r\nLet's focus on the Spectrogram approach. Our target model from M4 takes spectrogram images as input. We want to generate a slightly modified spectrogram that fools the classifier.\r\n\r\n**Steps to Generate an Adversarial Spectrogram (using FGSM):**\r\n\r\n1.  **Load the Target Model:** Get the `tf.keras.Model` object saved from Module 4.\r\n2.  **Load a Clean RF Data Sample:** Take a spectrogram image of a known signal type (e.g., WiFi). We also need its true class label.\r\n3.  **Prepare Data for Gradient Calculation:** The ML framework needs to track operations to compute gradients. In TensorFlow, this is done with `tf.GradientTape`. The input data also needs to be a `tf.Tensor` and often requires a batch dimension (even if it's just one sample).\r\n4.  **Compute the Gradient:**\r\n    *   Inside the `tf.GradientTape` context, feed the prepared data through the model to get predictions.\r\n    *   Calculate the loss between the predictions and the true label. For an *untargeted* attack, you want to maximize the loss for the *true* class. For a *targeted* attack, you want to minimize the loss for the *target* class you want it to be misclassified as.\r\n    *   Use `tape.gradient()` to compute the gradient of the loss with respect to the input data tensor.\r\n5.  **Calculate the Perturbation:**\r\n    *   Get the `sign()` of the computed gradient.\r\n    *   Multiply the signed gradient by your chosen `epsilon` value. This is your raw perturbation.\r\n6.  **Create the Adversarial Example:**\r\n    *   Add the perturbation to the original clean data sample.\r\n    *   **Crucially:** Clip the resulting values to the valid range of your spectrogram data (e.g., if your spectrograms were normalized to [0, 1], clip the perturbed values to stay within [0, 1]). This ensures the generated data is still a valid \"image\" representation.\r\n7.  **Evaluate Digitally:** Feed the resulting adversarial spectrogram into the *same* loaded model and see what it predicts. If it predicts a class different from the true class (for untargeted) or predicts the specific target class (for targeted), your digital attack was successful!\r\n\r\n### 6.4 Implementing FGSM for RF Spectrograms (Code Example)\r\n\r\nLet's write some Python code using TensorFlow to demonstrate this. We'll need:\r\n*   TensorFlow (`tensorflow`)\r\n*   NumPy (`numpy`)\r\n*   Matplotlib (`matplotlib`) for visualization (optional but helpful)\r\n\r\nAssume:\r\n*   Your model is saved as `rf_classifier_model.h5`.\r\n*   You have a dataset of spectrograms as NumPy arrays, and corresponding labels. Let's assume a single sample `clean_spectrogram` and its `true_label`.\r\n*   Spectrogram data is normalized between 0 and 1.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\n# Assume model path and data loading\r\nMODEL_PATH = 'rf_classifier_model.h5'\r\n# Assume you have loaded your spectrogram dataset and labels\r\n# clean_spectrogram: a numpy array representing one spectrogram (e.g., shape [height, width, 1] for grayscale)\r\n# true_label: the integer label for the clean_spectrogram\r\n# target_label: (only for targeted attack) the integer label you want the model to predict\r\n\r\n# --- 1. Load the Target Model ---\r\ntry:\r\n    model = tf.keras.models.load_model(MODEL_PATH)\r\n    print(f\"Successfully loaded model from {MODEL_PATH}\")\r\n    model.summary() # Print model summary to confirm\r\nexcept Exception as e:\r\n    print(f\"Error loading model: {e}\")\r\n    print(\"Please ensure 'rf_classifier_model.h5' exists and is a valid Keras model.\")\r\n    # Exit or handle error appropriately if model loading fails\r\n    exit() # Or raise an exception\r\n\r\n# --- Assume you have a clean sample and its label ---\r\n# Replace with your actual data loading logic\r\n# Example dummy data creation (replace with your actual data)\r\nspectrogram_height = 128 # Example dimensions from M4\r\nspectrogram_width = 128\r\nnum_classes = 10 # Example number of classes from M4\r\n\r\n# Create a dummy clean spectrogram (e.g., random noise for demonstration)\r\n# In reality, this would be loaded from your captured/processed RF data\r\nclean_spectrogram = np.random.rand(spectrogram_height, spectrogram_width, 1).astype(np.float32)\r\ntrue_label = 2 # Example: Let's say this is a 'WiFi' signal (class 2)\r\n\r\n# For targeted attack, define the target label (e.g., make WiFi look like LTE, class 5)\r\n# targeted_attack = True\r\n# target_label = 5\r\n\r\nprint(f\"Clean spectrogram shape: {clean_spectrogram.shape}\")\r\nprint(f\"True label: {true_label}\")\r\n\r\n# --- 2. Define Attack Parameters ---\r\nepsilon = 0.05 # Controls the magnitude of the perturbation. Tune this!\r\n# A smaller epsilon means a smaller, less noticeable perturbation,\r\n# but potentially less likely to succeed.\r\n\r\n# --- 3. Prepare Data for Gradient Calculation ---\r\n# Add batch dimension: (1, height, width, channels)\r\nclean_spectrogram_tensor = tf.convert_to_tensor(clean_spectrogram[np.newaxis, ...])\r\n# Prepare label as a tensor\r\ntrue_label_tensor = tf.convert_to_tensor([true_label])\r\n# For targeted attack, use target label tensor\r\n# target_label_tensor = tf.convert_to_tensor([target_label])\r\n\r\n# --- 4. Compute the Gradient using GradientTape ---\r\nwith tf.GradientTape() as tape:\r\n    tape.watch(clean_spectrogram_tensor) # Tell tape to record operations on this tensor\r\n\r\n    # Get model predictions\r\n    predictions = model(clean_spectrogram_tensor)\r\n\r\n    # Calculate Loss\r\n    # Use SparseCategoricalCrossentropy if your labels are integers (like true_label)\r\n    # Use CategoricalCrossentropy if your labels are one-hot encoded\r\n    loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\r\n\r\n    # --- Determine the target for the loss calculation ---\r\n    # For UNTARGETED attack: Maximize loss for the TRUE class\r\n    loss = loss_object(true_label_tensor, predictions)\r\n\r\n    # For TARGETED attack: Minimize loss for the TARGET class\r\n    # if targeted_attack:\r\n    #     loss_object_targeted = tf.keras.losses.SparseCategoricalCrossentropy()\r\n    #     loss = -loss_object_targeted(target_label_tensor, predictions) # Minimize loss for target class = Maximize negative loss\r\n\r\n    # --- 5. Compute Gradient of Loss WRT Input ---\r\n    gradient = tape.gradient(loss, clean_spectrogram_tensor)\r\n\r\n# --- 6. Calculate the Perturbation ---\r\n# Get the sign of the gradient\r\nsigned_gradient = tf.sign(gradient)\r\n\r\n# Multiply by epsilon\r\nperturbation = epsilon * signed_gradient\r\n\r\n# --- 7. Create the Adversarial Example ---\r\nadversarial_spectrogram_tensor = clean_spectrogram_tensor + perturbation\r\n\r\n# Crucially, clip the values to the valid data range (e.g., [0, 1])\r\nadversarial_spectrogram_tensor = tf.clip_by_value(adversarial_spectrogram_tensor, 0, 1)\r\n\r\n# Convert back to numpy array (remove batch dimension)\r\nadversarial_spectrogram = adversarial_spectrogram_tensor.numpy().squeeze() # squeeze removes the batch dim and channel dim if it was 1\r\n\r\nprint(f\"Generated adversarial spectrogram shape: {adversarial_spectrogram.shape}\")\r\n\r\n# --- 8. Evaluate Digital Adversarial Example ---\r\nprint(\"\\n--- Digital Evaluation ---\")\r\n\r\n# Evaluate clean spectrogram\r\nclean_prediction = model.predict(clean_spectrogram_tensor)\r\nclean_predicted_class = np.argmax(clean_prediction)\r\nprint(f\"Prediction for clean spectrogram: Class {clean_predicted_class} (Confidence: {np.max(clean_prediction):.2f})\")\r\nif clean_predicted_class == true_label:\r\n    print(\"Clean sample classified correctly.\")\r\nelse:\r\n    print(\"Warning: Clean sample misclassified by the model itself!\")\r\n\r\n# Evaluate adversarial spectrogram\r\nadversarial_prediction = model.predict(adversarial_spectrogram_tensor)\r\nadversarial_predicted_class = np.argmax(adversarial_prediction)\r\nprint(f\"Prediction for adversarial spectrogram: Class {adversarial_predicted_class} (Confidence: {np.max(adversarial_prediction):.2f})\")\r\n\r\n# Check if the attack was successful (digitally)\r\nattack_successful_digital = False\r\nif adversarial_predicted_class != true_label:\r\n    if 'targeted_attack' in locals() and targeted_attack:\r\n        if adversarial_predicted_class == target_label:\r\n            print(\"Targeted attack SUCCESSFUL (digitally)!\")\r\n            attack_successful_digital = True\r\n        else:\r\n            print(f\"Targeted attack FAILED (digitally): Misclassified as {adversarial_predicted_class} instead of target {target_label}.\")\r\n    else: # Untargeted attack\r\n         print(\"Untargeted attack SUCCESSFUL (digitally)!\")\r\n         attack_successful_digital = True\r\nelse:\r\n    print(\"Attack FAILED (digitally): Adversarial sample still classified as the true class.\")\r\n\r\n# --- Optional: Visualize the Spectrograms and Perturbation ---\r\n# This is mainly for debugging and understanding\r\nif spectrogram_height <= 64 and spectrogram_width <= 64: # Avoid plotting massive arrays\r\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\r\n    axes[0].imshow(clean_spectrogram.squeeze(), aspect='auto', origin='lower', cmap='viridis')\r\n    axes[0].set_title(f'Clean Spectrogram (True Class: {true_label})')\r\n    axes[1].imshow(adversarial_spectrogram.squeeze(), aspect='auto', origin='lower', cmap='viridis')\r\n    axes[1].set_title(f'Adversarial Spectrogram (Predicted: {adversarial_predicted_class})')\r\n    # Plot the perturbation itself (difference between adversarial and clean)\r\n    perturbation_viz = (adversarial_spectrogram - clean_spectrogram).squeeze()\r\n    # Use a diverging colormap for the perturbation\r\n    max_val = np.max(np.abs(perturbation_viz))\r\n    axes[2].imshow(perturbation_viz, aspect='auto', origin='lower', cmap='coolwarm', vmin=-max_val, vmax=max_val)\r\n    axes[2].set_title(f'Perturbation (Scaled by {epsilon})')\r\n    plt.tight_layout()\r\n    plt.show()\r\nelse:\r\n    print(\"\\nSpectrogram too large to visualize easily. Skipping plot.\")\r\n    print(\"You can inspect min/max values of perturbation:\",\r\n          np.min(adversarial_spectrogram - clean_spectrogram),\r\n          np.max(adversarial_spectrogram - clean_spectrogram))\r\n\r\n# --- Saving the adversarial example ---\r\n# You'll likely want to save this generated adversarial spectrogram\r\n# for use in the Module 7 over-the-air transmission.\r\n# np.save('adversarial_spectrogram.npy', adversarial_spectrogram)\r\n# print(\"Adversarial spectrogram saved as 'adversarial_spectrogram.npy'\")\r\n\r\n```\r\n\r\n**Explanation of the Code:**\r\n\r\n1.  We load the Keras model we trained in Module 4.\r\n2.  We define `epsilon`, the critical parameter controlling the *strength* of the perturbation. Tuning this is key!\r\n3.  We take a clean spectrogram sample (`clean_spectrogram`) and its true `true_label`.\r\n4.  `tf.GradientTape()` is used to record the forward pass (`model(clean_spectrogram_tensor)`) so TensorFlow knows how to calculate the gradient of the output (loss) with respect to the input (`clean_spectrogram_tensor`).\r\n5.  We calculate the loss. For an *untargeted* attack, we calculate the standard loss between the model's prediction and the *true* label. The gradient will then point in the direction that increases this loss.\r\n6.  `tape.gradient(loss, clean_spectrogram_tensor)` computes the gradient. This is the core step.\r\n7.  `tf.sign(gradient)` gives us the direction of the gradient for each \"pixel\" in the spectrogram.\r\n8.  We scale this signed gradient by `epsilon` to get the perturbation.\r\n9.  We add the perturbation to the original clean spectrogram.\r\n10. `tf.clip_by_value(..., 0, 1)` is crucial. It ensures that the resulting adversarial spectrogram's values stay within the valid range (0 to 1 in this example). This is a simple form of constraint. More complex constraints (like L2 norm limits on the *perturbation itself* or frequency domain constraints) would be implemented here or in the perturbation calculation.\r\n11. Finally, we evaluate the generated `adversarial_spectrogram` by feeding it back into the model and checking the prediction. This confirms the attack's success *in the digital domain*.\r\n\r\n### 6.5 Targeted vs. Untargeted Attacks (Code Variation)\r\n\r\nThe code snippet above shows an untargeted attack (maximize loss for the true class). To make it a *targeted* attack (make the model predict a specific `target_label`), you simply change how the loss is calculated inside the `GradientTape`:\r\n\r\n```python\r\n# ... (previous code setting up model, data, epsilon) ...\r\n\r\n# For TARGETED attack:\r\ntarget_label = 5 # Example: Make WiFi (class 2) look like LTE (class 5)\r\ntarget_label_tensor = tf.convert_to_tensor([target_label])\r\n\r\nwith tf.GradientTape() as tape:\r\n    tape.watch(clean_spectrogram_tensor)\r\n    predictions = model(clean_spectrogram_tensor)\r\n\r\n    # --- Change the loss calculation for targeted attack ---\r\n    # We want to minimize the loss between the prediction and the TARGET label.\r\n    # Minimizing loss is equivalent to maximizing negative loss.\r\n    loss_object_targeted = tf.keras.losses.SparseCategoricalCrossentropy()\r\n    loss = -loss_object_targeted(target_label_tensor, predictions) # Note the negative sign!\r\n\r\n    gradient = tape.gradient(loss, clean_spectrogram_tensor)\r\n\r\n# ... (rest of the code is the same - calculating perturbation, creating adversarial example, clipping, evaluation) ...\r\n```\r\nBy minimizing the loss with respect to the *target* label, the gradient points in the direction that makes the prediction *closer* to the target class.\r\n\r\n### 6.6 Constraining the Adversarial Perturbation (Beyond Clipping)\r\n\r\nClipping to the valid data range [0, 1] is a basic constraint. More sophisticated attacks and evaluations often constrain the *magnitude of the perturbation itself* using Lp norms.\r\n\r\n*   **L-infinity norm (`L_inf`):** Limits the *maximum absolute value* of any single element in the perturbation. `||perturbation||_inf <= epsilon`. This is what our FGSM code with clipping *approximates* if the data range is [0,1] and epsilon is small. The code `adversarial = clean + epsilon * sign(gradient)` and then clipping ensures `|adversarial - clean| <= epsilon` (assuming clean is 0-1 and perturbation doesn't push it beyond 0 or 1).\r\n*   **L-2 norm (`L_2`):** Limits the square root of the sum of the squares of all elements in the perturbation. `||perturbation||_2 <= epsilon`. This corresponds to the \"energy\" or \"power\" of the perturbation.\r\n\r\nImplementing L2 constrained FGSM or PGD requires normalizing the gradient differently or projecting the perturbation back onto an L2 ball. PGD (Projected Gradient Descent) is essentially an iterative FGSM where, after each small gradient step, the result is \"projected\" back into the allowed epsilon-ball (defined by L-inf or L2 norm).\r\n\r\nFor this module's project, starting with the simple FGSM and clipping is sufficient. Understanding Lp norms is key for reading AML papers and implementing more robust attacks/defenses.\r\n\r\n### 6.7 Applying to Raw IQ Data (Conceptual)\r\n\r\nIf your M4 classifier worked directly on raw IQ data (e.g., a 1D CNN or LSTM taking complex samples over time), the process would be conceptually similar:\r\n\r\n1.  Your input data would be a tensor of complex numbers (or two tensors, one for I and one for Q).\r\n2.  Your model would be designed to process this sequence.\r\n3.  You would use `tf.GradientTape` to compute the gradient of the loss with respect to the *IQ input tensor*.\r\n4.  The perturbation would be calculated on this IQ tensor.\r\n5.  **Constraints are harder here:** Simple clipping [0, 1] doesn't make sense for IQ. Constraints would likely be on:\r\n    *   The magnitude of the perturbation vector at each time step (`|p_i + j*q_i| <= epsilon_t`).\r\n    *   The total power of the perturbed signal over a time window (`Sum(|clean_i + pert_i|^2) <= MaxPower`).\r\n    *   Potentially frequency domain constraints (e.g., limiting out-of-band emissions).\r\n6.  The output of this process would be an *adversarial IQ sequence*.\r\n\r\nThis is more complex because the relationship between a change in a complex IQ sample and its physical manifestation (amplitude/phase) is direct, and physical constraints (like peak or average power) translate differently than simple image pixel clipping.\r\n\r\n### 6.8 Module Project/Exercise\r\n\r\nAlright, time to get your hands dirty and build the digital adversarial generator!\r\n\r\n**Steps:**\r\n\r\n1.  **Ensure Prerequisites:** Make sure your Module 4 target classifier (`rf_classifier_model.h5`) is saved and accessible. Have a small dataset of clean RF spectrograms (or whatever format your M4 model used) and their true labels ready.\r\n2.  **Choose a Framework:** Use the ML framework you used in M4 (TensorFlow or PyTorch). The code example above is for TensorFlow. If using PyTorch, the concepts (gradient calculation via `requires_grad=True` and backpropagation, applying perturbation, clipping) are analogous, just the syntax differs (`torch.autograd.backward`, `tensor.grad`, `torch.sign`, `torch.clamp`).\r\n3.  **Implement the Attack:** Write a Python script that:\r\n    *   Loads your trained RF classifier model from M4.\r\n    *   Loads at least one clean sample of RF data (spectrogram recommended) and its true label.\r\n    *   Implements the FGSM attack algorithm as shown in the TensorFlow example (or its PyTorch equivalent).\r\n    *   **Experiment with `epsilon`:** Start with a small `epsilon` (e.g., 0.01) and gradually increase it (e.g., 0.05, 0.1, 0.2) until the digital attack is successful. Note how the perturbation visually changes (if you plot it) and how the model's prediction confidence changes.\r\n    *   **Choose Targeted or Untargeted:** Decide if you want to try an untargeted attack first (simpler) or a targeted one (more challenging, requires picking a target class). Implement the corresponding loss calculation.\r\n    *   Generate the adversarial version of your clean sample.\r\n    *   Digitally evaluate the adversarial sample by feeding it back into the loaded model.\r\n    *   Print the model's prediction for both the clean and adversarial samples.\r\n    *   Verify and report if the digital attack was successful (i.e., the model misclassified the adversarial sample as intended).\r\n4.  **Save the Adversarial Sample:** Once you have a digital adversarial example that successfully fools the model, save it to a file (e.g., using `numpy.save`). This saved data is the input you will feed into your SDR's transmission buffer in Module 7.\r\n5.  **Document:** Note down the `epsilon` value used, the true class, the predicted class for the clean sample, and the predicted class for the adversarial sample. Describe the attack goal (targeted/untargeted) and the data representation used (spectrogram).\r\n\r\n**Tips for the Project:**\r\n\r\n*   Start simple! Get the untargeted FGSM working first.\r\n*   Use a single, well-understood clean sample from your M4 dataset for initial testing.\r\n*   Debug by checking the shapes of your tensors and numpy arrays at each step.\r\n*   If the digital attack isn't working, double-check:\r\n    *   Is the model loading correctly?\r\n    *   Is the data format correct (e.g., batch dimension, data type float32)?\r\n    *   Is `tape.watch()` being called correctly (TensorFlow)?\r\n    *   Is the loss function appropriate for your labels (sparse vs. categorical)?\r\n    *   Is `epsilon` large enough? (But don't make it *too* large; the goal is a *small* perturbation).\r\n    *   Are you clipping correctly?\r\n*   If using a targeted attack, ensure the `target_label` is different from the `true_label`.\r\n\r\n---\r\n\r\n### 6.9 Summary\r\n\r\nIn this module, we've taken the adversarial concepts from Module 5"
    },
    {
      "title": "module_7",
      "description": "module_7 Overview",
      "order": 7,
      "content": "Okay, RF, Offensive Security, AI, and coding enthusiasts! Welcome to Module 7. We've journeyed from the fundamentals of RF and SDRs, through building an AI classifier, to crafting digital adversarial examples. Now, we stand at the exciting, challenging, and ethically sensitive threshold of taking our digital creations and pushing them into the physical world – over the airwaves.\r\n\r\nThis module is where the rubber meets the road (or where the electrons meet the antenna). We'll confront the harsh realities of the physical layer and learn how to use our SDRs not just for listening, but for actively transmitting our carefully crafted adversarial signals.\r\n\r\nLet's dive deep.\r\n\r\n---\r\n\r\n## Module 7: Over-the-Air Attacks: Transmitting Adversarial RF\r\n\r\n**Module Objective:** Bridge the gap between digital adversarial examples and physical RF transmission, addressing the challenges of over-the-air effects and executing adversarial attacks using SDRs in a controlled environment.\r\n\r\n**Recap from Module 6:** In Module 6, we learned the theory of Adversarial Machine Learning and, crucially, developed the code to generate *digital* adversarial examples. We took clean IQ data or spectrograms and calculated small perturbations that, when added, fooled our target AI classifier *in simulation*. We verified their effectiveness by feeding them directly into the loaded model.\r\n\r\n**The Challenge Ahead:** The digital world is perfect. The physical RF world is messy. Our carefully calculated digital perturbation, effective in isolation, must now survive conversion to analog, transmission through a noisy, unpredictable channel, and reception by the target system, all while ideally remaining \"small\" or inconspicuous in the physical domain. This is the core challenge of Over-the-Air (OTA) Adversarial ML.\r\n\r\n### 7.1 The Chasm: Why Over-the-Air AML is Hard\r\n\r\nTaking a digital adversarial example and transmitting it isn't as simple as just loading the IQ data into the SDR's transmit buffer. Several factors conspire against the attacker:\r\n\r\n*   **Channel Effects:**\r\n    *   **Fading:** Signal strength varies significantly over time and space due to multipath propagation. A perturbation calculated for a strong signal might be ineffective if the signal fades.\r\n    *   **Multipath:** Signals bounce off objects, arriving at the receiver via multiple paths with different delays and phases. This causes constructive and destructive interference, distorting the signal waveform and potentially wiping out or altering the delicate adversarial perturbation.\r\n    *   **Doppler Shift:** If the transmitter or receiver is moving, the frequency of the signal appears to shift. This can disrupt synchronization and signal structure.\r\n*   **Noise and Interference:** The real world is full of thermal noise and signals from other devices. The adversarial perturbation is often designed to be low-power. Can it survive being buried in the noise floor or masked by other signals?\r\n*   **Hardware Non-linearities:** SDR transmitters, especially amplifiers, are not perfectly linear. They can introduce distortion (e.g., spectral regrowth, intermodulation distortion) that changes the transmitted waveform from the ideal digital IQ data. This distortion can alter or destroy the adversarial perturbation.\r\n*   **Synchronization:** For many signal types, correct demodulation and classification require precise timing and frequency synchronization. The attacker's signal might not be perfectly synchronized with the target receiver or the signal it's trying to impersonate/attack.\r\n*   **Power Constraints:** Adversarial perturbations are often designed to be small relative to the clean signal power. This is an attacker's constraint for stealth or plausibility. Can the perturbation still be effective at realistic power levels after channel loss and noise? The attacker's *total* transmission power is also often limited by regulations or operational constraints.\r\n*   **Bandwidth and Filtering:** The adversarial perturbation might introduce frequency components outside the intended signal's bandwidth. These can be filtered out by the transmitter's hardware, the channel, or the receiver's front end, again destroying the attack.\r\n\r\n**Key Takeaway:** Over-the-Air AML is an active area of research because the physical layer acts as a powerful, unpredictable defense mechanism against digitally crafted attacks. Success requires careful consideration of these real-world effects.\r\n\r\n### 7.2 Translating Digital Adversarial Examples to Analog Waveforms (SDR TX)\r\n\r\nOur adversarial examples from Module 6 are typically in the form of modified IQ data arrays (NumPy arrays of complex numbers) or derived features like spectrograms. To transmit them, we need to convert the IQ data into a continuous analog waveform. This is precisely what the Digital-to-Analog Converter (DAC) and associated RF front-end hardware in a TX-capable SDR do.\r\n\r\nThe process generally involves:\r\n\r\n1.  **Loading the IQ data:** The digital IQ samples are loaded into the SDR's transmit buffer.\r\n2.  **DAC Conversion:** The DAC converts the discrete digital samples into a continuous analog baseband signal (I and Q components).\r\n3.  **Upconversion:** This analog baseband signal is mixed with a local oscillator (LO) frequency to shift it up to the desired carrier frequency.\r\n4.  **Amplification and Filtering:** The signal is amplified to the desired transmission power and filtered to remove unwanted spectral components (like the LO image).\r\n5.  **Transmission:** The amplified and filtered signal is sent to the antenna and radiated.\r\n\r\n**Crucial Parameters for Transmission:**\r\n\r\n*   **Center Frequency:** The carrier frequency at which the signal will be transmitted. Must match the frequency the target system is listening on.\r\n*   **Sample Rate:** The rate at which the DAC processes the IQ samples. This determines the *bandwidth* of the transmitted signal (Nyquist theorem: max bandwidth is half the sample rate). The sample rate of the transmitted data *must* match the sample rate configured on the SDR.\r\n*   **Gain (TX Power):** Controls the amplification stage. Needs careful setting based on the testbed setup and legal limits.\r\n*   **Antenna:** The choice and placement of antenna significantly impact the radiated signal and its interaction with the environment.\r\n\r\n**Using SDR Libraries for Transmission:**\r\n\r\nJust as we used libraries like `pyrtlsdr` or `pysdr` for reception, libraries like `pyhackrf` (for HackRF), `pylimesdr` (for LimeSDR), `pyuhd` (for USRP), or `pyadi-iio` (for ADALM-PLUTO) provide Python interfaces to control the SDR's transmission capabilities.\r\n\r\nThe core function is usually something like `tx_stream.send()` or `sdr.tx()`, which takes an array of complex IQ samples and sends them out the antenna.\r\n\r\n**Example: Conceptual SDR Transmission Script (using a hypothetical `pysdr_tx` interface):**\r\n\r\n```python\r\nimport numpy as np\r\nimport time\r\n# Assume pysdr_tx is installed and configured for your TX SDR\r\n# (In reality, you'd use pyhackrf, pylimesdr, pyuhd, pyadi-iio, etc.)\r\n# from pysdr_tx import SDRTxDevice\r\n\r\n# --- Configuration ---\r\n# Replace with your actual SDR device initialization\r\n# sdr = SDRTxDevice(...)\r\n# For demonstration, let's mock the SDR object\r\nclass MockSDRTx:\r\n    def __init__(self, center_freq, sample_rate, tx_gain):\r\n        self.center_freq = center_freq\r\n        self.sample_rate = sample_rate\r\n        self.tx_gain = tx_gain\r\n        print(f\"MockSDRTx configured: Freq={center_freq/1e6} MHz, SR={sample_rate/1e6} MSps, Gain={tx_gain} dB\")\r\n\r\n    def tx(self, iq_samples):\r\n        print(f\"MockSDRTx: Transmitting {len(iq_samples)} IQ samples...\")\r\n        # In a real SDR library, this would send data to the hardware\r\n        # For mock, simulate transmission time\r\n        time.sleep(len(iq_samples) / self.sample_rate)\r\n        print(\"MockSDRTx: Transmission finished.\")\r\n\r\n# --- SDR Setup ---\r\nTX_CENTER_FREQ = 433.92e6  # Example: ISM band frequency\r\nTX_SAMPLE_RATE = 1e6       # Example: 1 MSps\r\nTX_GAIN = 0                # Example: dB gain (adjust carefully!)\r\n\r\n# Initialize the SDR (replace with real code for your device)\r\nsdr = MockSDRTx(TX_CENTER_FREQ, TX_SAMPLE_RATE, TX_GAIN)\r\n\r\n# --- Prepare Data for Transmission ---\r\n# Load your adversarial IQ data generated in Module 6\r\n# This would be a numpy array of complex numbers\r\n# For this example, let's create a simple sine wave as placeholder data\r\nduration = 0.01 # seconds\r\nt = np.arange(0, duration, 1/TX_SAMPLE_RATE)\r\n# Create a tone at 100 kHz offset from center frequency\r\ntone_freq = 100e3\r\niq_data_to_transmit = 0.5 * np.exp(2j * np.pi * tone_freq * t)\r\nprint(f\"Prepared {len(iq_data_to_transmit)} IQ samples for transmission.\")\r\n\r\n# --- Transmit ---\r\nprint(\"Starting transmission...\")\r\ntry:\r\n    # In a real scenario, you might transmit in chunks or continuously\r\n    # For a simple example, transmit the whole array\r\n    sdr.tx(iq_data_to_transmit)\r\n    print(\"Transmission successful (mock).\")\r\n\r\nexcept Exception as e:\r\n    print(f\"Error during transmission: {e}\")\r\nfinally:\r\n    # In a real script, you'd have sdr.close() or similar\r\n    print(\"Transmission script finished.\")\r\n\r\n# --- Important Considerations for Real Transmission ---\r\n# 1. Power/Gain: START WITH VERY LOW GAIN and use attenuators.\r\n# 2. Frequency/Bandwidth: Ensure you are transmitting within legal/allowed bands.\r\n# 3. Data Type: SDR libraries often expect specific data types (e.g., complex64).\r\n# 4. Buffer Management: For continuous transmission, manage transmit buffers efficiently.\r\n# 5. Synchronization: If attacking a specific signal instance, timing is critical.\r\n```\r\n\r\nThis example shows the basic structure: configure SDR, prepare IQ data, call the transmit function. The real complexity lies in preparing the *correct* adversarial IQ data and managing the transmission parameters safely and effectively.\r\n\r\n### 7.3 Setting up the Controlled Over-the-Air Testbed\r\n\r\nExecuting adversarial RF attacks requires a controlled environment. You *must not* transmit adversarial signals that could interfere with legitimate communications. This is illegal and unethical. A controlled testbed allows repeatable experiments while minimizing risk.\r\n\r\n**Goal:** Create a setup where you can transmit your adversarial signal and have your target AI classifier (running on the RX SDR from M4) receive it, isolated from external interference and without causing external interference.\r\n\r\n**Components:**\r\n\r\n1.  **TX SDR:** Your transmit-capable SDR.\r\n2.  **RX SDR:** Your receive-capable SDR connected to the system running your Module 4 AI classifier.\r\n3.  **Antennas:** Simple antennas for TX and RX.\r\n4.  **Attenuators:** **CRITICAL.** These reduce the signal power significantly. Needed to keep signals low power and, importantly, to control the *relative* strength of your adversarial signal at the receiver.\r\n5.  **Cables:** Connect SDRs to attenuators and antennas.\r\n6.  **Optional but Recommended:**\r\n    *   **Shielded Box/Faraday Cage:** A metal enclosure to isolate your testbed from the outside world. Even a simple metal box can help.\r\n    *   **Directional Antennas:** Can help focus signal energy between TX and RX, reducing spill-over.\r\n    *   **Signal Generator:** Useful for providing known, clean signals to test both your TX and RX/Classifier chain independently.\r\n\r\n**Testbed Configurations (Increasing Control/Complexity):**\r\n\r\n*   **Direct Cable Connection (with Attenuation):**\r\n    *   TX SDR -> Cable -> Attenuator -> Cable -> RX SDR\r\n    *   *Pros:* Complete control over the signal path. No channel effects, noise, or external interference. Repeatable. Easiest for initial testing of the digital-to-analog-to-digital chain.\r\n    *   *Cons:* Doesn't simulate real over-the-air effects (fading, multipath).\r\n    *   *Use Case:* Verify your adversarial IQ data is correctly transmitted and received *digitally* by the RX SDR *before* hitting the classifier, and that the classifier behaves as expected in a perfect scenario. Attenuation is still needed to protect SDR inputs/outputs.\r\n\r\n*   **Attenuated Over-the-Air (Close Proximity with Attenuators):**\r\n    *   TX SDR -> Cable -> Attenuator -> Antenna\r\n    *   RX SDR -> Cable -> Attenuator -> Antenna\r\n    *   Antennas placed very close (e.g., < 1 meter), possibly pointed at each other.\r\n    *   *Pros:* Introduces some minimal over-the-air effects (line-of-sight path). Still relatively controlled.\r\n    *   *Cons:* Minimal simulation of real-world channel effects. Still requires significant attenuation. Risk of interference if not careful with power and frequency.\r\n    *   *Use Case:* Test the basic OTA transmission and reception chain with minimal channel impact.\r\n\r\n*   **Over-the-Air in a Shielded Environment:**\r\n    *   TX SDR -> Cable -> Attenuator -> Antenna (inside box)\r\n    *   RX SDR -> Cable -> Attenuator -> Antenna (inside box)\r\n    *   All inside a metal enclosure (Faraday cage).\r\n    *   *Pros:* Best simulation of OTA effects *without* causing external interference. Can introduce objects inside the box to simulate multipath.\r\n    *   *Cons:* Requires building or acquiring a shielded box. More complex setup.\r\n    *   *Use Case:* Realistic OTA testing in a safe, repeatable environment.\r\n\r\n**Safety and Legality Note:**\r\n*   **ALWAYS** use appropriate attenuation, especially in direct cable connections or close proximity. SDRs have power limits on their ports.\r\n*   **NEVER** transmit at high power or on frequencies you are not licensed for, especially outside a shielded environment. Know the legal regulations in your region (e.g., ISM bands, amateur radio licenses).\r\n*   **START WITH MINIMUM POWER/GAIN** and gradually increase *only* as needed within your controlled setup.\r\n*   Consider using very low power test signals or only transmitting for very short durations.\r\n\r\n**Example Testbed Setup (Conceptual Diagram):**\r\n\r\n```\r\n+---------------------+       +------------+       +------------+       +---------------------+\r\n| TX SDR (Python TX  | ----> | Cable      | ----> | Attenuator | ----> | TX Antenna (inside  |\r\n| Script w/ M6 data)  |       |            |       |            |       | test environment)   |\r\n+---------------------+       +------------+       +------------+       +---------------------+\r\n                                                                                   |\r\n                                                                                   |  Over-the-Air\r\n                                                                                   |  (Controlled, Attenuated Path)\r\n                                                                                   |\r\n+---------------------+       +------------+       +------------+       +---------------------+\r\n| RX SDR (Python RX  | <---- | Cable      | <---- | Attenuator | <---- | RX Antenna (inside  |\r\n| Script feeding M4   |       |            |       |            |       | test environment)   |\r\n| Classifier)         |       +------------+       +------------+       +---------------------+\r\n+---------------------+\r\n\r\n```\r\n\r\n### 7.4 Executing the Over-the-Air Attack\r\n\r\nNow we combine our components: the target AI classifier system (M4), the adversarial generation code (M6), the SDR transmission capability (M7.2), and the controlled testbed (M7.3).\r\n\r\nThe attack flow we will implement for the capstone project (and test in this module) is generally:\r\n\r\n1.  **Set up the Testbed:** Connect TX SDR, RX SDR, antennas, attenuators, and place them in the controlled environment.\r\n2.  **Configure SDRs:** Set center frequency, sample rate, and initial low gain for both TX and RX SDRs.\r\n3.  **Start Target Classifier System:** Launch the Python script from M4 that uses the RX SDR to capture data and feed it to the trained AI model for classification. Ensure it's running and ready to process incoming RF data.\r\n4.  **Prepare Clean Signal:** Have the IQ data for a *clean* example of the target signal class ready.\r\n5.  **Transmit Clean Signal (Verification Step):**\r\n    *   Load the clean IQ data into the TX SDR transmit buffer.\r\n    *   Transmit the clean signal.\r\n    *   Observe the output of the target classifier system. It *should* correctly classify the signal. This verifies the basic OTA chain and the classifier's ability to recognize the clean signal over the air in your setup. Adjust TX/RX gain until this works reliably at the lowest possible power.\r\n6.  **Generate Adversarial Signal (Digital):**\r\n    *   Use the code from Module 6 to take the *clean* IQ data (or a representation like its spectrogram) and generate the *adversarial* perturbation.\r\n    *   Add this perturbation to the clean IQ data to create the *adversarial IQ data* for transmission. Ensure the resulting IQ data respects any physical constraints you chose (e.g., power limits).\r\n7.  **Transmit Adversarial Signal:**\r\n    *   Load the adversarial IQ data into the TX SDR transmit buffer.\r\n    *   Transmit the adversarial signal.\r\n    *   *Note:* If attacking a continuous stream or a specific instance of a signal, timing is critical. You might need to transmit the adversarial signal concurrently with a legitimate signal source (if allowed and controlled) or transmit a modified version of the legitimate signal itself. For this course's capstone, modifying and transmitting a known signal sample is the most direct approach.\r\n8.  **Observe and Record Classifier Output:** Watch the target classifier system's output while it receives the adversarial signal.\r\n9.  **Evaluate Success:** Did the classifier misclassify the signal? Did it classify it as \"unknown\"? Compare this to the clean signal classification.\r\n\r\n**Conceptual Code Flow (Integrating M4, M6, M7):**\r\n\r\nThis is the core logic you'll build towards the capstone, but you'll implement the TX part here in M7.\r\n\r\n```python\r\n# --- Imports ---\r\nimport numpy as np\r\nimport time\r\n# Assume your SDR libraries are imported (e.g., pyhackrf, pyadi_iio)\r\n# Assume your M4 classifier code is importable\r\n# from my_classifier_system import RFClassifierSystem, iq_to_spectrogram\r\n\r\n# --- Configuration ---\r\nTX_CENTER_FREQ = 433.92e6\r\nTX_SAMPLE_RATE = 1e6\r\nTX_GAIN = -10 # Start low!\r\nRX_CENTER_FREQ = TX_CENTER_FREQ # RX and TX on the same frequency\r\nRX_SAMPLE_RATE = TX_SAMPLE_RATE\r\nRX_GAIN = 20 # Adjust based on signal strength after attenuation\r\n\r\n# --- Initialize SDRs ---\r\n# tx_sdr = init_tx_sdr(TX_CENTER_FREQ, TX_SAMPLE_RATE, TX_GAIN)\r\n# rx_sdr = init_rx_sdr(RX_CENTER_FREQ, RX_SAMPLE_RATE, RX_GAIN)\r\n\r\n# Mock SDRs for concept demonstration\r\nclass MockSDR:\r\n    def __init__(self, freq, sr, gain, mode):\r\n        self.freq = freq\r\n        self.sr = sr\r\n        self.gain = gain\r\n        self.mode = mode\r\n        print(f\"Mock SDR ({mode}) configured: Freq={freq/1e6} MHz, SR={sr/1e6} MSps, Gain={gain} dB\")\r\n\r\n    def tx(self, iq_samples):\r\n        print(f\"Mock SDR (TX): Transmitting {len(iq_samples)} samples...\")\r\n        # Simulate sending data - in reality, this pushes to hardware buffer\r\n        time.sleep(len(iq_samples) / self.sr)\r\n        print(\"Mock SDR (TX): Transmission complete.\")\r\n        # In a real system, handle buffer management for continuous TX\r\n\r\n    def rx(self, num_samples):\r\n        print(f\"Mock SDR (RX): Capturing {num_samples} samples...\")\r\n        # Simulate capturing data - in reality, this reads from hardware buffer\r\n        # For simulation, let's return some random noise\r\n        captured_iq = np.random.randn(num_samples) + 1j * np.random.randn(num_samples)\r\n        time.sleep(num_samples / self.sr)\r\n        print(\"Mock SDR (RX): Capture complete.\")\r\n        return captured_iq\r\n\r\n# Initialize mock SDRs\r\ntx_sdr = MockSDR(TX_CENTER_FREQ, TX_SAMPLE_RATE, TX_GAIN, mode='TX')\r\nrx_sdr = MockSDR(RX_CENTER_FREQ, RX_SAMPLE_RATE, RX_GAIN, mode='RX')\r\n\r\n\r\n# --- Load Trained Classifier Model (from M4) ---\r\n# classifier_model = load_trained_model(\"path/to/your/model.h5\")\r\n# classifier_system = RFClassifierSystem(classifier_model) # Assuming a class wrapper\r\n\r\n# Mock classifier system\r\nclass MockRFClassifierSystem:\r\n    def __init__(self, model=None):\r\n        self.model = model # Mock model\r\n        self.classes = [\"ClassA\", \"ClassB\", \"Noise\"] # Example classes\r\n        print(\"Mock RF Classifier System initialized.\")\r\n\r\n    def classify(self, iq_data):\r\n        # In M4, this would process IQ to features (e.g., spectrogram) and run model.predict()\r\n        print(f\"Mock RF Classifier: Processing {len(iq_data)} IQ samples...\")\r\n        # Simulate classification based on some simple rule or randomness\r\n        if np.sum(np.abs(iq_data)) > 100: # Simple mock rule\r\n             predicted_class_idx = np.random.randint(0, len(self.classes)) # Random prediction for demo\r\n             prediction = self.classes[predicted_class_idx]\r\n             print(f\"Mock RF Classifier: Predicted class: {prediction}\")\r\n             return prediction\r\n        else:\r\n             print(\"Mock RF Classifier: Predicted class: Noise\")\r\n             return \"Noise\"\r\n\r\nclassifier_system = MockRFClassifierSystem()\r\n\r\n\r\n# --- Prepare Clean Signal Data ---\r\n# Load or generate your clean signal IQ data (e.g., a recorded WiFi burst)\r\n# clean_iq_data = load_iq_data(\"path/to/clean_wifi.iq\")\r\n# For mock, generate a simple signal\r\nduration_clean = 0.005 # seconds\r\nt_clean = np.arange(0, duration_clean, 1/TX_SAMPLE_RATE)\r\nclean_iq_data = 0.8 * np.exp(2j * np.pi * 50e3 * t_clean) # Mock clean signal\r\n\r\n\r\n# --- STEP 5: Transmit Clean Signal (Verification) ---\r\nprint(\"\\n--- Transmitting CLEAN Signal ---\")\r\ntx_sdr.tx(clean_iq_data)\r\n\r\n# In a real system, the RX SDR would capture and the classifier would run continuously\r\n# For this example, let's simulate capturing the transmitted signal\r\n# In the actual M4 script, the RX loop would be running in parallel\r\ncaptured_clean_iq = rx_sdr.rx(len(clean_iq_data)) # Simulate capturing what was TX'd\r\n\r\n# Feed captured data to classifier\r\nprint(\"Classifying captured CLEAN signal...\")\r\nclassifier_system.classify(captured_clean_iq) # Expected output: Correct class (e.g., \"ClassA\")\r\n\r\n\r\n# --- STEP 6: Generate Adversarial Signal (using M6 logic) ---\r\nprint(\"\\n--- Generating ADVERSARIAL Signal (Digital) ---\")\r\n# This is where you integrate your M6 adversarial generation code\r\n# Assume a function generate_adversarial_iq(clean_iq, target_class_idx, model)\r\n# The output is the IQ data that, when transmitted, should fool the classifier\r\n\r\n# Mock adversarial generation: Just add some structured noise\r\nperturbation = 0.1 * np.random.randn(len(clean_iq_data)) + 0.1j * np.random.randn(len(clean_iq_data))\r\n# A real M6 function would calculate this perturbation based on gradients\r\nadversarial_iq_data = clean_iq_data + perturbation\r\n\r\nprint(f\"Generated adversarial IQ data (shape: {adversarial_iq_data.shape})\")\r\n\r\n# Optional: Digitally verify the adversarial example before transmission\r\n# This is what you did in M6\r\n# print(\"Digitally classifying adversarial data...\")\r\n# classifier_system.classify(adversarial_iq_data) # Expected: Misclassification/Evasion digitally\r\n\r\n\r\n# --- STEP 7: Transmit Adversarial Signal ---\r\nprint(\"\\n--- Transmitting ADVERSARIAL Signal ---\")\r\ntx_sdr.tx(adversarial_iq_data)\r\n\r\n# Simulate capturing the transmitted adversarial signal\r\ncaptured_adversarial_iq = rx_sdr.rx(len(adversarial_iq_data))\r\n\r\n# --- STEP 8 & 9: Observe and Evaluate ---\r\nprint(\"Classifying captured ADVERSARIAL signal...\")\r\nclassifier_system.classify(captured_adversarial_iq) # Expected output: Misclassification/Evasion OTA\r\n\r\nprint(\"\\n--- Over-the-Air Attack Execution Complete ---\")\r\n\r\n# --- Cleanup"
    },
    {
      "title": "module_8",
      "description": "module_8 Overview",
      "order": 8,
      "content": "Okay, buckle up! We've journeyed from the fundamental waves of RF to the intricate world of AI classification and the theory of adversarial attacks. Module 8 is where it all converges. This isn't just another lesson; it's the culmination – the building of your functional adversarial RF lab and the demonstration of your newfound skills.\r\n\r\nWe will meticulously integrate the components you've built in Modules 4, 6, and 7, address the real-world challenges of over-the-air attacks, explore how to defend against them, and discuss the vital ethical implications of this powerful knowledge.\r\n\r\n---\r\n\r\n## **Module 8: Capstone Project: Building the Adversarial RF Lab & Future Directions**\r\n\r\n**Module Objective:** Integrate all previously built components into a cohesive system that demonstrates a functional adversarial RF attack against an AI classifier, and explore potential defenses and future research directions.\r\n\r\n**Learning Outcomes:** By the end of this module, you will be able to:\r\n\r\n1.  Design and implement an integrated workflow for an end-to-end adversarial RF attack using SDRs and an AI classifier.\r\n2.  Set up and manage a controlled physical testbed for executing over-the-air adversarial RF experiments.\r\n3.  Execute the full adversarial RF attack pipeline, from signal capture/generation to transmission and classification result observation.\r\n4.  Evaluate the success of an over-the-air adversarial attack and document the methodology and results.\r\n5.  Discuss and conceptually outline potential defense strategies against adversarial RF attacks.\r\n6.  Identify key limitations of the implemented system and potential directions for future research in adversarial RF.\r\n7.  Articulate the ethical responsibilities associated with researching and demonstrating adversarial RF techniques.\r\n8.  Present your functional adversarial RF system and explain its components and operation.\r\n\r\n**Prerequisites:** Successful completion of Modules 1 through 7, including having the functional code components from Module 4 (Target Classifier System), Module 6 (Digital Adversarial Generator), and Module 7 (SDR Transmission Setup/Code). Access to your chosen SDRs (at least one TX-capable, ideally two total for TX/RX).\r\n\r\n---\r\n\r\n### **Section 8.1: Reviewing the Building Blocks (Recap)**\r\n\r\nBefore we integrate, let's quickly list the essential pieces we've developed in previous modules:\r\n\r\n*   **From Module 4:**\r\n    *   A functional RF signal capture and processing pipeline using an SDR (the RX side of the target).\r\n    *   A trained AI model (e.g., a CNN) for classifying RF signals (the brain of the target).\r\n    *   Code to load the trained model and perform inference on processed SDR data.\r\n    *   The *target* system – essentially, Python code that listens with an SDR, processes the data, and reports the classification according to your loaded AI model.\r\n*   **From Module 6:**\r\n    *   Code to implement a digital adversarial attack algorithm (e.g., FGSM, PGD) adapted for your RF data representation (IQ or Spectrograms).\r\n    *   The ability to take a *clean* digital representation of a signal, feed it into your *loaded target model*, calculate the required perturbation to cause misclassification, and generate the *adversarial digital signal* (clean signal + perturbation).\r\n*   **From Module 7:**\r\n    *   Code to configure your TX-capable SDR for transmission.\r\n    *   Code to load digital IQ data (representing the signal waveform) into the SDR's transmission buffer and transmit it over the air.\r\n    *   Understanding of power control, sample rates, and frequency settings for transmission.\r\n    *   Experience setting up a basic controlled test environment.\r\n\r\nOur goal in Module 8 is to wire these pieces together into a single, executable workflow that demonstrates the attack end-to-end.\r\n\r\n### **Section 8.2: Designing the Integrated System Architecture**\r\n\r\nHow do these components interact in a full attack scenario? Let's visualize the data flow for our capstone system:\r\n\r\n```\r\n+-------------------+      +---------------------+      +-----------------------+\r\n|  Clean Signal     |----->|  Digital Data Rep |----->| Adversarial Generator |\r\n| (Source: File,    |      |  (IQ or Spectrogram)|      |    (M6 Code)          |\r\n|  Captured, Synth) |      |                     |      |                       |\r\n+-------------------+      +---------------------+      +-----------+-----------+\r\n                                   |                          |\r\n                                   |                          | (Needs Target Model)\r\n                           +-------+-------+                  |\r\n                           | Trained AI    |<-----------------+\r\n                           | Classifier    |\r\n                           | (M4 Model)    |\r\n                           +---------------+\r\n                                   |\r\n                                   | (Perturbation)\r\n                                   |\r\n+-----------------------+      +--+------------------+\r\n|  SDR Transmission     |<-----|  Adversarial Signal |\r\n|     (M7 Code)         |      |    (Digital)        |\r\n| (TX-Capable SDR)      |      +---------------------+\r\n+-----------------------+\r\n         |\r\n         | (Over-the-Air RF)\r\n         V\r\n+-----------------------+      +---------------------+      +-----------------------+\r\n|  SDR Reception        |----->|  Captured RF Data   |----->| Data Processing       |\r\n|     (M4 Code)         |      |    (IQ)             |      | (Spectrogram, etc.)   |\r\n| (RX-Capable SDR)      |      +---------------------+      +-----------+-----------+\r\n                                                                          |\r\n                                                                          |\r\n                                                                +-------+-------+\r\n                                                                | Trained AI    |\r\n                                                                | Classifier    |\r\n                                                                | (M4 Model)    |\r\n                                                                +---------------+\r\n                                                                          |\r\n                                                                          V\r\n                                                              +-----------------------+\r\n                                                              | Classification Result |\r\n                                                              | (Observe & Evaluate)  |\r\n                                                              +-----------------------+\r\n```\r\n\r\n**Key Workflow Steps:**\r\n\r\n1.  **Obtain a Clean Signal Sample:** Get a digital representation (IQ, Spectrogram) of a signal you want to attack. This could be loaded from a file, captured live, or synthetically generated.\r\n2.  **Generate Adversarial Perturbation:** Use the digital adversarial generation code (M6) and the *target classifier model* (M4) to calculate a small perturbation for the clean signal sample.\r\n3.  **Create Adversarial Signal:** Add the calculated perturbation to the clean signal sample *in the digital domain*.\r\n4.  **Prepare for Transmission:** Convert the adversarial digital signal representation into a format suitable for SDR transmission (usually IQ data).\r\n5.  **Transmit Adversarial Signal:** Use the SDR transmission code (M7) to send the adversarial signal over the air using the TX SDR.\r\n6.  **Receive and Classify:** Use the target system's SDR reception and classification code (M4) to capture the transmitted signal and feed it to the AI model.\r\n7.  **Observe Results:** Check the output of the target classifier. Did it misclassify the signal? Did it fail to classify it (evasion)?\r\n\r\n### **Section 8.3: Implementing the Integrated Attack Workflow**\r\n\r\nNow, let's translate the architecture into code. We'll likely use a main Python script (`capstone_attack.py`) to orchestrate this. We'll assume you have helper functions or classes built in previous modules, and this script will call them.\r\n\r\n**Assumed Helper Functions/Modules:**\r\n\r\n*   `sdr_utils.py`: Contains functions for SDR capture (`capture_samples(freq, bw, duration, sample_rate)`) and transmission (`transmit_samples(iq_data, freq, sample_rate, power)`).\r\n*   `data_processing.py`: Contains functions to convert raw IQ data to your chosen ML input format (e.g., `iq_to_spectrogram(iq_data, sample_rate)`).\r\n*   `ml_utils.py`: Contains functions to load your trained model (`load_classifier_model(model_path)`) and perform inference (`predict(model, processed_data)`).\r\n*   `adversarial_utils.py`: Contains functions to generate adversarial examples (`generate_adversarial_example(model, clean_data, target_label=None, epsilon=0.01, attack_type='fgsm')`).\r\n\r\n**`capstone_attack.py` (Conceptual Structure):**\r\n\r\n```python\r\nimport numpy as np\r\n# Import your helper modules/functions\r\n# from sdr_utils import capture_samples, transmit_samples\r\n# from data_processing import iq_to_spectrogram\r\n# from ml_utils import load_classifier_model, predict\r\n# from adversarial_utils import generate_adversarial_example\r\n\r\n# --- Configuration ---\r\nMODEL_PATH = 'path/to/your/trained/model.h5' # Or .pth, depending on framework\r\nTX_SDR_CONFIG = {\r\n    'freq': 915e6,       # Example frequency (ISM band)\r\n    'sample_rate': 2e6,  # Example sample rate\r\n    'power': -10         # Example power level (dBm - keep LOW for testing!)\r\n}\r\nRX_SDR_CONFIG = {\r\n    'freq': 915e6,       # Must match TX freq\r\n    'sample_rate': 2e6,  # Must match TX sample rate\r\n    'duration': 0.1      # Duration to capture for classification\r\n}\r\nATTACK_PARAMS = {\r\n    'epsilon': 0.05,     # Perturbation magnitude (tune carefully!)\r\n    'attack_type': 'fgsm', # Or 'pgd', etc.\r\n    'target_label': None # Set to a specific class index for targeted attack, None for untargeted\r\n}\r\nCLEAN_SIGNAL_SOURCE = 'file' # Or 'capture', 'synthetic'\r\nCLEAN_SIGNAL_PATH = 'path/to/your/clean_signal.iq' # Or 'path/to/synthetic_generator'\r\nCLEAN_SIGNAL_TRUE_LABEL = 0 # The index of the true class of the clean signal\r\n\r\n# --- 1. Load the Target Classifier Model ---\r\nprint(\"Loading target classifier model...\")\r\ntry:\r\n    model = load_classifier_model(MODEL_PATH)\r\n    print(\"Model loaded successfully.\")\r\nexcept Exception as e:\r\n    print(f\"Error loading model: {e}\")\r\n    exit() # Cannot proceed without the model\r\n\r\n# --- 2. Obtain Clean Signal Sample ---\r\nprint(f\"Obtaining clean signal sample from {CLEAN_SIGNAL_SOURCE}...\")\r\nclean_iq_data = None\r\nif CLEAN_SIGNAL_SOURCE == 'file':\r\n    try:\r\n        # Load IQ data from file (assuming complex numpy array)\r\n        clean_iq_data = np.fromfile(CLEAN_SIGNAL_PATH, dtype=np.complex64)\r\n        print(f\"Loaded {len(clean_iq_data)} IQ samples.\")\r\n    except Exception as e:\r\n        print(f\"Error loading clean signal file: {e}\")\r\n        exit()\r\nelif CLEAN_SIGNAL_SOURCE == 'capture':\r\n    # This is more complex - you'd need to capture a specific signal instance\r\n    # from the air, isolate it, and ensure it's clean.\r\n    # For a simple capstone, loading from a pre-captured file is easier.\r\n    print(\"Live capture for clean signal source is advanced. Using file for capstone.\")\r\n    exit() # Or implement capture logic here\r\nelif CLEAN_SIGNAL_SOURCE == 'synthetic':\r\n     # Implement synthetic generation logic here\r\n     print(\"Synthetic generation for clean signal source is advanced. Using file for capstone.\")\r\n     exit() # Or implement synth logic\r\n\r\nif clean_iq_data is None or len(clean_iq_data) == 0:\r\n    print(\"Failed to obtain clean signal data.\")\r\n    exit()\r\n\r\n# Ensure the clean data length matches what your model expects after processing\r\n# (e.g., if Spectrograms need a fixed input size)\r\n# You might need padding, trimming, or capturing a specific length.\r\n# Let's assume for this example, clean_iq_data is already the correct length needed\r\n# for processing into one input sample for the model.\r\n# Example: If your spectrogram processing takes N samples:\r\n# clean_iq_data = clean_iq_data[:N] # Trim or handle size mismatch\r\n\r\n# --- 3. Preprocess Clean Signal for Model Input ---\r\n# Convert IQ data to the format the model expects (e.g., Spectrogram image)\r\nprint(\"Preprocessing clean signal for model input...\")\r\ntry:\r\n    # This function needs to match the preprocessing done in Module 4/6\r\n    clean_processed_data = iq_to_spectrogram(clean_iq_data, RX_SDR_CONFIG['sample_rate'])\r\n    # Models usually expect a batch dimension, even for a single sample\r\n    clean_processed_data = np.expand_dims(clean_processed_data, axis=0) # Add batch dimension\r\n    print(\"Preprocessing complete.\")\r\nexcept Exception as e:\r\n    print(f\"Error during clean signal preprocessing: {e}\")\r\n    exit()\r\n\r\n\r\n# --- 4. Classify the Clean Signal (Verify Target System) ---\r\nprint(\"Classifying the clean signal with the target model...\")\r\ntry:\r\n    clean_prediction = predict(model, clean_processed_data)\r\n    predicted_label = np.argmax(clean_prediction)\r\n    print(f\"Clean signal classified as label: {predicted_label}\")\r\n    if predicted_label != CLEAN_SIGNAL_TRUE_LABEL:\r\n        print(\"WARNING: Clean signal was NOT correctly classified by the target model!\")\r\n        print(\"This might indicate an issue with the model, data processing, or true label.\")\r\n        # Decide if you want to continue or exit\r\n        # exit()\r\n    else:\r\n         print(\"Clean signal correctly classified. Proceeding with attack generation.\")\r\nexcept Exception as e:\r\n    print(f\"Error during clean signal classification: {e}\")\r\n    exit()\r\n\r\n\r\n# --- 5. Generate Adversarial Perturbation ---\r\nprint(f\"Generating adversarial perturbation ({ATTACK_PARAMS['attack_type']} attack)...\")\r\ntry:\r\n    # This function takes the model and the *processed* clean data\r\n    # It needs access to the model's gradients (for white-box attacks like FGSM/PGD)\r\n    # The output is the *perturbation* in the *processed data* domain\r\n    # Or, your generate function might output the *adversarial processed data* directly.\r\n    # Let's assume it outputs the perturbation in the *processed data* domain for clarity.\r\n    perturbation_processed = generate_adversarial_example(\r\n        model,\r\n        clean_processed_data, # Input to the model\r\n        target_label=ATTACK_PARAMS['target_label'],\r\n        epsilon=ATTACK_PARAMS['epsilon'],\r\n        attack_type=ATTACK_PARAMS['attack_type']\r\n    )\r\n    print(\"Perturbation generated in processed data domain.\")\r\n\r\n    # --- IMPORTANT: Translate Perturbation back to IQ Domain ---\r\n    # This is a CRITICAL and often DIFFICULT step.\r\n    # If your attack was on Spectrograms, how do you create an IQ waveform\r\n    # that results in *that specific* adversarial spectrogram?\r\n    # For this capstone, a simpler approach is often taken:\r\n    # a) Attack directly in the IQ domain (requires an IQ-domain model or differentiable IQ->Processed conversion).\r\n    # b) Generate perturbation in processed domain, then use an approximation or optimization\r\n    #    to find an IQ perturbation that *approximates* the desired processed perturbation.\r\n    # c) Simplify: Generate perturbation *in the IQ domain* directly using an IQ-based attack\r\n    #    (e.g., FGSM on IQ data, requires model taking IQ as input or a differentiable pipeline).\r\n\r\n    # Let's assume for this example, your `generate_adversarial_example` function\r\n    # takes the *IQ data* and the *model* and returns the *adversarial IQ data* directly.\r\n    # This implies your model or a wrapper is differentiable w.r.t. the IQ input.\r\n    # This was a key challenge discussed in Module 6 - choose the method implemented there.\r\n    # If you attacked spectrograms, you need a function here to go from\r\n    # `adversarial_processed_data` back to `adversarial_iq_data`. This is non-trivial.\r\n    #\r\n    # REVISITING M6/M7: A common capstone approach is to generate the perturbation\r\n    # directly in the IQ domain if your model can handle it, or if you use a simpler\r\n    # attack that *adds* noise directly to IQ based on processed gradients.\r\n    # Let's adapt the flow slightly based on a simpler IQ-domain attack generation:\r\n\r\n    print(\"Generating adversarial perturbation directly in IQ domain...\")\r\n    # We need the model accessible within the IQ-domain perturbation generation\r\n    # Or, the function needs access to the model's prediction/gradient function.\r\n    # Let's assume `generate_adversarial_iq` exists from M6 that takes clean IQ and model.\r\n    adversarial_iq_data = generate_adversarial_iq(\r\n        model, # The target model\r\n        clean_iq_data, # The clean IQ data\r\n        target_label=ATTACK_PARAMS['target_label'],\r\n        epsilon=ATTACK_PARAMS['epsilon'], # Epsilon applied in IQ domain (L-inf norm usually)\r\n        attack_type=ATTACK_PARAMS['attack_type']\r\n    )\r\n    print(\"Adversarial IQ data generated.\")\r\n\r\nexcept Exception as e:\r\n    print(f\"Error during adversarial generation: {e}\")\r\n    exit()\r\n\r\n# --- 6. Prepare Adversarial Signal for Transmission ---\r\n# (If generate_adversarial_iq already output the final IQ data, this step is minimal)\r\n# Ensure data type is correct (complex64), apply any final scaling if needed for SDR TX.\r\nadversarial_tx_iq = adversarial_iq_data\r\nprint(f\"Adversarial signal prepared for TX. Length: {len(adversarial_tx_iq)}\")\r\n\r\n# --- 7. Transmit the Adversarial Signal ---\r\nprint(f\"Transmitting adversarial signal on {TX_SDR_CONFIG['freq']/1e6} MHz...\")\r\ntry:\r\n    # Ensure your transmit function is non-blocking or run in a thread\r\n    # if you need to simultaneously capture with another SDR on the same machine.\r\n    transmit_samples(\r\n        adversarial_tx_iq,\r\n        TX_SDR_CONFIG['freq'],\r\n        TX_SDR_CONFIG['sample_rate'],\r\n        TX_SDR_CONFIG['power']\r\n    )\r\n    print(\"Adversarial signal transmission initiated.\")\r\n    # Add a small delay if needed before stopping TX or starting RX capture\r\n    # time.sleep(len(adversarial_tx_iq) / TX_SDR_CONFIG['sample_rate'] + 0.1)\r\n    # Ensure transmit function handles stopping TX after sending the data once.\r\n\r\nexcept Exception as e:\r\n    print(f\"Error during SDR transmission: {e}\")\r\n    # You might want to stop any running TX process here\r\n    exit()\r\n\r\n# --- 8. Receive and Classify the Transmitted Signal ---\r\nprint(f\"Capturing transmitted signal on {RX_SDR_CONFIG['freq']/1e6} MHz for classification...\")\r\ntry:\r\n    # Capture the signal as it's received over the air\r\n    # This capture needs to align with when the TX happens.\r\n    # If TX sends a single burst, RX needs to capture during that burst.\r\n    # If TX sends repeatedly, RX captures a segment.\r\n    # This synchronization is a key challenge!\r\n    # For a simple test, you might just capture *after* initiating TX, hoping to catch it.\r\n    # A more robust approach involves triggering TX and RX simultaneously or using a loop.\r\n    received_iq_data = capture_samples(\r\n        RX_SDR_CONFIG['freq'],\r\n        RX_SDR_CONFIG['sample_rate'],\r\n        RX_SDR_CONFIG['duration'] # Capture duration\r\n    )\r\n    print(f\"Captured {len(received_iq_data)} samples.\")\r\n\r\n    # Find the segment in the received data that corresponds to the transmitted signal\r\n    # This is another challenge! Simple correlation or energy detection might work\r\n    # if the signal is isolated. If it's added to existing noise, it's harder.\r\n    # For the capstone, you might simplify by assuming the *entire* capture is the signal,\r\n    # or that the signal is at the beginning of the capture.\r\n    # Let's assume the capture duration is exactly the signal length for simplicity.\r\n    if len(received_iq_data) < len(adversarial_tx_iq):\r\n         print(\"Warning: Captured data is shorter than transmitted data. Classification might fail.\")\r\n         # Pad or handle size mismatch\r\n    # Trim or pad received_iq_data to the expected size for processing\r\n    processed_received_iq = received_iq_data[:len(adversarial_tx_iq)] # Example trimming\r\n\r\n    # Preprocess the received data for the model\r\n    print(\"Preprocessing received signal for model input...\")\r\n    processed_received_data = iq_to_spectrogram(processed_received_iq, RX_SDR_CONFIG['sample_rate'])\r\n    processed_received_data = np.expand_dims(processed_received_data, axis=0) # Add batch dimension\r\n    print(\"Preprocessing complete.\")\r\n\r\n    # Classify the received adversarial signal\r\n    print(\"Classifying the received adversarial signal...\")\r\n    adversarial_prediction = predict(model, processed_received_data)\r\n    adversarial_predicted_label = np.argmax(adversarial_prediction)\r\n    print(f\"Received adversarial signal classified as label: {adversarial_predicted_label}\")\r\n\r\n    # --- 9. Evaluate Attack Success ---\r\n    print(\"\\n--- Attack Evaluation ---\")\r\n    print(f\"True Clean Label: {CLEAN_SIGNAL_TRUE_LABEL}\")\r\n    print(f\"Clean Signal Classified As: {predicted_label}\")\r\n    print(f\"Adversarial Signal Classified As: {adversarial_predicted_label}\")\r\n\r\n    if adversarial_predicted_label != CLEAN_SIGNAL_TRUE_LABEL:\r\n        print(\"Attack Successful (Untargeted Misclassification)!\")\r\n        if ATTACK_PARAMS['target_label'] is not None and adversarial_predicted_label == ATTACK_PARAMS['target_label']:\r\n             print(\"Attack Successful (Targeted Misclassification)!\")\r\n        elif ATTACK_PARAMS['target_label'] is not None:\r\n             print(\"Attack resulted in misclassification, but not the targeted label.\")\r\n    else:\r\n        print(\"Attack Failed: Adversarial signal still classified as the true label.\")\r\n\r\n    # Optional: Quantify perturbation magnitude in IQ domain\r\n    # perturbation_iq = adversarial_iq_data - clean_iq_data\r\n    # l_inf_norm = np.max(np.abs(perturbation_iq))\r\n    # l_2_norm = np.linalg.norm(perturbation_iq)\r\n    # print(f\"Perturbation L-inf norm (IQ): {l_inf_norm}\")\r\n    # print(f\"Perturbation L-2 norm (IQ): {l_2_norm}\")\r\n\r\n\r\nexcept Exception as e:\r\n    print(f\"Error during reception or classification: {e}\")\r\n    exit()\r\n\r\nprint(\"\\nCapstone attack simulation finished.\")\r\n```\r\n\r\n**Explanation of Code Structure and Challenges:**\r\n\r\n*   **Modularity:** The code relies heavily on the helper functions developed in previous modules. This makes the main script cleaner and easier to understand.\r\n*   **Configuration:** All parameters (frequencies, sample rates, file paths, attack epsilon) are at the top for easy modification.\r\n*   **Workflow Steps:** The code follows the logical steps outlined in Section 8.2.\r\n*   **Error Handling:** Basic `try...except` blocks are included, but real-world robustness requires more.\r\n*   **Key Challenge - IQ to Processed Data Differentiability:** The most complex part of generating adversarial RF examples for *over-the-air* transmission is often translating the required perturbation in the *processed data domain* (like Spectrograms) back into the *IQ domain*. The simplified code above assumes your `generate_adversarial_iq` function somehow handles this, or that your attack operates *directly* on IQ data, which is a significant simplification. A more realistic approach might involve iterative optimization in the IQ domain guided by the processed data gradient. Be prepared to discuss this limitation!\r\n*   **Key Challenge - Synchronization:** Capturing the signal *exactly* when it's transmitted is hard. The code assumes the RX capture duration aligns perfectly or that the signal is easy to find in the capture. In reality, you might need energy detection, correlation, or precise timing/triggering between SDRs.\r\n*   **Key Challenge - Channel Effects:** The digital adversarial example assumes a perfect channel. The over-the-air transmission introduces noise, fading, multipath, and hardware non-linearities. The perturbation needs to be robust enough to survive these effects and *still* cause misclassification upon reception and processing. Your chosen `epsilon` (perturbation magnitude) is crucial here – too small, and the channel destroys it; too large, and it might be detectable or exceed legal power limits.\r\n\r\n### **Section 8.4: Setting up the Physical Testbed**\r\n\r\nExecuting the attack requires a controlled physical environment.\r\n\r\n**Recommended Setup:**\r\n\r\n1.  **Two SDRs:** One configured for Transmission (TX SDR), one for Reception (RX SDR). They should be connected to the same computer running your attack script, or on separate computers communicating over a network (more complex).\r\n2.  **Antennas:** Use small, low-gain antennas appropriate for your chosen frequency band.\r\n3.  **Attenuators:** **CRITICAL!** Use inline RF attenuators between the TX SDR and its antenna, and potentially between the RX SDR and its antenna. This allows you to control the signal strength precisely and keep transmission power extremely low to stay within legal limits and avoid interfering with others. Start with high attenuation (~30-40 dB or more) and reduce it gradually.\r\n4.  **Cables:** Using RF cables to directly connect the TX and RX SDRs (via attenuators!) is the *most controlled* environment. This eliminates multipath and external interference, making your results repeatable. This is highly recommended for the capstone demonstration.\r\n5.  **Shielding (Optional but Recommended):** Conduct the experiment in a shielded box (like a Faraday cage) if possible, especially if transmitting any power over the air, to contain your signals.\r\n6.  **Environment:** Choose a location away from sensitive RF systems. Check local regulations *rigorously* before transmitting anything. Using ISM bands (like 915 MHz, 2.4 GHz, 5.8 GHz) at very low power is generally safer, but *always* verify local laws. **When in doubt, use cables and attenuators instead of antennas for purely lab-based testing.**\r\n\r\n**Diagram (Cabled Testbed):**\r\n\r\n```\r\n+-----------------+       +--------------+       +--------------+       +-----------------+\r\n|  TX SDR         |-------|  RF Cable    |-------|  Attenuator  |-------|  RF Cable    |-------+\r\n| (Computer A/Same)|       |              |       |              |       |              |       |\r\n+-----------------+       +--------------+       +--------------+       +--------------+       |\r\n                                                                                                |\r\n                                                                                                |\r\n+-----------------+       +--------------+       +--------------+       +-----------------+   |\r\n|  RX SDR         |-------|  RF Cable    |-------|  Attenuator  |-------|  RF Cable    |<------+\r\n| (Computer B/Same)|       |              |       |              |       |              |\r\n+-----------------+       +--------------+       +--------------+       +--------------+\r\n```\r\n\r\n**Setup Steps:**\r\n\r\n1.  Connect the TX SDR to its antenna (or cable/attenuator chain).\r\n2.  Connect the RX SDR to its antenna (or cable/attenuator chain).\r\n3.  If using cables,"
    }
  ]
}
        </script>
    
    </div>
    <script src="../script.js"></script> <!-- Include script based on flag -->
</body>
</html>
